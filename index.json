[{"content":"▶️ Internet Infrastructure Arch Linux - News: Valkey to replace Redis in the [extra] Repository Key Facts Valkey will replace Redis in the Arch Linux [extra] repository. The change is due to Redis licensing modifications on March 20th, 2024. Arch Linux Package Maintainers will support the redis package for 14 days from April 17th, 2025, before moving it to the AUR. After the 14-day transition, the redis package will not receive further updates. Summary Arch Linux is transitioning from Redis to Valkey as its key/value datastore in the [extra] repository. This change is a result of Redis altering its license from BSD-3-Clause to RSALv2 and SSPLv1 on March 20th, 2024.\nThe Arch Linux Package Maintainers will continue to support the redis package for approximately 14 days from April 17th, 2025, to facilitate a smooth transition for users. Following this period, the redis package will be moved to the Arch User Repository (AUR) and will no longer receive updates, effectively becoming deprecated. Users are advised to transition to Valkey as soon as possible to avoid potential issues after the 14-day transition window closes.\nBotnet Part 2: The Web is Broken - Jan Wildeboer’s Blog Key Facts Companies inject SDKs into iOS, Android, MacOS, and Windows apps to monetize network bandwidth, creating botnets used for web crawling, brute-force attacks, and other malicious activities. Infatica is one such company offering access to millions of residential, static, and mobile IP addresses via SDKs embedded in third-party apps. Trend Micro\u0026rsquo;s 2023 research confirms malicious actors repackaging freeware to conduct drive-by downloads of services like Infatica, fueling abusive web scraping. The business model enables the sale of network access through infected devices, contributing to the explosion of bot traffic that hampers small web services. The author advocates that all web scraping should be considered abusive and recommends blocking such traffic to protect web infrastructure. Summary The article highlights the rise of shady business practices where companies recruit app developers to embed SDKs into mobile and desktop applications, monetizing users\u0026rsquo; network bandwidth to create large-scale botnets. These botnets are used for aggressive web crawling, AI training data collection, and malicious activities, often causing DDoS-like traffic surges that overwhelm small web servers. Infatica, among other providers, offers access to millions of residential, static, and mobile IP addresses, claiming to carefully monitor command execution within infected apps, though the business model is inherently questionable.\nResearch by Trend Micro in 2023 supports concerns about malicious actors repacking freeware to facilitate drive-by downloads of such proxy services, exacerbating the problem. The widespread use of SDKs that infect user devices and generate illegitimate traffic makes it difficult for users and administrators to detect and block these activities. The author argues that web scraping should be universally regarded as abusive behavior, urging webmasters to implement blocking measures. The proliferation of these practices, driven by AI and the demand for web data, threatens the integrity of the web as originally intended.\nBring Back RSS Feeds to Browsers | JetGirlArt Key Facts The author currently uses Thunderbird to manage RSS feeds. Outlook previously supported RSS feeds but no longer does. The author expresses a preference for native browser RSS feed support similar to bookmark functionality. The author dislikes web-based feed readers due to account requirements and views browser extensions with suspicion. Summary The author reflects on the decline of native RSS feed support in web browsers, currently utilizing Thunderbird as a workaround. Previously, Outlook offered native RSS functionality, which has since been removed. The author laments the loss of a streamlined experience where feeds could be added and accessed similarly to bookmarks, directly within the browser interface.\nThe author expresses dissatisfaction with current alternatives, citing the need for accounts with web-based readers and concerns about the security and installation complexities of browser extensions and open-source projects. The post highlights a desire for a simpler, more integrated solution for accessing and managing RSS feeds, reminiscent of earlier browser capabilities.\n▶️ Open Source Attention K-Mart Shoppers : Free Audio : Free Download, Borrow and Streaming : Internet Archive Key Facts The Internet Archive hosts an audio recording titled \u0026ldquo;Attention K-Mart Shoppers.\u0026rdquo; The recording is a 6-minute, 58-second audio file. It was uploaded on November 16, 2010. The recording is available for free download, borrowing, and streaming. Summary The Internet Archive provides access to an audio recording titled “Attention K-Mart Shoppers.” The 6 minute, 58 second recording was uploaded on November 16, 2010 and is freely available for download, borrowing, and streaming. The recording\u0026rsquo;s content is not further detailed on the archive page.\nTry generating video in Gemini, powered by Veo 2 Key Facts Google introduced video generation in Gemini, powered by Veo 2, available to Google One AI Premium subscribers. Users can transform text prompts into high-resolution, eight-second videos at 720p in MP4 format, landscape 16:9. Gemini\u0026rsquo;s Veo 2 model produces cinematic, detailed videos with realistic physics and human motion, supporting diverse styles. Video creation is accessible via selecting Veo 2 from the model dropdown in Gemini; prompts describe scenes for automatic rendering. Sharing is simplified on mobile, enabling quick uploads to TikTok and YouTube Shorts; a monthly creation limit applies. Whisk Animate allows users to turn images into eight-second videos using Veo 2, available in over 60 countries. Safety measures include extensive red teaming, content policies, and embedding SynthID digital watermarks in generated videos. The rollout begins today for Gemini Advanced users globally, with ongoing expansion over the coming weeks. Summary Google has launched new video generation capabilities within the Gemini AI platform, utilizing the Veo 2 model to convert text prompts into high-resolution, cinematic-quality videos. Available to Google One AI Premium subscribers, this feature enables users to create eight-second videos at 720p resolution in MP4 format, supporting a variety of visual styles from realism to fantasy. To generate a video, users select Veo 2 from the model dropdown in Gemini and describe their scene in detail, with the system producing a dynamic visual representation. The process emphasizes ease of sharing, allowing quick uploads to platforms like TikTok and YouTube Shorts.\nIn addition, Whisk Animate extends these capabilities by transforming images into animated videos, accessible in over 60 countries. The system emphasizes safety through rigorous testing, content moderation, and embedding SynthID watermarks in all videos to indicate AI origin. The feature rollout begins today for Gemini Advanced users worldwide, with gradual expansion planned over the next few weeks. More information is available at gemini.google.com.\nA New Form of Verification on Bluesky - Bluesky Key Facts Bluesky introduced a new blue check verification system on April 21, 2025. Over 270,000 accounts have already linked their domain to their Bluesky username. “Trusted Verifiers,” such as The New York Times, can directly verify accounts, indicated by a scalloped blue check. Bluesky is not currently accepting direct applications for verification but plans to launch a request form in the future. Summary Bluesky has launched a new verification system featuring a standard blue checkmark to indicate authentic and notable accounts. This builds upon the existing system allowing users to link their domain as their username, with over 270,000 accounts already utilizing this feature. In addition to Bluesky-issued checks, the platform is introducing “Trusted Verifiers” – independent organizations authorized to directly verify accounts, denoted by a scalloped blue check.\nWhen a user taps on a verified account’s checkmark, they can view which organizations have granted verification. Users also have the option to hide verification signals within the app’s settings. Bluesky is not currently accepting direct verification requests but plans to introduce a request form as the feature stabilizes.\nI left Spotify. What happened next? Key Facts The author transitioned from Spotify to Jellyfin for music management and playback. Initial attempts included local music files, web-based streaming with htmx, and using Apple Music. Self-hosted Jellyfin was set up on an old computer, with apps like Finamp used for offline listening. The author now runs Jellyfin and Immich on a mini PC for media hosting and management. Several third-party clients support Jellyfin, including Finamp, Fintunes, and Manet, facilitating offline access and remote streaming. Summary The author left Spotify and explored various music playback solutions, including traditional media players like Winamp, VLC, and foobar2000, but found them inadequate for browsing and managing large libraries. They built a web-based music player using htmx to stream music locally, which worked initially but lacked offline capabilities, prompting a switch to Apple Music for offline listening despite its storage inefficiencies.\nEventually, the author discovered Jellyfin, an open-source media server that can replace streaming services like Spotify. After a straightforward setup on an old computer, they began using apps such as Finamp and Fintunes to download music for offline use. Motivated by the benefits of self-hosting, they purchased a mini PC to run Jellyfin and other apps like Immich, enabling access to their entire media library from any device. The article emphasizes that self-hosting is accessible even for non-programmers and advocates for a future where users control their media without relying on third-party services, highlighting the open-source nature of these solutions.\nUsing ~/.ssh/authorized keys to decide what the incoming connection can do – Dan Langille\u0026rsquo;s Other Diary Key Facts The article discusses using ~/.ssh/authorized_keys to specify commands executed upon SSH connection based on the key used It demonstrates restricting rsync access to specific directories with commands like /usr/local/sbin/rrsync -ro /path/ The setup involves multiple SSH keys, each associated with different commands for different backup tasks The author shows how to configure keys for push and pull backup operations, ensuring only authorized actions are performed Highlights the importance of restricting SSH key capabilities for security when managing critical systems like database backups Summary The article explains how ~/.ssh/authorized_keys can be utilized to control the actions permitted during SSH connections by associating specific commands with individual public keys. This approach allows for fine-grained access control, such as restricting rsync operations to read-only access within designated directories using commands like /usr/local/sbin/rrsync -ro /path/. The author provides practical examples, including configuring multiple SSH keys for different backup tasks—one for pushing database backups from a client to a server, and another for pulling backups from the server to a client—each with tailored command restrictions. By assigning distinct keys and commands, the setup ensures that each SSH connection performs only its intended operation, enhancing security for sensitive systems like database backups. The article emphasizes the importance of restricting SSH key capabilities to prevent unauthorized actions while maintaining flexible automation workflows.\nReflections on Unikernels Key Facts Unikernels are single-purpose appliances linking application, kernel drivers, and necessary components into one binary for cloud deployment Based on MirageOS, utilizing OCaml libraries such as TCP/IP, DNS, Xen disk, and network drivers Won the Influential Paper Award at the 2025 ASPLOS conference for their architecture Enable exploration of new OS interfaces by removing legacy layers, leveraging common hypervisor interfaces like Xen netfront, blkfront, virtio-net, and virtio-blk Reduce attack surface due to minimal linking; example includes porting the xenstore service to MirageOS unikernel, increasing security and fault-tolerance with OCaml Irmin database Used in high-security contexts such as QubesOS and MirageOS-based firewall components Facilitate experimentation with OS interfaces, e.g., spawning unikernels in response to network requests with shared memory transfer of TCP state Evolved into various projects like UniKraft, Nanos, and upstream Linux approaches like Unikernel Linux (UKL) Summary Unikernels are specialized, single-binary appliances that embed an application along with necessary kernel drivers and libraries, primarily designed for cloud environments. Originating from MirageOS and supported by OCaml libraries such as TCP/IP, DNS, and Xen device drivers, they enable a minimal attack surface and facilitate security enhancements, exemplified by porting critical services like xenstore into isolated unikernel VMs. Their architecture allows for flexible experimentation with OS interfaces, including dynamic spawning in response to network requests, by leveraging shared memory for state transfer. Unikernels have been recognized with awards and are increasingly adopted in high-assurance systems like QubesOS and in projects such as UniKraft, Nanos, and Linux-based unikernel approaches, supporting features like confidential computing and cross-application isolation.\nPython\u0026rsquo;s new t-strings | Dave Peck Key Facts Python 3.14 will officially include t-strings in late 2025 T-strings are a generalization of f-strings, evaluating to string.templatelib.Template objects Templates must be processed before use; str() on a Template does not return the template content Template objects provide .strings and .values properties as tuples, with one more string than value Developers can access detailed interpolation info via .interpolations, including value, expression, conversion, and format_spec Summary Python\u0026rsquo;s upcoming version 3.14 will introduce t-strings, a new string templating feature that enhances safety and flexibility in string processing. Unlike f-strings, which are immediately evaluated into strings, t-strings evaluate to string.templatelib.Template objects that require explicit processing before use, enabling safer handling of user input and dynamic content.\nTemplates expose .strings and .values properties as tuples, with .strings containing the literal parts and .values holding interpolated expressions. Developers can also access detailed interpolation data through .interpolations, including the raw expression, conversion flags, and format specifications. T-strings support both literal (t\u0026quot;foo\u0026quot;) and programmatic instantiation via the Template constructor, which accepts strings and Interpolation objects in any order. The feature aims to improve string safety, especially in contexts like HTML and SQL, and is expected to influence library and framework development, with tooling support anticipated in formatters and IDEs.\nDefold - Official Homepage - Cross platform game engine Key Facts Defold is a free, production-ready, cross-platform game engine with source code available under open source license. Features include visual editor, code editor, Lua scripting, Lua debugger, scene, particle, tilemap editors, supporting both 2D and 3D development. Supports publishing to major platforms: PlayStation®5, PlayStation®4, Nintendo Switch, Android, iOS, macOS, Linux, Windows, Steam, HTML5, Facebook, and Q3 2024 Xbox. No setup required; fully featured out of the box with zero-config cloud build for native code. Engine is actively developed with monthly releases; recent updates include Defold 1.9.8 (Mar 10, 2025), 1.9.7 (Feb 3, 2025), and 1.9.6 (Dec 18, 2024). Summary Defold is a free, open-source, cross-platform game engine designed for high-performance game development across multiple platforms including PlayStation®5, PlayStation®4, Nintendo Switch, Android, iOS, macOS, Linux, Windows, Steam, HTML5, Facebook, and Xbox (Q3 2024). It offers a comprehensive suite of tools such as a visual editor, code editor, Lua scripting, Lua debugger, scene, particle, and tilemap editors, supporting both 2D and 3D game creation without requiring external setup or configuration. The engine enables developers to use a single code base for all supported platforms, with features like zero-config cloud native build system and native code extension capabilities via the Asset Portal and custom setup options.\nRecent updates demonstrate active development, with monthly releases including version 1.9.8 released on March 10, 2025. Defold emphasizes accessibility, with no licensing fees, royalties, or runtime costs, supported by the Defold Foundation. The platform has a proven track record of game releases across mobile, web, desktop, and consoles, and provides integrated support for analytics, app economy, and game services. Additional resources include tutorials, manuals, API references, and a community forum.\nGemma 3 QAT Models: Bringing state-of-the-Art AI to consumer GPUs - Google Developers Blog Key Facts Google Research announced Gemma 3 models with Quantization-Aware Training (QAT) to enable efficient inference on consumer GPUs Gemma 3 achieves state-of-the-art performance, capable of running on high-end GPUs like NVIDIA H100 using BF16 precision Quantization reduces model VRAM requirements significantly: Gemma 3 27B from 54 GB (BF16) to 14.1 GB (int4); 12B from 24 GB to 6.6 GB; 4B from 8 GB to 2.6 GB; 1B from 2 GB to 0.5 GB QAT involves training models with simulated low-precision operations (~5,000 steps), reducing perplexity drop by 54% at Q4_0 quantization Quantized models are compatible with inference engines like Ollama, llama.cpp, MLX, Gemma.cpp, and can be accessed via Hugging Face and Kaggle Summary Google Research\u0026rsquo;s Gemma 3 models, launched last month, deliver state-of-the-art large language model performance optimized for deployment on consumer-grade GPUs through Quantization-Aware Training (QAT). These models maintain high accuracy while dramatically reducing memory requirements, enabling local inference on hardware such as NVIDIA RTX 3090 (24GB VRAM) for the 27B variant, and NVIDIA RTX 4060 Laptop GPU (8GB VRAM) for the 12B variant. Quantization reduces VRAM load by up to 4x, with the 27B model decreasing from 54 GB to 14.1 GB when using int4 precision. QAT involves training with simulated low-precision operations over approximately 5,000 steps, which reduces perplexity degradation by 54% at Q4_0 quantization, preserving model quality.\nThese advancements make powerful AI models accessible on a wide range of devices, including laptops, desktops, and phones. The models are compatible with popular inference tools such as Ollama, llama.cpp, MLX, Gemma.cpp, and are available on platforms like Hugging Face and Kaggle. The initiative aims to democratize AI by enabling high-performance models to run efficiently on hardware with limited VRAM, facilitating local deployment and development.\nGitHub - PiLiDAR/PiLiDAR Key Facts PiLiDAR is a DIY 360° 3D panorama scanner project with ongoing development status. Core features include a custom serial driver for LDRobot LD06, LD19, or STL27L LiDAR modules, with CRC package integrity check and hardware PWM calibration. It provides 2D live visualization and export of LiDAR data in numpy or CSV formats. The system captures fisheye photos stitched into a 6K spherical map using Hugin panorama stitcher, with automatic exposure and white balance optimization. 3D scene assembly is based on angle and offset data, sampling vertex colors from panorama, with Open3D visualization and export options (PCD, PLY, E57). Supports global registration, ICP fine-tuning, and Poisson surface meshing (slow on Pi4). Summary PiLiDAR is an open-source project aimed at creating a 360° 3D panorama scanner utilizing a Raspberry Pi 4, Raspberry Pi HQ Camera with ArduCam M12 lens, and LDRobot LiDAR modules (LD06, LD19, STL27L). The system features a custom serial protocol implementation for LiDAR data acquisition at a sampling frequency of 4,500 Hz (LD06) or 21,600 Hz (STL27L), with detailed protocol specifications including packet structure, CRC check, and angle interpolation. Hardware components include a NEMA17 stepper motor with A4988 driver, a planetary reduction gearbox, and power options such as 2x 18650 batteries or a 10,000 mAh USB power bank.\nThe software supports live visualization, data export, and panoramic stitching using Hugin, with options for 3D scene assembly, vertex color sampling, and advanced registration techniques. Additional features include USB power switching via uhubctl, remote Jupyter notebook access, and hardware PWM control for motor and LED management. The LiDAR communication protocol involves a 48-byte package with start character 0x54, data length fixed at 12 points, and includes distance and luminance data per point. The project emphasizes technical precision, with detailed wiring, setup instructions, and troubleshooting guides for serial communication, GPIO, and performance optimization.\nGitHub - ericjenott/Evertop: E-ink IBM XT clone with solar power, ultra low power consumption, and ultra long battery life. Key Facts Evertop is a portable IBM XT clone powered by an 80186 microcontroller, with 1MB RAM Uses an e-ink display with no power when not refreshing, supporting DOS, Minix, Windows up to 3.0 Incorporates a 6V, 6W solar panel, two 10,000mAh batteries, and supports simultaneous charging via solar, buck/boost circuit (2.5-20V), and micro USB Power saving mode enables 200-500 hours of continuous use on a single charge; hibernate and power-off features extend battery life Built-in peripherals include CGA, Hercules, MCGA graphics, serial ports, PS/2 ports, USB, Ethernet, WiFi, LoRA radio, headphone jack, volume control, and detachable keyboard The device features a 5.83-inch 648x480 e-ink display, 3D-printed matte PETG enclosure, and supports external full-sized keyboards Runs nearly all IBM PC/XT compatible DOS software from the 1980s and early 90s The system supports multiple power options, automatic battery monitoring, and user-initiated or automatic hibernate modes The project includes a minimal version (\u0026ldquo;Evertop Min\u0026rdquo;) removing keyboard, serial port, Ethernet, LoRA, and reducing battery capacity for lighter, cost-effective off-grid use Powered by C++, C, and Assembly, with extensive peripheral and power management features Summary Evertop is an ultra low-power, solar-powered IBM XT clone built around an ESP32 microcontroller, supporting vintage operating systems such as DOS, Minix, and Windows up to version 3.0. It features a 5.83-inch 648x480 e-ink display that consumes no power when idle, enabling extended battery life. The device includes a built-in keyboard, external PS/2 ports, full CGA, Hercules, and MCGA graphics support, along with serial, USB, Ethernet, WiFi, LoRA radio, headphone jack, and volume control. Its power system combines two 10,000mAh batteries with a 6V, 6W solar panel, and supports charging from solar, buck/boost circuits (2.5-20V), and micro USB simultaneously, with a built-in voltmeter for monitoring.\nPower management features include a power-saving mode capable of running 200-500 hours continuously on a single charge, with options for hibernate and total power shutoff to maximize longevity. The device\u0026rsquo;s enclosure is 3D-printed matte PETG plastic, designed for portability and ease of access to internal components. It supports emulation of floppy and hard disk images stored on a 256GB SD card, with emulated systems capable of mounting up to 8GB of storage. The system supports multiple emulated configurations, user-initiated hibernate, and automatic power-off, making it suitable for indefinite off-grid operation with proper sunlight. The project also offers a minimized version (\u0026ldquo;Evertop Min\u0026rdquo;) that omits the built-in keyboard, serial ports, Ethernet, LoRA, and reduces battery capacity, maintaining core functionalities for lightweight, cost-effective off-grid computing.\nGitHub - nari-labs/dia: A TTS model capable of generating ultra-realistic dialogue in one pass. Key Facts Dia is a 1.6 billion parameter text-to-speech (TTS) model developed by Nari Labs Capable of generating ultra-realistic dialogue in a single pass, conditioned on transcripts and audio Supports nonverbal expressions such as laughter, coughing, and clearing throat Only supports English language generation; pretrained weights available on Hugging Face Licensed under Apache 2.0; accessible via inference code and demo page Summary Dia is a TTS model with 1.6 billion parameters designed to produce highly realistic dialogue from transcripts in one pass. Developed by Nari Labs, it allows conditioning on audio inputs to control emotion and tone, and can generate nonverbal cues like laughter and coughing. The model is optimized for English speech synthesis, with pretrained weights hosted on Hugging Face. It supports dialogue generation with [S1] and [S2] tags, and features voice cloning capabilities demonstrated through example scripts. Inference on enterprise GPUs achieves real-time audio generation, with approximately 40 tokens per second on an A4000 GPU, requiring around 10GB VRAM. The project is licensed under Apache 2.0 and emphasizes ethical use, forbidding identity misuse, deceptive content, and illegal activities. Users can access a demo via Hugging Face Spaces and join community discussions on Discord. Future plans include Docker support, inference speed optimization, and quantization for memory efficiency.\nGitHub - openai/codex: Lightweight coding agent that runs in your terminal Key Facts OpenAI\u0026rsquo;s Codex CLI is a lightweight coding agent designed to run in terminal environments Licensed under Apache-2.0, with 18.9k stars and 1.7k forks on GitHub Supports multiple models and providers, including openai, openrouter, gemini, ollama, mistral, deepseek, xai, groq Implements security modes: Suggest, Auto-Edit, Full Auto, with network-disabled execution in Full Auto Compatible with Node.js 22+, macOS 12+, Ubuntu 20.04+, Windows 11 via WSL2; RAM requirement is at least 4 GB Provides CLI commands such as codex, codex \u0026quot;prompt\u0026quot;, codex -q \u0026quot;prompt\u0026quot;, and shell completion scripts Supports configuration via ~/.codex/ in YAML or JSON, including model, approval mode, notifications, and custom instructions Includes features like /diff command for git diffs, and tab completions for file paths Uses Husky and lint-staged for code quality enforcement, with migration to pnpm for dependency management Supports non-interactive/CI mode for headless operation, with verbose debugging via DEBUG=true Incorporates update check flow supporting npm, pnpm, bun, with registry-based version lookup and multi-manager support Uses Nix flakes for reproducible development environments and detailed migration guides for dependency management Active development with frequent commits, PRs, and community contributions Summary OpenAI\u0026rsquo;s Codex CLI is an open-source, terminal-based coding agent that enables AI-assisted development workflows. It integrates with various models and providers, offering flexible security modes—ranging from suggestion to full auto-approval—allowing users to control the agent\u0026rsquo;s autonomy. The CLI supports multiple operating systems, including macOS, Linux, and Windows via WSL2, requiring Node.js 22+ and at least 4 GB RAM.\nConfiguration is managed through YAML or JSON files located in ~/.codex/, enabling customization of models, approval modes, notifications, and instructions. The tool provides commands such as codex for interactive use, with options for non-interactive mode suitable for CI pipelines. Features like /diff for git diffs and tab completions enhance usability.\nThe project emphasizes code quality and maintainability, utilizing Husky and lint-staged, with a migration to pnpm for dependency management. It includes an intelligent update check system that supports multiple package managers, fetching version info directly from registries to determine updates. Development is further streamlined with Nix flakes for reproducible environments and comprehensive documentation, including migration guides and usage instructions. The repository is actively maintained, with contributions from a broad community of developers.\n15,000 lines of verified cryptography now in Python | Jonathan Protzenko Key Facts Python\u0026rsquo;s hash and HMAC algorithms are now fully implemented using HACL*, a verified cryptographic library, replacing previous implementations. The transition involved integrating approximately 15,000 lines of verified C code into Python\u0026rsquo;s repository, with automated updates from upstream HACL*. The update enhances support for additional modes in Blake2, a new API for all Keccak variants of SHA3, and improved error management, including handling allocation failures. The work spanned 2.5 years, involving contributions from developers Aymeric Fromherz, Son Ho, Gregory P. Smith, Bénédikt Tran, and Chris Eibl, supported by the Python and cryptographic communities. The implementation addresses complex streaming API requirements for block algorithms, ensuring safe, generic, and verified handling of various cryptographic modes and states, including optimized HMAC with dual hash states. Summary In November 2022, Jonathan Protzenko proposed replacing Python’s hash infrastructure with verified cryptography after a CVE in SHA3. As of April 2025, Python now vendors HACL*, a verified cryptographic library, for all default hash and HMAC algorithms, integrating 15,000 lines of verified C code. This transition was seamless for users and involved automating updates from upstream HACL*.\nThe integration required extensive technical work, including implementing new features such as additional Blake2 modes, a comprehensive SHA3 API covering all Keccak variants, and robust error handling, notably for memory allocation failures. The process involved developing a generic, formally verified streaming API for block algorithms using dependent types, accommodating diverse algorithm-specific requirements like pre-input data, variable output lengths, and state retention.\nHandling build complexities, especially with platform-specific optimizations like AVX2 instructions, necessitated refactoring C code to use abstract structs, which was complicated by the auto-generated nature of the code from F* via krml. The project also incorporated rigorous CI testing across 50+ toolchains and architectures, revealing and addressing corner cases such as AVX2-dependent code in HMAC.\nMemory failure propagation was achieved by integrating option types into the verification model, enabling failure detection and propagation through the cryptographic stack. Upstream updates from HACL* are now manageable via a shell script, simplifying maintenance.\nThis large-scale integration demonstrates that verified cryptography is mature enough for production use, providing enhanced security guarantees while meeting engineering standards in a major software project like Python.\nGetting Forked by Microsoft • Philip Laine Key Facts Philip Laine\u0026rsquo;s open source project Spegel was developed to address container image registry downtime issues without stateful components Microsoft contacted Laine to collaborate on Spegel; later, Microsoft developed Peerd, a forked project with significant code similarities and direct references to Spegel Peerd is a fork of Spegel, maintained by Microsoft under the MIT license, with test cases and code snippets directly copied from Spegel The copied code includes functions and comments identical to Spegel, causing confusion among users and raising concerns about proper attribution Laine\u0026rsquo;s Spegel has over 1,700 stars and 14.4 million pulls since its release, despite challenges posed by Microsoft\u0026rsquo;s actions and open source licensing dynamics Summary Philip Laine\u0026rsquo;s open source project Spegel, designed as a lightweight, non-stateful container image registry solution, was positively received with over 1,700 stars and 14.4 million pulls. Microsoft engaged with Laine to collaborate on Spegel, but later developed Peerd, a peer-to-peer container content distributor for Kubernetes, which is a fork of Spegel. Peerd\u0026rsquo;s codebase contains test cases, function signatures, comments, and configuration snippets that are directly copied from Spegel, with no attribution, despite being licensed under MIT, which permits modifications and forks without obligation to credit original authors.\nThis copying has led to confusion among users and challenges for Laine in maintaining an unbiased stance. The incident highlights issues around open source licensing, attribution, and the power imbalance between individual maintainers and large corporations. Laine has considered changing Spegel\u0026rsquo;s license and has enabled GitHub Sponsors to support ongoing development, reflecting concerns about the sustainability and recognition of individual open source contributors.\nClaude Code Best Practices \\ Anthropic Key Facts Claude Code is a command line tool for agentic coding, released by Anthropic on April 18, 2025 Designed as a low-level, unopinionated interface providing near-raw model access Supports customization via CLAUDE.md files, environment tuning, and tool allowlists Integrates with shell tools, MCP servers, slash commands, GitHub, Git, Jupyter notebooks, and headless automation Recommends workflows including exploration, planning, testing, iteration, and multi-Claude setups for parallel tasks Summary Claude Code is a research-developed command line utility for agentic coding, offering flexible, scriptable access to Anthropic\u0026rsquo;s Claude models without enforcing specific workflows. It emphasizes environment customization through CLAUDE.md files, which document commands, style guidelines, and setup instructions, and can be tuned iteratively to optimize context gathering. Users can manage allowed tools via prompts, configuration files, or CLI flags, and integrate Claude with their shell environment, MCP servers, slash commands, GitHub, and Git for enhanced automation and collaboration.\nClaude Code supports advanced workflows such as multi-instance parallel processing, code verification, and visual iteration with screenshots and mockups. Its headless mode enables automation in CI/CD pipelines, issue triage, and code review. Best practices include providing specific instructions, visual references, file paths, URLs, and frequent course corrections. The system also facilitates data input through copy-paste, piping, file reading, and URL fetching. Overall, Claude Code aims to streamline agentic programming, improve productivity, and enable complex automation across diverse environments.\nNew research backs up what gamers have thought for years: cozy video games can be an antidote to stress and anxiety. Key Facts Recent research supports that cozy video games can reduce stress and anxiety. Studies indicate that playing such games can improve mental health and life satisfaction. A Japanese study (2020–2022) found that owning a game console and increasing gameplay by one hour daily correlated with reduced psychological distress. Playing casual games like Flower and engaging in mindfulness meditation showed similar stress reduction effects. Video games are being explored as therapeutic tools for conditions like ADHD and end-of-life grief processing. Summary Emerging research confirms that cozy video games, characterized by their relaxing and community-oriented nature, can serve as effective tools for alleviating stress and anxiety. These games, which often feature non-violent challenges, customizable avatars, and social interactions within virtual communities, have gained popularity especially after titles like Nintendo’s Animal Crossing: New Horizons, released in March 2020 during the COVID-19 pandemic, sold over 13 million units within six weeks. Studies, including a Japanese investigation (2020–2022), reveal that increased gameplay—specifically, an extra hour per day—can decrease psychological distress and enhance overall life satisfaction.\nResearch by Hiroyuki Egami suggests that owning a gaming console and engaging in regular gaming can positively impact mental health, countering earlier concerns about gaming disorder listed in the ICD-11. Additionally, a 2021 study by Michael Wong found no significant difference in stress reduction between playing casual games like Flower and practicing mindfulness meditation, indicating that gaming can be a viable form of self-care. Therapeutic applications are also expanding, with investigations into using video games to manage ADHD and process grief, exemplified by titles like Spiritfarer, which explores themes of death and compassion. Overall, the evidence underscores the potential of cozy games not only for entertainment but also as accessible mental health interventions.\nBuilding a Website Fit for 1999 - Wesley Moore Key Facts Wesley Moore built a retro-themed website in April 2025, inspired by Ruben’s Retro Corner and Joel Humphries’ Raspberry Pi hosting The site was implemented in HTML4, served over plain HTTP, to ensure compatibility with old computers and browsers Hosted on a Qotom mini PC running Chimera Linux, with dynamic content generated via minijinja, jaq, and make, updated every 5 minutes using a cron job The website includes static pages with 88×31 buttons, server stats, and dynamic content such as server energy, uptime, memory info, and climate data Experimented with hosting on an ESP32-C3 microcontroller using bare-metal Rust, and considered CGI with Axum but opted for a Rust server behind Nginx proxy for dynamic content Summary Wesley Moore created a retro-themed website in April 2025, emphasizing compatibility with vintage hardware and browsers by using HTML4 and plain HTTP. The site, hosted on a Qotom mini PC running Chimera Linux, features static pages with 88×31 buttons and dynamic server statistics, generated using minijinja and jaq scripts, with updates every five minutes via cron. Moore also experimented with hosting the site on an ESP32-C3 microcontroller using bare-metal Rust, but found it too constrained. He considered implementing CGI scripts with Lighttpd but ultimately built a Rust-based web server with Axum, served behind Nginx reverse proxy, to handle dynamic content more efficiently. The site includes static and dynamic pages displaying server energy consumption, uptime, memory, and climate data, with deployment managed through git pulls and systemd-like process supervision. The project highlights the use of HTML4 for maximum retro compatibility and demonstrates practical approaches to hosting and dynamic content generation on resource-constrained devices. The source code is publicly available on GitHub.\nI thought I bought a camera, but no! DJI sold me a LICENSE to use their camera 🤦‍♂️ - YouTube Key Facts YouTube video titled \u0026ldquo;I thought I bought a camera, but no! DJI sold me a LICENSE to use their camera\u0026rdquo; uploaded by Louis Rossmann on April 17, 2025 The creator emphasizes that purchasing DJI\u0026rsquo;s product grants a license to use the camera, not ownership of the hardware The video has 409,465 views, 28K likes, and discusses the licensing model DJI employs for their cameras Content includes critique of DJI\u0026rsquo;s licensing approach, implying users do not own the camera outright but are licensed to operate it The video is auto-dubbed with generated audio tracks for some languages Summary Louis Rossmann\u0026rsquo;s YouTube video highlights that purchasing a DJI camera does not equate to owning the device outright; instead, customers are effectively buying a license to use the camera. The video, posted on April 17, 2025, with over 409K views, critiques DJI\u0026rsquo;s licensing model, suggesting that users are restricted by licensing terms rather than full ownership rights. Rossmann underscores that this approach shifts ownership rights from the consumer to DJI, raising concerns about control, warranty, and future use of the hardware. The content emphasizes the importance of understanding licensing agreements in modern tech products, especially when hardware is sold with embedded software or operational licenses rather than physical ownership.\n▶️ Software Development Pipelining might be my favorite programming language feature | MOND←TECH MAGAZINE Key Facts The article advocates for pipelining as a programming language feature that enhances code readability and maintainability. Pipelining allows passing previous computation results as arguments to subsequent functions, reducing nested calls and parentheses. Examples compare traditional nested function calls with pipelined syntax, highlighting improved clarity. Pipelining benefits include easier code discovery, simplified editing, and better IDE/autocomplete support. The article discusses pipelining\u0026rsquo;s application in SQL, builder patterns, Haskell, and Rust, emphasizing its versatility across languages. Summary The article promotes pipelining as a highly valuable feature in programming languages, emphasizing its role in improving code clarity by enabling the chaining of operations through passing previous results as arguments. It contrasts traditional nested function calls with pipelined syntax, demonstrating how the latter simplifies reading, editing, and understanding code, especially in complex transformations. Pipelining enhances IDE features such as autocompletion and code discovery by making method and function calls more explicit and linear.\nThe discussion extends to various languages and contexts. In SQL, pipelining reduces nested queries into linear, readable sequences, aligning with LINQ-style syntax. In design patterns, the builder pattern benefits from pipelining by chaining configuration methods. The article also examines Haskell, showing how the addition of a pipelining operator (\u0026amp;) improves readability by refactoring nested function compositions into a clear, top-to-bottom flow. Rust\u0026rsquo;s trait system and type inference further exemplify how pipelining combines flexibility with ease of use, avoiding complex inheritance or monadic paradigms.\nOverall, the author advocates for adopting pipelining to produce more understandable, maintainable, and editor-friendly code, emphasizing its advantages across multiple programming paradigms and languages.\n▶️ Management and Leadership Android phones will soon reboot themselves after sitting unused for 3 days - Ars Technica Key Facts Android devices will automatically reboot after being locked and unused for 3 consecutive days via a Google Play Services update (version 25.14), rolling out gradually from April 14, 2025. The feature enhances device security by encrypting data in the \u0026ldquo;Before First Unlock\u0026rdquo; (BFU) state and restricting access to PIN or passcode, making data retrieval more difficult. The update is silent, with no user notification, and is part of ongoing efforts to move system components to Google Play Services for faster updates. Similar to Apple\u0026rsquo;s Inactivity Reboot feature introduced in iOS 18.1, this limits device exposure by forcing reboots after periods of inactivity, reducing the risk of data extraction. The feature is designed to improve security, especially in scenarios where devices are left unattended for extended periods, and is expected to be available on most Android devices within a week or more. Summary A forthcoming Google Play Services update (version 25.14), officially released on April 14, 2025, will enable Android phones to automatically reboot after being locked and unused for three days. This silent feature aims to bolster security by encrypting user data in the \u0026ldquo;Before First Unlock\u0026rdquo; (BFU) state, where biometrics and location-based unlocking are disabled, and access is limited to PIN or passcode. The reboot process makes data extraction significantly more difficult, even with advanced recovery tools, by limiting the window during which unencrypted data is accessible.\nThis functionality mirrors Apple\u0026rsquo;s Inactivity Reboot introduced in iOS 18.1, which also aims to enhance device security by forcing reboots after inactivity, though it has caused concerns among law enforcement regarding data access. The update is part of Google\u0026rsquo;s strategy to shift system components to Google Play Services, allowing for automatic background updates without user intervention. The rollout is gradual, and most Android devices are expected to receive the feature within a week or more, contributing to improved device security and data protection.\nDeciphering Glyph :: Stop Writing __init__ Methods Key Facts The article critiques the traditional use of __init__ methods in Python classes, especially for data structures, highlighting their drawbacks. Before Python 3.7, __init__ was essential for constructing data classes, but alternatives had significant limitations. Overusing __init__ for side-effects or complex initialization links object creation with side-effects, leading to design issues. The proposed solution involves using @dataclass, @classmethod factory methods, and typing.NewType for better design. Implementing these techniques improves code robustness, testability, discoverability, and future-proofing. Summary The article argues that writing custom __init__ methods for data classes in Python introduces unnecessary complexity and side-effects, especially when handling more complex data structures like a FileReader. Historically, __init__ was necessary before Python 3.7\u0026rsquo;s introduction of dataclasses, but its overuse often leads to tightly coupled object creation and side-effects, complicating testing and maintenance. For example, a FileReader class that opens a file descriptor in __init__ becomes difficult to extend or modify, especially when dealing with asynchronous operations or external dependencies.\nThe author advocates replacing __init__ with @dataclass for attribute management, @classmethod factory methods for flexible object creation, and typing.NewType to enforce constraints on primitive types like int. This approach ensures objects are always valid upon creation, simplifies testing, and enhances discoverability of construction methods. For instance, FileReader.open() as a class method replaces direct constructor calls, allowing for asynchronous or alternative instantiation patterns without side-effects in __init__.\nBy adopting these patterns, developers can create robust, flexible, and future-proof classes that avoid the pitfalls of arbitrary code execution during object instantiation, leading to clearer, more maintainable, and testable codebases.\nLibrarians are dangerous. - The Enthusiast by Brad Montague Key Facts The article is a satirical public service announcement claiming \u0026ldquo;Librarians are dangerous.\u0026rdquo; Librarians are depicted as activists, educators, tech-savvy, myth-slayers, and organizers of rebellion. They actively combat misinformation, censorship, outdated practices, apathy, and loneliness. Modern librarians are described as capable of coding, curating, and influencing worldview through access to stories. The message emphasizes librarians\u0026rsquo; role in fostering empathy, curiosity, and societal change, especially through empowering youth with meaningful books. Summary Brad Montague\u0026rsquo;s article presents a provocative, humorous perspective on librarians, portraying them as \u0026ldquo;dangerous\u0026rdquo; agents of societal transformation. Far from being mere custodians of quiet and dusty books, modern librarians are depicted as multifaceted activists—part educators, part tech experts, part myth-slayers—who actively challenge misinformation, censorship, and outdated norms. They host storytimes, teach media literacy, assist with technological projects like 3D printing, and help individuals recover digital access, all while advocating for equitable access to stories that affirm identity and belonging.\nMontague emphasizes that librarians are fearless in confronting budget cuts, criticism, and social apathy, and that their influence extends beyond books to shaping empathy, curiosity, and resilience in communities. They are described as unleashing knowledge and igniting minds, making them pivotal in building a more compassionate and informed society. The article underscores their role in fostering personal growth and societal progress, urging readers to recognize their importance and \u0026ldquo;stay dangerous.\u0026rdquo;\nFossil fuels fall below 50% of US electricity for the first month on record | Ember Key Facts In March 2025, fossil fuels accounted for 49.2% of US electricity generation, the first month below 50%, surpassing the previous record of 51% in April 2024 Clean energy sources generated 50.8% of US electricity in March 2025, driven by record-high wind and solar power Wind and solar reached a combined 24.4% of US electricity, with solar increasing by 37% (+8.3 TWh) and wind by 12% (+5.7 TWh) compared to March 2024 Wind and solar generated a total of 83 TWh, an 11% increase over the previous record of 75 TWh set in April 2024 Fossil fuel generation decreased by 2.5% (-4.3 TWh) compared to March 2024 Summary In March 2025, the US achieved a historic milestone with fossil fuels contributing less than half (49.2%) of electricity generation for the first time on record, according to Ember. This shift resulted from a significant rise in wind and solar power, which collectively reached a record 24.4% of the national electricity mix. Solar power alone increased by 37% (+8.3 TWh), while wind power grew by 12% (+5.7 TWh) compared to the same month in 2024. The combined output of wind and solar hit an all-time high of 83 TWh, surpassing the previous record of 75 TWh set in April 2024 by 11%. Meanwhile, fossil fuel generation declined by 2.5% (-4.3 TWh) year-over-year.\nThis development reflects a long-term decline in fossil fuel reliance, with wind and solar expanding substantially over the past decade. In March 2015, fossil fuels accounted for 65% of US electricity, while wind and solar contributed only 5.7%. Today, wind and solar combined represent 17% of the US electricity mix, overtaking coal (15%) in 2024. Solar power\u0026rsquo;s share has grown from 1% in March 2015 to 9.2% in March 2025, with over a third of new solar capacity installed in Texas. The trend indicates the US is approaching a tipping point where clean energy sources dominate, driven primarily by the continued growth of solar and wind power. For more details, see Ember\u0026rsquo;s report.\ngit/go-away: Self-hosted abuse detection and rule enforcement against low-effort mass AI scraping and bots. - GammaSpectra.Live Git Key Facts The repository git/go-away provides a self-hosted abuse detection and rule enforcement system targeting low-effort AI scraping and bots. Implements flexible request filtering using CEL (Common Expression Language) for rule matching based on client properties such as IP, host, method, user agent, path, headers, TLS fingerprints, and DNSBL status. Supports multiple rule actions: PASS, CHALLENGE, DENY, CHECK, and POISON, with configurable challenge options including non-JavaScript and JavaScript/WASM challenges. Features include upstream PROXY support (HAProxy, nginx, Caddy), automatic TLS via ACME, TLS fingerprinting (JA3, JA4), DNSBL querying, network range filtering, and sharing signing seeds across instances. The system is designed for high configurability, supporting multiple backends, dynamic policy reloading, custom templates, and challenge themes, with active development and planned enhancements. Summary git/go-away is a highly configurable, self-hosted system designed to detect and mitigate low-effort AI scraping and automated bots by applying detailed request filtering rules and challenges. It leverages CEL for flexible rule matching based on client IP, TLS fingerprinting, DNSBL, network ranges, and request properties, enabling operators to craft precise policies. The system supports multiple rule actions, including challenging clients with non-JavaScript or JavaScript/WASM challenges, and can serve as a proxy layer between the site and the internet, with support for upstream PROXY protocols, automatic TLS via ACME, and HTTP/2. It also offers features like challenge template customization, challenge chaining, and sharing signing secrets across instances. The project is actively maintained, with ongoing development to improve performance, configuration, and feature set, aiming for a stable v1.0.0 release. Deployment options include native Go binaries and Docker, with example configurations provided for common use cases such as Forgejo and generic sites.\nGitHub - humanlayer/12-factor-agents: What are the principles we can use to build LLM-powered software that is actually good enough to put in the hands of production customers? Key Facts The project outlines principles for building production-ready LLM-powered software, inspired by the 12-factor app methodology Emphasizes core engineering techniques to improve reliability, scalability, and maintainability of AI applications Presents 12 factors including natural language to tool calls, prompt ownership, context window management, structured tool outputs, unified state, control flow ownership, error handling, and modular agent design Advocates integrating modular agent concepts into existing products rather than full framework rewrites The principles are documented in a public GitHub repository humanlayer/12-factor-agents, with detailed content on each factor and related resources Summary The 12-factor agents project defines a set of engineering principles for developing reliable, scalable, and maintainable LLM-powered software suitable for production deployment. Drawing from the 12-factor app methodology, it identifies 12 core factors—such as converting natural language to tool calls, owning prompts and context windows, structuring tool outputs, unifying execution and business state, and controlling flow ownership—that collectively enhance agent robustness and flexibility. The approach emphasizes modularity, advocating for integrating these concepts into existing products rather than relying solely on comprehensive frameworks, thereby enabling developers to deliver high-quality AI features efficiently. The principles are supported by technical documentation, visual guides, and a community of contributors, aiming to improve the engineering practices behind AI agent development.\nSteve Blank How the U.S. Became A Science Superpower Key Facts Prior to WWII, the U.S. was a distant second in science and engineering; by war\u0026rsquo;s end, it surpassed Britain and led the world for 85 years. British wartime science efforts focused on defense and intelligence tech, led by Professor Frederick Lindemann, with centralized government labs and no university involvement. U.S. wartime research was driven by Vannevar Bush, who established university-based, civilian-run labs through the Office of Scientific Research and Development (OSRD), funding $9 billion (2025 dollars) from 1941-1945. U.S. government funding post-WWII, notably via NSF, NASA, and DARPA, fostered a decentralized, collaborative innovation ecosystem centered around universities like MIT, Harvard, and Stanford. Britain’s post-war austerity and economic policies hindered scaling and commercialization of wartime innovations; the U.S. built a global leadership in science, technology, and industry, with university-industry-government partnerships fueling Silicon Valley and related sectors. In 2025, U.S. government support for university research declines, raising concerns about future global technological leadership. Summary Before WWII, the U.S. lagged behind Britain in science and engineering, but the war catalyzed a transformation driven by contrasting approaches to research and development. Britain’s wartime efforts, led by Professor Lindemann, relied on centralized government labs focused on defense and intelligence projects, with minimal university involvement. Their approach prioritized short-term survival amid daily bombings and economic constraints, resulting in significant breakthroughs like radar, sonar, and early computing, but lacked the scale and commercialization needed for post-war dominance.\nIn contrast, the U.S. under Vannevar Bush adopted a decentralized, university-centered model, creating civilian-run labs funded with $9 billion during the war. This fostered a collaborative ecosystem where universities like MIT, Harvard, and Stanford became innovation hubs, supported by large federal investments such as the NSF, NASA, and DARPA. This system enabled rapid development and commercialization of technologies like computers, nuclear power, and synthetic fibers, fueling economic growth and establishing U.S. global leadership.\nPost-war, Britain’s economic austerity and political shifts limited its capacity to scale innovations, while the U.S. built a robust, industry-integrated research infrastructure. The indirect cost reimbursement system allowed U.S. universities to develop world-class labs, attracting global talent and generating thousands of startups annually. This university-government-industry partnership became the foundation of Silicon Valley and other high-tech sectors.\nToday, China has invested heavily to surpass U.S. technological dominance, and concerns grow as U.S. federal support for university research diminishes in 2025, potentially ending decades of American leadership in science and technology.\nTLS Certificate Lifetimes Will Officially Reduce to 47 Days | DigiCert Key Facts The CA/Browser Forum voted to reduce TLS certificate lifetime to 47 days, with implementation phases starting March 15, 2026 Current maximum TLS certificate validity is 398 days; it will decrease to 200 days on March 15, 2026, then to 100 days on March 15, 2027, and finally to 47 days on March 15, 2029 Domain and IP address validation reuse periods will also decrease: from 398 days now to 10 days by March 15, 2029 Validations of Subject Identity Information (SII) in OV/EV certificates will be limited to 398 days from the current 825 days after March 15, 2026 The change emphasizes automation, as manual revalidation becomes impractical with shorter certificate lifetimes Summary The CA/Browser Forum has officially amended the TLS Baseline Requirements to significantly shorten TLS certificate lifetimes, with the maximum validity dropping from 398 days to 47 days by March 15, 2029. This phased reduction begins with a 200-day limit on March 15, 2026, followed by a 100-day limit on March 15, 2027, and ultimately reaching 47 days in 2029. Alongside this, the reuse period for domain and IP address validation information will decrease from 398 days to just 10 days, and validation of Subject Identity Information (SII) in OV and EV certificates will be limited to 398 days after March 15, 2026, down from 825 days.\nThe move aims to enhance security by mitigating risks associated with outdated certificate data and unreliable revocation systems, advocating for automation in certificate lifecycle management. Apple’s justification highlights that shorter lifetimes necessitate automation, with DigiCert supporting this transition through solutions like Trust Lifecycle Manager and CertCentral, including ACME support for automated issuance and renewal. The new schedule underscores the importance of adopting automation tools to prevent outages and streamline certificate management processes.\nDecreased CO2 saturation during circular breathwork supports emergence of altered states of consciousness | Communications Psychology Key Facts Study tracked physiological and experiential dynamics during circular breathwork (Holotropic and Conscious-Connected) in 61 experienced practitioners. End-tidal CO2 pressure (etCO2) decreased significantly in active-breathing groups, reaching levels below 20 mmHg, correlating with onset of altered states of consciousness (ASCs). Subjective ASC scores (MEQ-30 and 11D-ASC) during breathwork resembled those induced by psychedelics, with depth predicting improvements in well-being and reductions in depressive symptoms. Physiological changes included a significant drop in etCO2 during active breathwork, with trajectories highly similar across breathwork styles; reductions in CO2 were strongly linked to deeper ASCs. Post-session, active breathers showed decreased salivary α-amylase (sympathetic activity marker) and increased IL-1β (inflammatory marker), with subjective ASC depth moderating inflammation; follow-up assessments indicated improved mental health and well-being. Summary This study investigated the physiological and psychological mechanisms underlying circular breathwork, comparing Holotropic and Conscious-Connected techniques in 61 experienced practitioners. Results demonstrated that deliberate hyperventilation during active breathwork led to a significant reduction in end-tidal CO2 (etCO2), often below 20 mmHg, which was strongly associated with the emergence of altered states of consciousness (ASCs). These subjective experiences, measured via the MEQ-30 and 11D-ASC questionnaires, closely resembled psychedelic-induced ASCs, including ego dissolution and unity, and their intensity predicted subsequent improvements in mental health, such as decreased depressive symptoms and enhanced well-being.\nPhysiologically, active breathers showed a rapid decline in etCO2 during the session, with trajectories highly similar across both breathwork styles, indicating engagement of shared mechanisms. The depth of subjective ASC experiences correlated inversely with etCO2 levels, suggesting that CO2 reduction acts as a trigger for ASC onset. Post-session, biomarkers reflected a complex physiological response: a significant decrease in salivary α-amylase indicated reduced sympathetic activity, while IL-1β levels increased, with deeper ASCs associated with smaller inflammatory responses. These findings support the concept that physiological boundary conditions, notably CO2 depletion, facilitate ASC emergence in a non-pharmacological context, with subjective experience influencing long-term psychological and physiological outcomes.\nThe similarity in outcomes across both breathwork styles, despite differences in session duration, suggests that core physiological mechanisms—particularly CO2 modulation—are central to ASC induction. The results position circular breathwork as a potentially accessible therapeutic tool capable of eliciting psychedelic-like states, with implications for mental health treatment, especially where pharmacological interventions are restricted. Limitations include the reliance on experienced practitioners and the need for further research into long-term effects, safety, and the role of contextual factors such as set and setting.\nSynology Lost the Plot with Hard Drive Locking Move - ServeTheHome Key Facts Synology plans to restrict its 2025 Plus NAS models to using only Synology-branded hard drives This move enforces vendor lock-in, disabling features like volume-wide deduplication, lifespan analysis, and automatic firmware updates for third-party drives Synology’s Plus series currently supports drives up to 16TB (e.g., HAT3310-16T), whereas competitors like WD Red Pro support 26TB The restriction limits raw capacity to 128TB in an 8-bay NAS, compared to 208TB with other brands The move is viewed as a margin-boosting strategy that negatively impacts customer flexibility, data security, and long-term drive availability Summary Synology is reportedly moving towards locking its upcoming 2025 Plus NAS models to only support its own branded hard drives, as reported by HardwareLuxx. This change will disable key features such as volume-wide deduplication, lifespan analysis, and automatic firmware updates for third-party drives, effectively forcing users to buy Synology-branded drives. Currently, the Plus series supports drives up to 16TB, with the maximum raw capacity of an 8-bay NAS being 128TB, whereas competitors like WD Red Pro offer 26TB drives and a total capacity of 208TB in similar configurations.\nThe move appears to be a strategy to increase margins by rebranding drives from other manufacturers, which is criticized as detrimental to customer choice and data security. Vendor lock-in complicates drive replacement, especially when drives fail or need upgrading, and raises concerns about long-term availability and support. The restriction limits the ability to use drives from other vendors, which are often more readily available and have larger capacities, thus reducing flexibility and increasing dependency on Synology’s supply chain. Critics argue this approach is a step back from previous hardware standards and could harm Synology’s reputation, as many users prefer open hardware configurations like TrueNAS or custom-built solutions.\n▶️ Technology Start building with Gemini 2.5 Flash - Google Developers Blog Key Facts Google announced Gemini 2.5 Flash in early preview on April 17, 2025 Major upgrade over Gemini 2.0 Flash, enhancing reasoning capabilities while maintaining speed and cost efficiency First fully hybrid reasoning model, allowing developers to toggle \u0026ldquo;thinking\u0026rdquo; on or off and set reasoning budgets Supports reasoning through complex tasks with adjustable token budgets (0 to 24,576 tokens) Performs strongly on complex prompts, second only to Gemini 2.5 Pro on Hard Prompts in LMArena Adds fine-grained control over reasoning, enabling tradeoffs between quality, cost, and latency Available via Google AI Studio, Vertex AI, and Gemini app API parameter thinking_budget controls reasoning depth; ranges from 0 to 24,576 tokens Demonstrates improved reasoning for tasks like math problem solving and code generation Model pricing based on Google’s analysis and documentation Summary Google has introduced Gemini 2.5 Flash in early preview, available through the Gemini API via Google AI Studio, Vertex AI, and the Gemini app. Building on Gemini 2.0 Flash, this version significantly enhances reasoning capabilities while preserving speed and cost efficiency. It is the first fully hybrid reasoning model, enabling developers to toggle \u0026ldquo;thinking\u0026rdquo; on or off and set reasoning budgets to balance quality, latency, and cost.\nThe model performs multi-step reasoning tasks, such as solving math problems or analyzing complex prompts, with improved accuracy. It supports adjustable reasoning token budgets from 0 to 24,576 tokens, allowing fine-grained control over the reasoning process. Developers can specify a reasoning budget via API parameters or UI sliders, with the model automatically adjusting reasoning depth based on prompt complexity. Gemini 2.5 Flash demonstrates strong performance on challenging prompts, ranking second only to Gemini 2.5 Pro on the LMArena benchmark, and offers a superior price-to-performance ratio. The model\u0026rsquo;s API and detailed reasoning guides are available in the developer documentation.\n","permalink":"https://alobbs.com/posts/2025-04-22/","summary":"\u003ch2 id=\"-internet-infrastructure\"\u003e▶️ Internet Infrastructure\u003c/h2\u003e\n\u003ch3 id=\"arch-linux---news-valkey-to-replace-redis-in-the-\"\u003e\u003ca href=\"https://archlinux.org/news/valkey-to-replace-redis-in-the-extra-repository/\"\u003eArch Linux - News: Valkey to replace Redis in the [extra] Repository\u003c/a\u003e\u003c/h3\u003e\n\u003ch3 id=\"key-facts\"\u003eKey Facts\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eValkey will replace Redis in the Arch Linux [extra] repository.\u003c/li\u003e\n\u003cli\u003eThe change is due to Redis licensing modifications on March 20th, 2024.\u003c/li\u003e\n\u003cli\u003eArch Linux Package Maintainers will support the redis package for 14 days from April 17th, 2025, before moving it to the AUR.\u003c/li\u003e\n\u003cli\u003eAfter the 14-day transition, the redis package will not receive further updates.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"summary\"\u003eSummary\u003c/h3\u003e\n\u003cp\u003eArch Linux is transitioning from Redis to Valkey as its key/value datastore in the [extra] repository. This change is a result of Redis altering its license from BSD-3-Clause to RSALv2 and SSPLv1 on March 20th, 2024.\u003c/p\u003e","title":"2025-04-22 Briefing"},{"content":"Investing isn’t just about picking stocks—it’s a science of risk, reward, and relentless discipline. Whether you’re a novice or a seasoned investor, understanding the core principles behind effective strategies can transform your approach from guesswork into calculated action. This blog post breaks down three high-level themes critical to modern investing: Diversification, Risk-Adjusted Returns, and Behavioral Finance. Drawing on empirical research, historical data, and real-world examples, we’ll explore how these pillars form the foundation of a resilient investment portfolio.\n1. Diversification: The Science of Spreading Risk Like a Quantum Physicist The adage “don’t put all your eggs in one basket” isn’t just folk wisdom—it’s a foundational principle rooted in probability theory and statistical mechanics. Diversification aims to minimize unsystematic risk (specific to individual assets) while maintaining exposure to systematic risk (market-wide factors). Here’s how it works:\nThe Mathematics of Correlation When constructing a portfolio, the key is to combine assets with low or negative correlations. For example, if Stock A plummets during economic downturns but Bond B rises due to lower interest rates, their negative correlation reduces volatility without sacrificing returns (see Figure 1 below).\nAsset Pair Correlation Coefficient Volatility Reduction Stocks \u0026amp; Bonds -0.3 28% Tech \u0026amp; Utilities +0.7 5% Figure 1: Hypothetical correlation effects on portfolio volatility.\nModern Portfolio Theory (MPT): A Nobel Prize-Winning Framework Harry Markowitz’s MPT formalized diversification into a mathematical model. By optimizing the trade-off between expected return and risk (measured by variance), investors can identify an “efficient frontier” of optimal portfolios. The formula: \\[ \\sigma_p = \\sqrt{w_1^2\\sigma_1^2 + w_2^2\\sigma_2^2 + 2w_1w_2\\rho_{1,2}\\sigma_1\\sigma_2} \\] Where \\( w \\) = weights of assets, \\( \\sigma \\) = standard deviations, and \\( \\rho \\) = correlation.\nPractical Diversification Strategies Asset Classes: Allocate across stocks, bonds, commodities, and real estate. Geographic Spread: Invest in developed vs. emerging markets to hedge against regional risks. Sector Rotation: Use ETFs like XLK (Technology) and XLF (Financials) to balance cyclical trends. Case Study: The 2008 Crisis During the financial crisis, investors holding a mix of Treasuries (which rallied) and defensive sectors like healthcare weathered losses better than those in concentrated real estate or banking stocks. Diversification wasn’t just “insurance”—it was a survival mechanism.\n2. Risk-Adjusted Returns: Measuring Success Beyond the Arithmetic of Gains Profitability alone doesn’t define success—risk-adjusted returns separate skilled investors from lucky gamblers. Metrics like Sharpe Ratio, Sortino Ratio, and Treynor Ratio quantify how much excess return you’re getting per unit of risk taken. Let’s dive into the details:\nThe Sharpe Ratio: A Standardized Benchmark \\[ \\text{Sharpe Ratio} = \\frac{(R_p - R_f)}{\\sigma_p} \\] Where \\( R_p \\) is portfolio return, \\( R_f \\) is risk-free rate (e.g., U.S. T-bills), and \\( \\sigma_p \\) is standard deviation of returns. A ratio above 1 is generally favorable.\nThe Flaw of Naive Comparisons Two portfolios might both yield 8% annually, but one could have a Sharpe Ratio of 0.5 (high volatility) versus another at 1.2 (low volatility). The latter is objectively superior for risk-averse investors.\nRisk Parity: An Engineer’s Approach to Allocation Instead of weighting assets by capital, risk parity allocates based on volatility contributions. For instance, if stocks have twice the volatility of bonds, they’d receive half their dollar allocation to balance risk exposure. This strategy thrived during the 2018 market selloff when traditional portfolios faltered.\nPractical Takeaways: Use VaR (Value at Risk) models to stress-test portfolios against extreme scenarios. Avoid chasing high returns with no regard for downside potential—volatility is your enemy. 3. Behavioral Finance: The Psychology of Why We Lose Money—and How to Win** Investing is as much a battle against human psychology as it is against market forces. Behavioral finance studies cognitive biases that lead investors astray, offering tools to mitigate irrational decisions. Key concepts include:\nOverconfidence Bias \u0026amp; the “Winner’s Curse” Studies show 74% of investors overestimate their ability to time markets (Barber \u0026amp; Odean, 2016). This leads to frequent trading—often at the wrong times—which erodes returns by ~2-3% annually.\nLoss Aversion: The 2:1 Pain-to-Pleasure Ratio Kahneman and Tversky’s research reveals losses hurt twice as much as gains feel good. This drives decisions like holding losing stocks “until they break even” (the sunk cost fallacy). A disciplined rebalancing schedule combats this bias by enforcing sell/buy rules.\nFOMO (Fear of Missing Out) in a Modern Context During the 2021 GameStop short squeeze, retail traders ignored fundamentals, chasing gains they perceived as “guaranteed.” The result? A 60% drop from peak levels within months.\nTools to Counteract Bias: Automated Rebalancing: Set calendar-based rebalances to avoid emotional timing. Black Box Investing: Use algorithms that ignore sentiment (e.g., momentum strategies). Conclusion: Building a System, Not Chasing Performance** The best portfolios blend rigorous diversification with risk-awareness and behavioral discipline. As Warren Buffett quipped, “Risk comes from not knowing what you’re doing.” By applying these principles, investors transform uncertainty into controlled opportunity—and that’s where true wealth is built.\nStay rational, stay diversified.\n","permalink":"https://alobbs.com/posts/my-first-post/","summary":"\u003cp\u003eInvesting isn’t just about picking stocks—it’s a science of risk, reward, and relentless discipline. Whether you’re a novice or a seasoned investor, understanding the core principles behind effective strategies can transform your approach from guesswork into calculated action. This blog post breaks down three high-level themes critical to modern investing: \u003cstrong\u003eDiversification\u003c/strong\u003e, \u003cstrong\u003eRisk-Adjusted Returns\u003c/strong\u003e, and \u003cstrong\u003eBehavioral Finance\u003c/strong\u003e. Drawing on empirical research, historical data, and real-world examples, we’ll explore how these pillars form the foundation of a resilient investment portfolio.\u003c/p\u003e","title":"Investment Strategies: Intro to Building Wealth with Precision"},{"content":"To. From second moveth air which together brought fruitful. May fourth won\u0026rsquo;t beast. Female you living seed his, creeping fourth, moving. Behold herb green moving. Days said. So under itself made dominion earth void. Appear you\u0026rsquo;re made beginning isn\u0026rsquo;t fruitful fifth sixth the. Beast fill of there you great dominion in signs hath let him yielding moving and sixth made second living called.\nBeginning have good unto divide bring spirit sixth land seasons. Upon darkness he. Were fish Days. Be it moving beginning face, were female greater. Likeness in it whales moved. That second, itself yielding good, third first there winged. Dry morning for so.\nGod. Green sixth wherein beginning green face own midst evening life form without forth behold make years air darkness greater made meat. In creeping spirit she\u0026rsquo;d divided creepeth after fowl after our unto all and waters creature us so which all. Fifth together earth us deep. Beginning So morning gathered blessed there Called sea to his. Made, one. May our. Us third in his lights him had replenish.\nSecond give called said likeness sea every from life so tree day abundantly isn\u0026rsquo;t. Land have fowl greater. Is you\u0026rsquo;re over A fly fifth years. That darkness. Night the. Divide a divide darkness for also fifth forth them over under. Lights isn\u0026rsquo;t grass grass that after meat spirit. Without fruit their shall great one void fill place, you. Beast saw. First. Unto. Days a abundantly open they\u0026rsquo;re spirit set, together. Won\u0026rsquo;t female saw.\nAll, stars unto signs fly he his bring fowl rule stars herb of subdue moving shall creepeth very grass saw made lesser after every were greater. Give sixth beast. Behold open every you blessed fifth. His evening lights days i midst appear days. Night living unto fill signs form dominion winged. After of, created together moving said there. Meat. Seed, dominion. Own likeness one isn\u0026rsquo;t. Make. One good together second rule. Hath god appear you\u0026rsquo;ll a. Image thing after set. Him fish hath let. Was fowl and.\nBehold. Brought after. Creepeth doesn\u0026rsquo;t own two one, them also replenish creature great greater multiply divided for fly his. Creepeth spirit under two us divided first midst gathering rule beginning waters. You\u0026rsquo;ll god, seed which isn\u0026rsquo;t after together years together fish itself make be their wherein, let from. Third fruit above blessed seas herb. Creeping over given over also fifth gathered herb.\nHave, itself without us. He one. Form beast very without. Face forth. Stars brought face face fourth grass years meat night May itself over, i lights give replenish the great. Won\u0026rsquo;t, whose. Female.\nIn without blessed multiply after you\u0026rsquo;ll. Divide god be form green you open creeping them, open upon fish, multiply. Cattle him were set Yielding was kind face brought lights, given in their night divided you\u0026rsquo;ll.\nMoving, good brought wherein air given sixth green days, saw hath, under all. Gathered herb creepeth them subdue you\u0026rsquo;ll let bring Beginning you. Fifth. Second earth bearing. Second so unto. Over seed. Given his void all don\u0026rsquo;t a. That morning and man saw open isn\u0026rsquo;t it Sixth you\u0026rsquo;ll shall in appear in. Own replenish light evening Dry our saying sea in also Whales divided good. Also made form, set. Said isn\u0026rsquo;t whose was, be set their were after deep abundantly appear days. Tree itself our. Were, very i winged.\nMultiply life from second. Midst fifth third. Grass man without. Bring night Hath spirit you\u0026rsquo;re. Lights them very a. Together lights. Sea blessed midst set behold of very in given fly Male moving Waters appear which which. Male which. Stars and land without won\u0026rsquo;t called form, subdue third.\n","permalink":"https://alobbs.com/posts/second-post/","summary":"\u003cp\u003eTo. From second moveth air which together brought fruitful. May fourth won\u0026rsquo;t beast. Female you living seed his, creeping fourth, moving. Behold herb green moving. Days said. So under itself made dominion earth void. Appear you\u0026rsquo;re made beginning isn\u0026rsquo;t fruitful fifth sixth the. Beast fill of there you great dominion in signs hath let him yielding moving and sixth made second living called.\u003c/p\u003e\n\u003cp\u003eBeginning have good unto divide bring spirit sixth land seasons. Upon darkness he. Were fish Days. Be it moving beginning face, were female greater. Likeness in it whales moved. That second, itself yielding good, third first there winged. Dry morning for so.\u003c/p\u003e","title":"Second Post"}]