<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>2025-04-22 Briefing | Alvaro's Site</title>
<meta name=keywords content><meta name=description content="‚ñ∂Ô∏è Internet Infrastructure
Arch Linux - News: Valkey to replace Redis in the [extra] Repository
Key Facts

Valkey will replace Redis in the Arch Linux [extra] repository.
The change is due to Redis licensing modifications on March 20th, 2024.
Arch Linux Package Maintainers will support the redis package for 14 days from April 17th, 2025, before moving it to the AUR.
After the 14-day transition, the redis package will not receive further updates.

Summary
Arch Linux is transitioning from Redis to Valkey as its key/value datastore in the [extra] repository. This change is a result of Redis altering its license from BSD-3-Clause to RSALv2 and SSPLv1 on March 20th, 2024."><meta name=author content="Alvaro Lopez Ortega"><link rel=canonical href=https://alobbs.com/posts/2025-04-22/><link crossorigin=anonymous href=/assets/css/stylesheet.f49d66caae9ea0fd43f21f29e71a8d3e284517ed770f2aa86fa012953ad3c9ef.css integrity="sha256-9J1myq6eoP1D8h8p5xqNPihFF+13Dyqob6ASlTrTye8=" rel="preload stylesheet" as=style><link rel=icon href=https://alobbs.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://alobbs.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://alobbs.com/favicon-32x32.png><link rel=apple-touch-icon href=https://alobbs.com/apple-touch-icon.png><link rel=mask-icon href=https://alobbs.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://alobbs.com/posts/2025-04-22/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://alobbs.com/posts/2025-04-22/"><meta property="og:site_name" content="Alvaro's Site"><meta property="og:title" content="2025-04-22 Briefing"><meta property="og:description" content="‚ñ∂Ô∏è Internet Infrastructure Arch Linux - News: Valkey to replace Redis in the [extra] Repository Key Facts Valkey will replace Redis in the Arch Linux [extra] repository. The change is due to Redis licensing modifications on March 20th, 2024. Arch Linux Package Maintainers will support the redis package for 14 days from April 17th, 2025, before moving it to the AUR. After the 14-day transition, the redis package will not receive further updates. Summary Arch Linux is transitioning from Redis to Valkey as its key/value datastore in the [extra] repository. This change is a result of Redis altering its license from BSD-3-Clause to RSALv2 and SSPLv1 on March 20th, 2024."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-04-22T21:51:27+00:00"><meta property="article:modified_time" content="2025-04-22T21:51:27+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="2025-04-22 Briefing"><meta name=twitter:description content="‚ñ∂Ô∏è Internet Infrastructure
Arch Linux - News: Valkey to replace Redis in the [extra] Repository
Key Facts

Valkey will replace Redis in the Arch Linux [extra] repository.
The change is due to Redis licensing modifications on March 20th, 2024.
Arch Linux Package Maintainers will support the redis package for 14 days from April 17th, 2025, before moving it to the AUR.
After the 14-day transition, the redis package will not receive further updates.

Summary
Arch Linux is transitioning from Redis to Valkey as its key/value datastore in the [extra] repository. This change is a result of Redis altering its license from BSD-3-Clause to RSALv2 and SSPLv1 on March 20th, 2024."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://alobbs.com/posts/"},{"@type":"ListItem","position":2,"name":"2025-04-22 Briefing","item":"https://alobbs.com/posts/2025-04-22/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"2025-04-22 Briefing","name":"2025-04-22 Briefing","description":"‚ñ∂Ô∏è Internet Infrastructure Arch Linux - News: Valkey to replace Redis in the [extra] Repository Key Facts Valkey will replace Redis in the Arch Linux [extra] repository. The change is due to Redis licensing modifications on March 20th, 2024. Arch Linux Package Maintainers will support the redis package for 14 days from April 17th, 2025, before moving it to the AUR. After the 14-day transition, the redis package will not receive further updates. Summary Arch Linux is transitioning from Redis to Valkey as its key/value datastore in the [extra] repository. This change is a result of Redis altering its license from BSD-3-Clause to RSALv2 and SSPLv1 on March 20th, 2024.\n","keywords":[],"articleBody":"‚ñ∂Ô∏è Internet Infrastructure Arch Linux - News: Valkey to replace Redis in the [extra] Repository Key Facts Valkey will replace Redis in the Arch Linux [extra] repository. The change is due to Redis licensing modifications on March 20th, 2024. Arch Linux Package Maintainers will support the redis package for 14 days from April 17th, 2025, before moving it to the AUR. After the 14-day transition, the redis package will not receive further updates. Summary Arch Linux is transitioning from Redis to Valkey as its key/value datastore in the [extra] repository. This change is a result of Redis altering its license from BSD-3-Clause to RSALv2 and SSPLv1 on March 20th, 2024.\nThe Arch Linux Package Maintainers will continue to support the redis package for approximately 14 days from April 17th, 2025, to facilitate a smooth transition for users. Following this period, the redis package will be moved to the Arch User Repository (AUR) and will no longer receive updates, effectively becoming deprecated. Users are advised to transition to Valkey as soon as possible to avoid potential issues after the 14-day transition window closes.\nBotnet Part 2: The Web is Broken - Jan Wildeboer‚Äôs Blog Key Facts Companies inject SDKs into iOS, Android, MacOS, and Windows apps to monetize network bandwidth, creating botnets used for web crawling, brute-force attacks, and other malicious activities. Infatica is one such company offering access to millions of residential, static, and mobile IP addresses via SDKs embedded in third-party apps. Trend Micro‚Äôs 2023 research confirms malicious actors repackaging freeware to conduct drive-by downloads of services like Infatica, fueling abusive web scraping. The business model enables the sale of network access through infected devices, contributing to the explosion of bot traffic that hampers small web services. The author advocates that all web scraping should be considered abusive and recommends blocking such traffic to protect web infrastructure. Summary The article highlights the rise of shady business practices where companies recruit app developers to embed SDKs into mobile and desktop applications, monetizing users‚Äô network bandwidth to create large-scale botnets. These botnets are used for aggressive web crawling, AI training data collection, and malicious activities, often causing DDoS-like traffic surges that overwhelm small web servers. Infatica, among other providers, offers access to millions of residential, static, and mobile IP addresses, claiming to carefully monitor command execution within infected apps, though the business model is inherently questionable.\nResearch by Trend Micro in 2023 supports concerns about malicious actors repacking freeware to facilitate drive-by downloads of such proxy services, exacerbating the problem. The widespread use of SDKs that infect user devices and generate illegitimate traffic makes it difficult for users and administrators to detect and block these activities. The author argues that web scraping should be universally regarded as abusive behavior, urging webmasters to implement blocking measures. The proliferation of these practices, driven by AI and the demand for web data, threatens the integrity of the web as originally intended.\nBring Back RSS Feeds to Browsers | JetGirlArt Key Facts The author currently uses Thunderbird to manage RSS feeds. Outlook previously supported RSS feeds but no longer does. The author expresses a preference for native browser RSS feed support similar to bookmark functionality. The author dislikes web-based feed readers due to account requirements and views browser extensions with suspicion. Summary The author reflects on the decline of native RSS feed support in web browsers, currently utilizing Thunderbird as a workaround. Previously, Outlook offered native RSS functionality, which has since been removed. The author laments the loss of a streamlined experience where feeds could be added and accessed similarly to bookmarks, directly within the browser interface.\nThe author expresses dissatisfaction with current alternatives, citing the need for accounts with web-based readers and concerns about the security and installation complexities of browser extensions and open-source projects. The post highlights a desire for a simpler, more integrated solution for accessing and managing RSS feeds, reminiscent of earlier browser capabilities.\n‚ñ∂Ô∏è Open Source Attention K-Mart Shoppers : Free Audio : Free Download, Borrow and Streaming : Internet Archive Key Facts The Internet Archive hosts an audio recording titled ‚ÄúAttention K-Mart Shoppers.‚Äù The recording is a 6-minute, 58-second audio file. It was uploaded on November 16, 2010. The recording is available for free download, borrowing, and streaming. Summary The Internet Archive provides access to an audio recording titled ‚ÄúAttention K-Mart Shoppers.‚Äù The 6 minute, 58 second recording was uploaded on November 16, 2010 and is freely available for download, borrowing, and streaming. The recording‚Äôs content is not further detailed on the archive page.\nTry generating video in Gemini, powered by Veo 2 Key Facts Google introduced video generation in Gemini, powered by Veo 2, available to Google One AI Premium subscribers. Users can transform text prompts into high-resolution, eight-second videos at 720p in MP4 format, landscape 16:9. Gemini‚Äôs Veo 2 model produces cinematic, detailed videos with realistic physics and human motion, supporting diverse styles. Video creation is accessible via selecting Veo 2 from the model dropdown in Gemini; prompts describe scenes for automatic rendering. Sharing is simplified on mobile, enabling quick uploads to TikTok and YouTube Shorts; a monthly creation limit applies. Whisk Animate allows users to turn images into eight-second videos using Veo 2, available in over 60 countries. Safety measures include extensive red teaming, content policies, and embedding SynthID digital watermarks in generated videos. The rollout begins today for Gemini Advanced users globally, with ongoing expansion over the coming weeks. Summary Google has launched new video generation capabilities within the Gemini AI platform, utilizing the Veo 2 model to convert text prompts into high-resolution, cinematic-quality videos. Available to Google One AI Premium subscribers, this feature enables users to create eight-second videos at 720p resolution in MP4 format, supporting a variety of visual styles from realism to fantasy. To generate a video, users select Veo 2 from the model dropdown in Gemini and describe their scene in detail, with the system producing a dynamic visual representation. The process emphasizes ease of sharing, allowing quick uploads to platforms like TikTok and YouTube Shorts.\nIn addition, Whisk Animate extends these capabilities by transforming images into animated videos, accessible in over 60 countries. The system emphasizes safety through rigorous testing, content moderation, and embedding SynthID watermarks in all videos to indicate AI origin. The feature rollout begins today for Gemini Advanced users worldwide, with gradual expansion planned over the next few weeks. More information is available at gemini.google.com.\nA New Form of Verification on Bluesky - Bluesky Key Facts Bluesky introduced a new blue check verification system on April 21, 2025. Over 270,000 accounts have already linked their domain to their Bluesky username. ‚ÄúTrusted Verifiers,‚Äù such as The New York Times, can directly verify accounts, indicated by a scalloped blue check. Bluesky is not currently accepting direct applications for verification but plans to launch a request form in the future. Summary Bluesky has launched a new verification system featuring a standard blue checkmark to indicate authentic and notable accounts. This builds upon the existing system allowing users to link their domain as their username, with over 270,000 accounts already utilizing this feature. In addition to Bluesky-issued checks, the platform is introducing ‚ÄúTrusted Verifiers‚Äù ‚Äì independent organizations authorized to directly verify accounts, denoted by a scalloped blue check.\nWhen a user taps on a verified account‚Äôs checkmark, they can view which organizations have granted verification. Users also have the option to hide verification signals within the app‚Äôs settings. Bluesky is not currently accepting direct verification requests but plans to introduce a request form as the feature stabilizes.\nI left Spotify. What happened next? Key Facts The author transitioned from Spotify to Jellyfin for music management and playback. Initial attempts included local music files, web-based streaming with htmx, and using Apple Music. Self-hosted Jellyfin was set up on an old computer, with apps like Finamp used for offline listening. The author now runs Jellyfin and Immich on a mini PC for media hosting and management. Several third-party clients support Jellyfin, including Finamp, Fintunes, and Manet, facilitating offline access and remote streaming. Summary The author left Spotify and explored various music playback solutions, including traditional media players like Winamp, VLC, and foobar2000, but found them inadequate for browsing and managing large libraries. They built a web-based music player using htmx to stream music locally, which worked initially but lacked offline capabilities, prompting a switch to Apple Music for offline listening despite its storage inefficiencies.\nEventually, the author discovered Jellyfin, an open-source media server that can replace streaming services like Spotify. After a straightforward setup on an old computer, they began using apps such as Finamp and Fintunes to download music for offline use. Motivated by the benefits of self-hosting, they purchased a mini PC to run Jellyfin and other apps like Immich, enabling access to their entire media library from any device. The article emphasizes that self-hosting is accessible even for non-programmers and advocates for a future where users control their media without relying on third-party services, highlighting the open-source nature of these solutions.\nUsing ~/.ssh/authorized keys to decide what the incoming connection can do ‚Äì Dan Langille‚Äôs Other Diary Key Facts The article discusses using ~/.ssh/authorized_keys to specify commands executed upon SSH connection based on the key used It demonstrates restricting rsync access to specific directories with commands like /usr/local/sbin/rrsync -ro /path/ The setup involves multiple SSH keys, each associated with different commands for different backup tasks The author shows how to configure keys for push and pull backup operations, ensuring only authorized actions are performed Highlights the importance of restricting SSH key capabilities for security when managing critical systems like database backups Summary The article explains how ~/.ssh/authorized_keys can be utilized to control the actions permitted during SSH connections by associating specific commands with individual public keys. This approach allows for fine-grained access control, such as restricting rsync operations to read-only access within designated directories using commands like /usr/local/sbin/rrsync -ro /path/. The author provides practical examples, including configuring multiple SSH keys for different backup tasks‚Äîone for pushing database backups from a client to a server, and another for pulling backups from the server to a client‚Äîeach with tailored command restrictions. By assigning distinct keys and commands, the setup ensures that each SSH connection performs only its intended operation, enhancing security for sensitive systems like database backups. The article emphasizes the importance of restricting SSH key capabilities to prevent unauthorized actions while maintaining flexible automation workflows.\nReflections on Unikernels Key Facts Unikernels are single-purpose appliances linking application, kernel drivers, and necessary components into one binary for cloud deployment Based on MirageOS, utilizing OCaml libraries such as TCP/IP, DNS, Xen disk, and network drivers Won the Influential Paper Award at the 2025 ASPLOS conference for their architecture Enable exploration of new OS interfaces by removing legacy layers, leveraging common hypervisor interfaces like Xen netfront, blkfront, virtio-net, and virtio-blk Reduce attack surface due to minimal linking; example includes porting the xenstore service to MirageOS unikernel, increasing security and fault-tolerance with OCaml Irmin database Used in high-security contexts such as QubesOS and MirageOS-based firewall components Facilitate experimentation with OS interfaces, e.g., spawning unikernels in response to network requests with shared memory transfer of TCP state Evolved into various projects like UniKraft, Nanos, and upstream Linux approaches like Unikernel Linux (UKL) Summary Unikernels are specialized, single-binary appliances that embed an application along with necessary kernel drivers and libraries, primarily designed for cloud environments. Originating from MirageOS and supported by OCaml libraries such as TCP/IP, DNS, and Xen device drivers, they enable a minimal attack surface and facilitate security enhancements, exemplified by porting critical services like xenstore into isolated unikernel VMs. Their architecture allows for flexible experimentation with OS interfaces, including dynamic spawning in response to network requests, by leveraging shared memory for state transfer. Unikernels have been recognized with awards and are increasingly adopted in high-assurance systems like QubesOS and in projects such as UniKraft, Nanos, and Linux-based unikernel approaches, supporting features like confidential computing and cross-application isolation.\nPython‚Äôs new t-strings | Dave Peck Key Facts Python 3.14 will officially include t-strings in late 2025 T-strings are a generalization of f-strings, evaluating to string.templatelib.Template objects Templates must be processed before use; str() on a Template does not return the template content Template objects provide .strings and .values properties as tuples, with one more string than value Developers can access detailed interpolation info via .interpolations, including value, expression, conversion, and format_spec Summary Python‚Äôs upcoming version 3.14 will introduce t-strings, a new string templating feature that enhances safety and flexibility in string processing. Unlike f-strings, which are immediately evaluated into strings, t-strings evaluate to string.templatelib.Template objects that require explicit processing before use, enabling safer handling of user input and dynamic content.\nTemplates expose .strings and .values properties as tuples, with .strings containing the literal parts and .values holding interpolated expressions. Developers can also access detailed interpolation data through .interpolations, including the raw expression, conversion flags, and format specifications. T-strings support both literal (t\"foo\") and programmatic instantiation via the Template constructor, which accepts strings and Interpolation objects in any order. The feature aims to improve string safety, especially in contexts like HTML and SQL, and is expected to influence library and framework development, with tooling support anticipated in formatters and IDEs.\nDefold - Official Homepage - Cross platform game engine Key Facts Defold is a free, production-ready, cross-platform game engine with source code available under open source license. Features include visual editor, code editor, Lua scripting, Lua debugger, scene, particle, tilemap editors, supporting both 2D and 3D development. Supports publishing to major platforms: PlayStation¬Æ5, PlayStation¬Æ4, Nintendo Switch, Android, iOS, macOS, Linux, Windows, Steam, HTML5, Facebook, and Q3 2024 Xbox. No setup required; fully featured out of the box with zero-config cloud build for native code. Engine is actively developed with monthly releases; recent updates include Defold 1.9.8 (Mar 10, 2025), 1.9.7 (Feb 3, 2025), and 1.9.6 (Dec 18, 2024). Summary Defold is a free, open-source, cross-platform game engine designed for high-performance game development across multiple platforms including PlayStation¬Æ5, PlayStation¬Æ4, Nintendo Switch, Android, iOS, macOS, Linux, Windows, Steam, HTML5, Facebook, and Xbox (Q3 2024). It offers a comprehensive suite of tools such as a visual editor, code editor, Lua scripting, Lua debugger, scene, particle, and tilemap editors, supporting both 2D and 3D game creation without requiring external setup or configuration. The engine enables developers to use a single code base for all supported platforms, with features like zero-config cloud native build system and native code extension capabilities via the Asset Portal and custom setup options.\nRecent updates demonstrate active development, with monthly releases including version 1.9.8 released on March 10, 2025. Defold emphasizes accessibility, with no licensing fees, royalties, or runtime costs, supported by the Defold Foundation. The platform has a proven track record of game releases across mobile, web, desktop, and consoles, and provides integrated support for analytics, app economy, and game services. Additional resources include tutorials, manuals, API references, and a community forum.\nGemma 3 QAT Models: Bringing state-of-the-Art AI to consumer GPUs - Google Developers Blog Key Facts Google Research announced Gemma 3 models with Quantization-Aware Training (QAT) to enable efficient inference on consumer GPUs Gemma 3 achieves state-of-the-art performance, capable of running on high-end GPUs like NVIDIA H100 using BF16 precision Quantization reduces model VRAM requirements significantly: Gemma 3 27B from 54 GB (BF16) to 14.1 GB (int4); 12B from 24 GB to 6.6 GB; 4B from 8 GB to 2.6 GB; 1B from 2 GB to 0.5 GB QAT involves training models with simulated low-precision operations (~5,000 steps), reducing perplexity drop by 54% at Q4_0 quantization Quantized models are compatible with inference engines like Ollama, llama.cpp, MLX, Gemma.cpp, and can be accessed via Hugging Face and Kaggle Summary Google Research‚Äôs Gemma 3 models, launched last month, deliver state-of-the-art large language model performance optimized for deployment on consumer-grade GPUs through Quantization-Aware Training (QAT). These models maintain high accuracy while dramatically reducing memory requirements, enabling local inference on hardware such as NVIDIA RTX 3090 (24GB VRAM) for the 27B variant, and NVIDIA RTX 4060 Laptop GPU (8GB VRAM) for the 12B variant. Quantization reduces VRAM load by up to 4x, with the 27B model decreasing from 54 GB to 14.1 GB when using int4 precision. QAT involves training with simulated low-precision operations over approximately 5,000 steps, which reduces perplexity degradation by 54% at Q4_0 quantization, preserving model quality.\nThese advancements make powerful AI models accessible on a wide range of devices, including laptops, desktops, and phones. The models are compatible with popular inference tools such as Ollama, llama.cpp, MLX, Gemma.cpp, and are available on platforms like Hugging Face and Kaggle. The initiative aims to democratize AI by enabling high-performance models to run efficiently on hardware with limited VRAM, facilitating local deployment and development.\nGitHub - PiLiDAR/PiLiDAR Key Facts PiLiDAR is a DIY 360¬∞ 3D panorama scanner project with ongoing development status. Core features include a custom serial driver for LDRobot LD06, LD19, or STL27L LiDAR modules, with CRC package integrity check and hardware PWM calibration. It provides 2D live visualization and export of LiDAR data in numpy or CSV formats. The system captures fisheye photos stitched into a 6K spherical map using Hugin panorama stitcher, with automatic exposure and white balance optimization. 3D scene assembly is based on angle and offset data, sampling vertex colors from panorama, with Open3D visualization and export options (PCD, PLY, E57). Supports global registration, ICP fine-tuning, and Poisson surface meshing (slow on Pi4). Summary PiLiDAR is an open-source project aimed at creating a 360¬∞ 3D panorama scanner utilizing a Raspberry Pi 4, Raspberry Pi HQ Camera with ArduCam M12 lens, and LDRobot LiDAR modules (LD06, LD19, STL27L). The system features a custom serial protocol implementation for LiDAR data acquisition at a sampling frequency of 4,500 Hz (LD06) or 21,600 Hz (STL27L), with detailed protocol specifications including packet structure, CRC check, and angle interpolation. Hardware components include a NEMA17 stepper motor with A4988 driver, a planetary reduction gearbox, and power options such as 2x 18650 batteries or a 10,000 mAh USB power bank.\nThe software supports live visualization, data export, and panoramic stitching using Hugin, with options for 3D scene assembly, vertex color sampling, and advanced registration techniques. Additional features include USB power switching via uhubctl, remote Jupyter notebook access, and hardware PWM control for motor and LED management. The LiDAR communication protocol involves a 48-byte package with start character 0x54, data length fixed at 12 points, and includes distance and luminance data per point. The project emphasizes technical precision, with detailed wiring, setup instructions, and troubleshooting guides for serial communication, GPIO, and performance optimization.\nGitHub - ericjenott/Evertop: E-ink IBM XT clone with solar power, ultra low power consumption, and ultra long battery life. Key Facts Evertop is a portable IBM XT clone powered by an 80186 microcontroller, with 1MB RAM Uses an e-ink display with no power when not refreshing, supporting DOS, Minix, Windows up to 3.0 Incorporates a 6V, 6W solar panel, two 10,000mAh batteries, and supports simultaneous charging via solar, buck/boost circuit (2.5-20V), and micro USB Power saving mode enables 200-500 hours of continuous use on a single charge; hibernate and power-off features extend battery life Built-in peripherals include CGA, Hercules, MCGA graphics, serial ports, PS/2 ports, USB, Ethernet, WiFi, LoRA radio, headphone jack, volume control, and detachable keyboard The device features a 5.83-inch 648x480 e-ink display, 3D-printed matte PETG enclosure, and supports external full-sized keyboards Runs nearly all IBM PC/XT compatible DOS software from the 1980s and early 90s The system supports multiple power options, automatic battery monitoring, and user-initiated or automatic hibernate modes The project includes a minimal version (‚ÄúEvertop Min‚Äù) removing keyboard, serial port, Ethernet, LoRA, and reducing battery capacity for lighter, cost-effective off-grid use Powered by C++, C, and Assembly, with extensive peripheral and power management features Summary Evertop is an ultra low-power, solar-powered IBM XT clone built around an ESP32 microcontroller, supporting vintage operating systems such as DOS, Minix, and Windows up to version 3.0. It features a 5.83-inch 648x480 e-ink display that consumes no power when idle, enabling extended battery life. The device includes a built-in keyboard, external PS/2 ports, full CGA, Hercules, and MCGA graphics support, along with serial, USB, Ethernet, WiFi, LoRA radio, headphone jack, and volume control. Its power system combines two 10,000mAh batteries with a 6V, 6W solar panel, and supports charging from solar, buck/boost circuits (2.5-20V), and micro USB simultaneously, with a built-in voltmeter for monitoring.\nPower management features include a power-saving mode capable of running 200-500 hours continuously on a single charge, with options for hibernate and total power shutoff to maximize longevity. The device‚Äôs enclosure is 3D-printed matte PETG plastic, designed for portability and ease of access to internal components. It supports emulation of floppy and hard disk images stored on a 256GB SD card, with emulated systems capable of mounting up to 8GB of storage. The system supports multiple emulated configurations, user-initiated hibernate, and automatic power-off, making it suitable for indefinite off-grid operation with proper sunlight. The project also offers a minimized version (‚ÄúEvertop Min‚Äù) that omits the built-in keyboard, serial ports, Ethernet, LoRA, and reduces battery capacity, maintaining core functionalities for lightweight, cost-effective off-grid computing.\nGitHub - nari-labs/dia: A TTS model capable of generating ultra-realistic dialogue in one pass. Key Facts Dia is a 1.6 billion parameter text-to-speech (TTS) model developed by Nari Labs Capable of generating ultra-realistic dialogue in a single pass, conditioned on transcripts and audio Supports nonverbal expressions such as laughter, coughing, and clearing throat Only supports English language generation; pretrained weights available on Hugging Face Licensed under Apache 2.0; accessible via inference code and demo page Summary Dia is a TTS model with 1.6 billion parameters designed to produce highly realistic dialogue from transcripts in one pass. Developed by Nari Labs, it allows conditioning on audio inputs to control emotion and tone, and can generate nonverbal cues like laughter and coughing. The model is optimized for English speech synthesis, with pretrained weights hosted on Hugging Face. It supports dialogue generation with [S1] and [S2] tags, and features voice cloning capabilities demonstrated through example scripts. Inference on enterprise GPUs achieves real-time audio generation, with approximately 40 tokens per second on an A4000 GPU, requiring around 10GB VRAM. The project is licensed under Apache 2.0 and emphasizes ethical use, forbidding identity misuse, deceptive content, and illegal activities. Users can access a demo via Hugging Face Spaces and join community discussions on Discord. Future plans include Docker support, inference speed optimization, and quantization for memory efficiency.\nGitHub - openai/codex: Lightweight coding agent that runs in your terminal Key Facts OpenAI‚Äôs Codex CLI is a lightweight coding agent designed to run in terminal environments Licensed under Apache-2.0, with 18.9k stars and 1.7k forks on GitHub Supports multiple models and providers, including openai, openrouter, gemini, ollama, mistral, deepseek, xai, groq Implements security modes: Suggest, Auto-Edit, Full Auto, with network-disabled execution in Full Auto Compatible with Node.js 22+, macOS 12+, Ubuntu 20.04+, Windows 11 via WSL2; RAM requirement is at least 4 GB Provides CLI commands such as codex, codex \"prompt\", codex -q \"prompt\", and shell completion scripts Supports configuration via ~/.codex/ in YAML or JSON, including model, approval mode, notifications, and custom instructions Includes features like /diff command for git diffs, and tab completions for file paths Uses Husky and lint-staged for code quality enforcement, with migration to pnpm for dependency management Supports non-interactive/CI mode for headless operation, with verbose debugging via DEBUG=true Incorporates update check flow supporting npm, pnpm, bun, with registry-based version lookup and multi-manager support Uses Nix flakes for reproducible development environments and detailed migration guides for dependency management Active development with frequent commits, PRs, and community contributions Summary OpenAI‚Äôs Codex CLI is an open-source, terminal-based coding agent that enables AI-assisted development workflows. It integrates with various models and providers, offering flexible security modes‚Äîranging from suggestion to full auto-approval‚Äîallowing users to control the agent‚Äôs autonomy. The CLI supports multiple operating systems, including macOS, Linux, and Windows via WSL2, requiring Node.js 22+ and at least 4 GB RAM.\nConfiguration is managed through YAML or JSON files located in ~/.codex/, enabling customization of models, approval modes, notifications, and instructions. The tool provides commands such as codex for interactive use, with options for non-interactive mode suitable for CI pipelines. Features like /diff for git diffs and tab completions enhance usability.\nThe project emphasizes code quality and maintainability, utilizing Husky and lint-staged, with a migration to pnpm for dependency management. It includes an intelligent update check system that supports multiple package managers, fetching version info directly from registries to determine updates. Development is further streamlined with Nix flakes for reproducible environments and comprehensive documentation, including migration guides and usage instructions. The repository is actively maintained, with contributions from a broad community of developers.\n15,000 lines of verified cryptography now in Python | Jonathan Protzenko Key Facts Python‚Äôs hash and HMAC algorithms are now fully implemented using HACL*, a verified cryptographic library, replacing previous implementations. The transition involved integrating approximately 15,000 lines of verified C code into Python‚Äôs repository, with automated updates from upstream HACL*. The update enhances support for additional modes in Blake2, a new API for all Keccak variants of SHA3, and improved error management, including handling allocation failures. The work spanned 2.5 years, involving contributions from developers Aymeric Fromherz, Son Ho, Gregory P. Smith, B√©n√©dikt Tran, and Chris Eibl, supported by the Python and cryptographic communities. The implementation addresses complex streaming API requirements for block algorithms, ensuring safe, generic, and verified handling of various cryptographic modes and states, including optimized HMAC with dual hash states. Summary In November 2022, Jonathan Protzenko proposed replacing Python‚Äôs hash infrastructure with verified cryptography after a CVE in SHA3. As of April 2025, Python now vendors HACL*, a verified cryptographic library, for all default hash and HMAC algorithms, integrating 15,000 lines of verified C code. This transition was seamless for users and involved automating updates from upstream HACL*.\nThe integration required extensive technical work, including implementing new features such as additional Blake2 modes, a comprehensive SHA3 API covering all Keccak variants, and robust error handling, notably for memory allocation failures. The process involved developing a generic, formally verified streaming API for block algorithms using dependent types, accommodating diverse algorithm-specific requirements like pre-input data, variable output lengths, and state retention.\nHandling build complexities, especially with platform-specific optimizations like AVX2 instructions, necessitated refactoring C code to use abstract structs, which was complicated by the auto-generated nature of the code from F* via krml. The project also incorporated rigorous CI testing across 50+ toolchains and architectures, revealing and addressing corner cases such as AVX2-dependent code in HMAC.\nMemory failure propagation was achieved by integrating option types into the verification model, enabling failure detection and propagation through the cryptographic stack. Upstream updates from HACL* are now manageable via a shell script, simplifying maintenance.\nThis large-scale integration demonstrates that verified cryptography is mature enough for production use, providing enhanced security guarantees while meeting engineering standards in a major software project like Python.\nGetting Forked by Microsoft ‚Ä¢ Philip Laine Key Facts Philip Laine‚Äôs open source project Spegel was developed to address container image registry downtime issues without stateful components Microsoft contacted Laine to collaborate on Spegel; later, Microsoft developed Peerd, a forked project with significant code similarities and direct references to Spegel Peerd is a fork of Spegel, maintained by Microsoft under the MIT license, with test cases and code snippets directly copied from Spegel The copied code includes functions and comments identical to Spegel, causing confusion among users and raising concerns about proper attribution Laine‚Äôs Spegel has over 1,700 stars and 14.4 million pulls since its release, despite challenges posed by Microsoft‚Äôs actions and open source licensing dynamics Summary Philip Laine‚Äôs open source project Spegel, designed as a lightweight, non-stateful container image registry solution, was positively received with over 1,700 stars and 14.4 million pulls. Microsoft engaged with Laine to collaborate on Spegel, but later developed Peerd, a peer-to-peer container content distributor for Kubernetes, which is a fork of Spegel. Peerd‚Äôs codebase contains test cases, function signatures, comments, and configuration snippets that are directly copied from Spegel, with no attribution, despite being licensed under MIT, which permits modifications and forks without obligation to credit original authors.\nThis copying has led to confusion among users and challenges for Laine in maintaining an unbiased stance. The incident highlights issues around open source licensing, attribution, and the power imbalance between individual maintainers and large corporations. Laine has considered changing Spegel‚Äôs license and has enabled GitHub Sponsors to support ongoing development, reflecting concerns about the sustainability and recognition of individual open source contributors.\nClaude Code Best Practices \\ Anthropic Key Facts Claude Code is a command line tool for agentic coding, released by Anthropic on April 18, 2025 Designed as a low-level, unopinionated interface providing near-raw model access Supports customization via CLAUDE.md files, environment tuning, and tool allowlists Integrates with shell tools, MCP servers, slash commands, GitHub, Git, Jupyter notebooks, and headless automation Recommends workflows including exploration, planning, testing, iteration, and multi-Claude setups for parallel tasks Summary Claude Code is a research-developed command line utility for agentic coding, offering flexible, scriptable access to Anthropic‚Äôs Claude models without enforcing specific workflows. It emphasizes environment customization through CLAUDE.md files, which document commands, style guidelines, and setup instructions, and can be tuned iteratively to optimize context gathering. Users can manage allowed tools via prompts, configuration files, or CLI flags, and integrate Claude with their shell environment, MCP servers, slash commands, GitHub, and Git for enhanced automation and collaboration.\nClaude Code supports advanced workflows such as multi-instance parallel processing, code verification, and visual iteration with screenshots and mockups. Its headless mode enables automation in CI/CD pipelines, issue triage, and code review. Best practices include providing specific instructions, visual references, file paths, URLs, and frequent course corrections. The system also facilitates data input through copy-paste, piping, file reading, and URL fetching. Overall, Claude Code aims to streamline agentic programming, improve productivity, and enable complex automation across diverse environments.\nNew research backs up what gamers have thought for years: cozy video games can be an antidote to stress and anxiety. Key Facts Recent research supports that cozy video games can reduce stress and anxiety. Studies indicate that playing such games can improve mental health and life satisfaction. A Japanese study (2020‚Äì2022) found that owning a game console and increasing gameplay by one hour daily correlated with reduced psychological distress. Playing casual games like Flower and engaging in mindfulness meditation showed similar stress reduction effects. Video games are being explored as therapeutic tools for conditions like ADHD and end-of-life grief processing. Summary Emerging research confirms that cozy video games, characterized by their relaxing and community-oriented nature, can serve as effective tools for alleviating stress and anxiety. These games, which often feature non-violent challenges, customizable avatars, and social interactions within virtual communities, have gained popularity especially after titles like Nintendo‚Äôs Animal Crossing: New Horizons, released in March 2020 during the COVID-19 pandemic, sold over 13 million units within six weeks. Studies, including a Japanese investigation (2020‚Äì2022), reveal that increased gameplay‚Äîspecifically, an extra hour per day‚Äîcan decrease psychological distress and enhance overall life satisfaction.\nResearch by Hiroyuki Egami suggests that owning a gaming console and engaging in regular gaming can positively impact mental health, countering earlier concerns about gaming disorder listed in the ICD-11. Additionally, a 2021 study by Michael Wong found no significant difference in stress reduction between playing casual games like Flower and practicing mindfulness meditation, indicating that gaming can be a viable form of self-care. Therapeutic applications are also expanding, with investigations into using video games to manage ADHD and process grief, exemplified by titles like Spiritfarer, which explores themes of death and compassion. Overall, the evidence underscores the potential of cozy games not only for entertainment but also as accessible mental health interventions.\nBuilding a Website Fit for 1999 - Wesley Moore Key Facts Wesley Moore built a retro-themed website in April 2025, inspired by Ruben‚Äôs Retro Corner and Joel Humphries‚Äô Raspberry Pi hosting The site was implemented in HTML4, served over plain HTTP, to ensure compatibility with old computers and browsers Hosted on a Qotom mini PC running Chimera Linux, with dynamic content generated via minijinja, jaq, and make, updated every 5 minutes using a cron job The website includes static pages with 88√ó31 buttons, server stats, and dynamic content such as server energy, uptime, memory info, and climate data Experimented with hosting on an ESP32-C3 microcontroller using bare-metal Rust, and considered CGI with Axum but opted for a Rust server behind Nginx proxy for dynamic content Summary Wesley Moore created a retro-themed website in April 2025, emphasizing compatibility with vintage hardware and browsers by using HTML4 and plain HTTP. The site, hosted on a Qotom mini PC running Chimera Linux, features static pages with 88√ó31 buttons and dynamic server statistics, generated using minijinja and jaq scripts, with updates every five minutes via cron. Moore also experimented with hosting the site on an ESP32-C3 microcontroller using bare-metal Rust, but found it too constrained. He considered implementing CGI scripts with Lighttpd but ultimately built a Rust-based web server with Axum, served behind Nginx reverse proxy, to handle dynamic content more efficiently. The site includes static and dynamic pages displaying server energy consumption, uptime, memory, and climate data, with deployment managed through git pulls and systemd-like process supervision. The project highlights the use of HTML4 for maximum retro compatibility and demonstrates practical approaches to hosting and dynamic content generation on resource-constrained devices. The source code is publicly available on GitHub.\nI thought I bought a camera, but no! DJI sold me a LICENSE to use their camera ü§¶‚Äç‚ôÇÔ∏è - YouTube Key Facts YouTube video titled ‚ÄúI thought I bought a camera, but no! DJI sold me a LICENSE to use their camera‚Äù uploaded by Louis Rossmann on April 17, 2025 The creator emphasizes that purchasing DJI‚Äôs product grants a license to use the camera, not ownership of the hardware The video has 409,465 views, 28K likes, and discusses the licensing model DJI employs for their cameras Content includes critique of DJI‚Äôs licensing approach, implying users do not own the camera outright but are licensed to operate it The video is auto-dubbed with generated audio tracks for some languages Summary Louis Rossmann‚Äôs YouTube video highlights that purchasing a DJI camera does not equate to owning the device outright; instead, customers are effectively buying a license to use the camera. The video, posted on April 17, 2025, with over 409K views, critiques DJI‚Äôs licensing model, suggesting that users are restricted by licensing terms rather than full ownership rights. Rossmann underscores that this approach shifts ownership rights from the consumer to DJI, raising concerns about control, warranty, and future use of the hardware. The content emphasizes the importance of understanding licensing agreements in modern tech products, especially when hardware is sold with embedded software or operational licenses rather than physical ownership.\n‚ñ∂Ô∏è Software Development Pipelining might be my favorite programming language feature | MOND‚ÜêTECH MAGAZINE Key Facts The article advocates for pipelining as a programming language feature that enhances code readability and maintainability. Pipelining allows passing previous computation results as arguments to subsequent functions, reducing nested calls and parentheses. Examples compare traditional nested function calls with pipelined syntax, highlighting improved clarity. Pipelining benefits include easier code discovery, simplified editing, and better IDE/autocomplete support. The article discusses pipelining‚Äôs application in SQL, builder patterns, Haskell, and Rust, emphasizing its versatility across languages. Summary The article promotes pipelining as a highly valuable feature in programming languages, emphasizing its role in improving code clarity by enabling the chaining of operations through passing previous results as arguments. It contrasts traditional nested function calls with pipelined syntax, demonstrating how the latter simplifies reading, editing, and understanding code, especially in complex transformations. Pipelining enhances IDE features such as autocompletion and code discovery by making method and function calls more explicit and linear.\nThe discussion extends to various languages and contexts. In SQL, pipelining reduces nested queries into linear, readable sequences, aligning with LINQ-style syntax. In design patterns, the builder pattern benefits from pipelining by chaining configuration methods. The article also examines Haskell, showing how the addition of a pipelining operator (\u0026) improves readability by refactoring nested function compositions into a clear, top-to-bottom flow. Rust‚Äôs trait system and type inference further exemplify how pipelining combines flexibility with ease of use, avoiding complex inheritance or monadic paradigms.\nOverall, the author advocates for adopting pipelining to produce more understandable, maintainable, and editor-friendly code, emphasizing its advantages across multiple programming paradigms and languages.\n‚ñ∂Ô∏è Management and Leadership Android phones will soon reboot themselves after sitting unused for 3 days - Ars Technica Key Facts Android devices will automatically reboot after being locked and unused for 3 consecutive days via a Google Play Services update (version 25.14), rolling out gradually from April 14, 2025. The feature enhances device security by encrypting data in the ‚ÄúBefore First Unlock‚Äù (BFU) state and restricting access to PIN or passcode, making data retrieval more difficult. The update is silent, with no user notification, and is part of ongoing efforts to move system components to Google Play Services for faster updates. Similar to Apple‚Äôs Inactivity Reboot feature introduced in iOS 18.1, this limits device exposure by forcing reboots after periods of inactivity, reducing the risk of data extraction. The feature is designed to improve security, especially in scenarios where devices are left unattended for extended periods, and is expected to be available on most Android devices within a week or more. Summary A forthcoming Google Play Services update (version 25.14), officially released on April 14, 2025, will enable Android phones to automatically reboot after being locked and unused for three days. This silent feature aims to bolster security by encrypting user data in the ‚ÄúBefore First Unlock‚Äù (BFU) state, where biometrics and location-based unlocking are disabled, and access is limited to PIN or passcode. The reboot process makes data extraction significantly more difficult, even with advanced recovery tools, by limiting the window during which unencrypted data is accessible.\nThis functionality mirrors Apple‚Äôs Inactivity Reboot introduced in iOS 18.1, which also aims to enhance device security by forcing reboots after inactivity, though it has caused concerns among law enforcement regarding data access. The update is part of Google‚Äôs strategy to shift system components to Google Play Services, allowing for automatic background updates without user intervention. The rollout is gradual, and most Android devices are expected to receive the feature within a week or more, contributing to improved device security and data protection.\nDeciphering Glyph :: Stop Writing __init__ Methods Key Facts The article critiques the traditional use of __init__ methods in Python classes, especially for data structures, highlighting their drawbacks. Before Python 3.7, __init__ was essential for constructing data classes, but alternatives had significant limitations. Overusing __init__ for side-effects or complex initialization links object creation with side-effects, leading to design issues. The proposed solution involves using @dataclass, @classmethod factory methods, and typing.NewType for better design. Implementing these techniques improves code robustness, testability, discoverability, and future-proofing. Summary The article argues that writing custom __init__ methods for data classes in Python introduces unnecessary complexity and side-effects, especially when handling more complex data structures like a FileReader. Historically, __init__ was necessary before Python 3.7‚Äôs introduction of dataclasses, but its overuse often leads to tightly coupled object creation and side-effects, complicating testing and maintenance. For example, a FileReader class that opens a file descriptor in __init__ becomes difficult to extend or modify, especially when dealing with asynchronous operations or external dependencies.\nThe author advocates replacing __init__ with @dataclass for attribute management, @classmethod factory methods for flexible object creation, and typing.NewType to enforce constraints on primitive types like int. This approach ensures objects are always valid upon creation, simplifies testing, and enhances discoverability of construction methods. For instance, FileReader.open() as a class method replaces direct constructor calls, allowing for asynchronous or alternative instantiation patterns without side-effects in __init__.\nBy adopting these patterns, developers can create robust, flexible, and future-proof classes that avoid the pitfalls of arbitrary code execution during object instantiation, leading to clearer, more maintainable, and testable codebases.\nLibrarians are dangerous. - The Enthusiast by Brad Montague Key Facts The article is a satirical public service announcement claiming ‚ÄúLibrarians are dangerous.‚Äù Librarians are depicted as activists, educators, tech-savvy, myth-slayers, and organizers of rebellion. They actively combat misinformation, censorship, outdated practices, apathy, and loneliness. Modern librarians are described as capable of coding, curating, and influencing worldview through access to stories. The message emphasizes librarians‚Äô role in fostering empathy, curiosity, and societal change, especially through empowering youth with meaningful books. Summary Brad Montague‚Äôs article presents a provocative, humorous perspective on librarians, portraying them as ‚Äúdangerous‚Äù agents of societal transformation. Far from being mere custodians of quiet and dusty books, modern librarians are depicted as multifaceted activists‚Äîpart educators, part tech experts, part myth-slayers‚Äîwho actively challenge misinformation, censorship, and outdated norms. They host storytimes, teach media literacy, assist with technological projects like 3D printing, and help individuals recover digital access, all while advocating for equitable access to stories that affirm identity and belonging.\nMontague emphasizes that librarians are fearless in confronting budget cuts, criticism, and social apathy, and that their influence extends beyond books to shaping empathy, curiosity, and resilience in communities. They are described as unleashing knowledge and igniting minds, making them pivotal in building a more compassionate and informed society. The article underscores their role in fostering personal growth and societal progress, urging readers to recognize their importance and ‚Äústay dangerous.‚Äù\nFossil fuels fall below 50% of US electricity for the first month on record | Ember Key Facts In March 2025, fossil fuels accounted for 49.2% of US electricity generation, the first month below 50%, surpassing the previous record of 51% in April 2024 Clean energy sources generated 50.8% of US electricity in March 2025, driven by record-high wind and solar power Wind and solar reached a combined 24.4% of US electricity, with solar increasing by 37% (+8.3 TWh) and wind by 12% (+5.7 TWh) compared to March 2024 Wind and solar generated a total of 83 TWh, an 11% increase over the previous record of 75 TWh set in April 2024 Fossil fuel generation decreased by 2.5% (-4.3 TWh) compared to March 2024 Summary In March 2025, the US achieved a historic milestone with fossil fuels contributing less than half (49.2%) of electricity generation for the first time on record, according to Ember. This shift resulted from a significant rise in wind and solar power, which collectively reached a record 24.4% of the national electricity mix. Solar power alone increased by 37% (+8.3 TWh), while wind power grew by 12% (+5.7 TWh) compared to the same month in 2024. The combined output of wind and solar hit an all-time high of 83 TWh, surpassing the previous record of 75 TWh set in April 2024 by 11%. Meanwhile, fossil fuel generation declined by 2.5% (-4.3 TWh) year-over-year.\nThis development reflects a long-term decline in fossil fuel reliance, with wind and solar expanding substantially over the past decade. In March 2015, fossil fuels accounted for 65% of US electricity, while wind and solar contributed only 5.7%. Today, wind and solar combined represent 17% of the US electricity mix, overtaking coal (15%) in 2024. Solar power‚Äôs share has grown from 1% in March 2015 to 9.2% in March 2025, with over a third of new solar capacity installed in Texas. The trend indicates the US is approaching a tipping point where clean energy sources dominate, driven primarily by the continued growth of solar and wind power. For more details, see Ember‚Äôs report.\ngit/go-away: Self-hosted abuse detection and rule enforcement against low-effort mass AI scraping and bots. - GammaSpectra.Live Git Key Facts The repository git/go-away provides a self-hosted abuse detection and rule enforcement system targeting low-effort AI scraping and bots. Implements flexible request filtering using CEL (Common Expression Language) for rule matching based on client properties such as IP, host, method, user agent, path, headers, TLS fingerprints, and DNSBL status. Supports multiple rule actions: PASS, CHALLENGE, DENY, CHECK, and POISON, with configurable challenge options including non-JavaScript and JavaScript/WASM challenges. Features include upstream PROXY support (HAProxy, nginx, Caddy), automatic TLS via ACME, TLS fingerprinting (JA3, JA4), DNSBL querying, network range filtering, and sharing signing seeds across instances. The system is designed for high configurability, supporting multiple backends, dynamic policy reloading, custom templates, and challenge themes, with active development and planned enhancements. Summary git/go-away is a highly configurable, self-hosted system designed to detect and mitigate low-effort AI scraping and automated bots by applying detailed request filtering rules and challenges. It leverages CEL for flexible rule matching based on client IP, TLS fingerprinting, DNSBL, network ranges, and request properties, enabling operators to craft precise policies. The system supports multiple rule actions, including challenging clients with non-JavaScript or JavaScript/WASM challenges, and can serve as a proxy layer between the site and the internet, with support for upstream PROXY protocols, automatic TLS via ACME, and HTTP/2. It also offers features like challenge template customization, challenge chaining, and sharing signing secrets across instances. The project is actively maintained, with ongoing development to improve performance, configuration, and feature set, aiming for a stable v1.0.0 release. Deployment options include native Go binaries and Docker, with example configurations provided for common use cases such as Forgejo and generic sites.\nGitHub - humanlayer/12-factor-agents: What are the principles we can use to build LLM-powered software that is actually good enough to put in the hands of production customers? Key Facts The project outlines principles for building production-ready LLM-powered software, inspired by the 12-factor app methodology Emphasizes core engineering techniques to improve reliability, scalability, and maintainability of AI applications Presents 12 factors including natural language to tool calls, prompt ownership, context window management, structured tool outputs, unified state, control flow ownership, error handling, and modular agent design Advocates integrating modular agent concepts into existing products rather than full framework rewrites The principles are documented in a public GitHub repository humanlayer/12-factor-agents, with detailed content on each factor and related resources Summary The 12-factor agents project defines a set of engineering principles for developing reliable, scalable, and maintainable LLM-powered software suitable for production deployment. Drawing from the 12-factor app methodology, it identifies 12 core factors‚Äîsuch as converting natural language to tool calls, owning prompts and context windows, structuring tool outputs, unifying execution and business state, and controlling flow ownership‚Äîthat collectively enhance agent robustness and flexibility. The approach emphasizes modularity, advocating for integrating these concepts into existing products rather than relying solely on comprehensive frameworks, thereby enabling developers to deliver high-quality AI features efficiently. The principles are supported by technical documentation, visual guides, and a community of contributors, aiming to improve the engineering practices behind AI agent development.\nSteve Blank How the U.S. Became A Science Superpower Key Facts Prior to WWII, the U.S. was a distant second in science and engineering; by war‚Äôs end, it surpassed Britain and led the world for 85 years. British wartime science efforts focused on defense and intelligence tech, led by Professor Frederick Lindemann, with centralized government labs and no university involvement. U.S. wartime research was driven by Vannevar Bush, who established university-based, civilian-run labs through the Office of Scientific Research and Development (OSRD), funding $9 billion (2025 dollars) from 1941-1945. U.S. government funding post-WWII, notably via NSF, NASA, and DARPA, fostered a decentralized, collaborative innovation ecosystem centered around universities like MIT, Harvard, and Stanford. Britain‚Äôs post-war austerity and economic policies hindered scaling and commercialization of wartime innovations; the U.S. built a global leadership in science, technology, and industry, with university-industry-government partnerships fueling Silicon Valley and related sectors. In 2025, U.S. government support for university research declines, raising concerns about future global technological leadership. Summary Before WWII, the U.S. lagged behind Britain in science and engineering, but the war catalyzed a transformation driven by contrasting approaches to research and development. Britain‚Äôs wartime efforts, led by Professor Lindemann, relied on centralized government labs focused on defense and intelligence projects, with minimal university involvement. Their approach prioritized short-term survival amid daily bombings and economic constraints, resulting in significant breakthroughs like radar, sonar, and early computing, but lacked the scale and commercialization needed for post-war dominance.\nIn contrast, the U.S. under Vannevar Bush adopted a decentralized, university-centered model, creating civilian-run labs funded with $9 billion during the war. This fostered a collaborative ecosystem where universities like MIT, Harvard, and Stanford became innovation hubs, supported by large federal investments such as the NSF, NASA, and DARPA. This system enabled rapid development and commercialization of technologies like computers, nuclear power, and synthetic fibers, fueling economic growth and establishing U.S. global leadership.\nPost-war, Britain‚Äôs economic austerity and political shifts limited its capacity to scale innovations, while the U.S. built a robust, industry-integrated research infrastructure. The indirect cost reimbursement system allowed U.S. universities to develop world-class labs, attracting global talent and generating thousands of startups annually. This university-government-industry partnership became the foundation of Silicon Valley and other high-tech sectors.\nToday, China has invested heavily to surpass U.S. technological dominance, and concerns grow as U.S. federal support for university research diminishes in 2025, potentially ending decades of American leadership in science and technology.\nTLS Certificate Lifetimes Will Officially Reduce to 47 Days | DigiCert Key Facts The CA/Browser Forum voted to reduce TLS certificate lifetime to 47 days, with implementation phases starting March 15, 2026 Current maximum TLS certificate validity is 398 days; it will decrease to 200 days on March 15, 2026, then to 100 days on March 15, 2027, and finally to 47 days on March 15, 2029 Domain and IP address validation reuse periods will also decrease: from 398 days now to 10 days by March 15, 2029 Validations of Subject Identity Information (SII) in OV/EV certificates will be limited to 398 days from the current 825 days after March 15, 2026 The change emphasizes automation, as manual revalidation becomes impractical with shorter certificate lifetimes Summary The CA/Browser Forum has officially amended the TLS Baseline Requirements to significantly shorten TLS certificate lifetimes, with the maximum validity dropping from 398 days to 47 days by March 15, 2029. This phased reduction begins with a 200-day limit on March 15, 2026, followed by a 100-day limit on March 15, 2027, and ultimately reaching 47 days in 2029. Alongside this, the reuse period for domain and IP address validation information will decrease from 398 days to just 10 days, and validation of Subject Identity Information (SII) in OV and EV certificates will be limited to 398 days after March 15, 2026, down from 825 days.\nThe move aims to enhance security by mitigating risks associated with outdated certificate data and unreliable revocation systems, advocating for automation in certificate lifecycle management. Apple‚Äôs justification highlights that shorter lifetimes necessitate automation, with DigiCert supporting this transition through solutions like Trust Lifecycle Manager and CertCentral, including ACME support for automated issuance and renewal. The new schedule underscores the importance of adopting automation tools to prevent outages and streamline certificate management processes.\nDecreased CO2 saturation during circular breathwork supports emergence of altered states of consciousness | Communications Psychology Key Facts Study tracked physiological and experiential dynamics during circular breathwork (Holotropic and Conscious-Connected) in 61 experienced practitioners. End-tidal CO2 pressure (etCO2) decreased significantly in active-breathing groups, reaching levels below 20 mmHg, correlating with onset of altered states of consciousness (ASCs). Subjective ASC scores (MEQ-30 and 11D-ASC) during breathwork resembled those induced by psychedelics, with depth predicting improvements in well-being and reductions in depressive symptoms. Physiological changes included a significant drop in etCO2 during active breathwork, with trajectories highly similar across breathwork styles; reductions in CO2 were strongly linked to deeper ASCs. Post-session, active breathers showed decreased salivary Œ±-amylase (sympathetic activity marker) and increased IL-1Œ≤ (inflammatory marker), with subjective ASC depth moderating inflammation; follow-up assessments indicated improved mental health and well-being. Summary This study investigated the physiological and psychological mechanisms underlying circular breathwork, comparing Holotropic and Conscious-Connected techniques in 61 experienced practitioners. Results demonstrated that deliberate hyperventilation during active breathwork led to a significant reduction in end-tidal CO2 (etCO2), often below 20 mmHg, which was strongly associated with the emergence of altered states of consciousness (ASCs). These subjective experiences, measured via the MEQ-30 and 11D-ASC questionnaires, closely resembled psychedelic-induced ASCs, including ego dissolution and unity, and their intensity predicted subsequent improvements in mental health, such as decreased depressive symptoms and enhanced well-being.\nPhysiologically, active breathers showed a rapid decline in etCO2 during the session, with trajectories highly similar across both breathwork styles, indicating engagement of shared mechanisms. The depth of subjective ASC experiences correlated inversely with etCO2 levels, suggesting that CO2 reduction acts as a trigger for ASC onset. Post-session, biomarkers reflected a complex physiological response: a significant decrease in salivary Œ±-amylase indicated reduced sympathetic activity, while IL-1Œ≤ levels increased, with deeper ASCs associated with smaller inflammatory responses. These findings support the concept that physiological boundary conditions, notably CO2 depletion, facilitate ASC emergence in a non-pharmacological context, with subjective experience influencing long-term psychological and physiological outcomes.\nThe similarity in outcomes across both breathwork styles, despite differences in session duration, suggests that core physiological mechanisms‚Äîparticularly CO2 modulation‚Äîare central to ASC induction. The results position circular breathwork as a potentially accessible therapeutic tool capable of eliciting psychedelic-like states, with implications for mental health treatment, especially where pharmacological interventions are restricted. Limitations include the reliance on experienced practitioners and the need for further research into long-term effects, safety, and the role of contextual factors such as set and setting.\nSynology Lost the Plot with Hard Drive Locking Move - ServeTheHome Key Facts Synology plans to restrict its 2025 Plus NAS models to using only Synology-branded hard drives This move enforces vendor lock-in, disabling features like volume-wide deduplication, lifespan analysis, and automatic firmware updates for third-party drives Synology‚Äôs Plus series currently supports drives up to 16TB (e.g., HAT3310-16T), whereas competitors like WD Red Pro support 26TB The restriction limits raw capacity to 128TB in an 8-bay NAS, compared to 208TB with other brands The move is viewed as a margin-boosting strategy that negatively impacts customer flexibility, data security, and long-term drive availability Summary Synology is reportedly moving towards locking its upcoming 2025 Plus NAS models to only support its own branded hard drives, as reported by HardwareLuxx. This change will disable key features such as volume-wide deduplication, lifespan analysis, and automatic firmware updates for third-party drives, effectively forcing users to buy Synology-branded drives. Currently, the Plus series supports drives up to 16TB, with the maximum raw capacity of an 8-bay NAS being 128TB, whereas competitors like WD Red Pro offer 26TB drives and a total capacity of 208TB in similar configurations.\nThe move appears to be a strategy to increase margins by rebranding drives from other manufacturers, which is criticized as detrimental to customer choice and data security. Vendor lock-in complicates drive replacement, especially when drives fail or need upgrading, and raises concerns about long-term availability and support. The restriction limits the ability to use drives from other vendors, which are often more readily available and have larger capacities, thus reducing flexibility and increasing dependency on Synology‚Äôs supply chain. Critics argue this approach is a step back from previous hardware standards and could harm Synology‚Äôs reputation, as many users prefer open hardware configurations like TrueNAS or custom-built solutions.\n‚ñ∂Ô∏è Technology Start building with Gemini 2.5 Flash - Google Developers Blog Key Facts Google announced Gemini 2.5 Flash in early preview on April 17, 2025 Major upgrade over Gemini 2.0 Flash, enhancing reasoning capabilities while maintaining speed and cost efficiency First fully hybrid reasoning model, allowing developers to toggle ‚Äúthinking‚Äù on or off and set reasoning budgets Supports reasoning through complex tasks with adjustable token budgets (0 to 24,576 tokens) Performs strongly on complex prompts, second only to Gemini 2.5 Pro on Hard Prompts in LMArena Adds fine-grained control over reasoning, enabling tradeoffs between quality, cost, and latency Available via Google AI Studio, Vertex AI, and Gemini app API parameter thinking_budget controls reasoning depth; ranges from 0 to 24,576 tokens Demonstrates improved reasoning for tasks like math problem solving and code generation Model pricing based on Google‚Äôs analysis and documentation Summary Google has introduced Gemini 2.5 Flash in early preview, available through the Gemini API via Google AI Studio, Vertex AI, and the Gemini app. Building on Gemini 2.0 Flash, this version significantly enhances reasoning capabilities while preserving speed and cost efficiency. It is the first fully hybrid reasoning model, enabling developers to toggle ‚Äúthinking‚Äù on or off and set reasoning budgets to balance quality, latency, and cost.\nThe model performs multi-step reasoning tasks, such as solving math problems or analyzing complex prompts, with improved accuracy. It supports adjustable reasoning token budgets from 0 to 24,576 tokens, allowing fine-grained control over the reasoning process. Developers can specify a reasoning budget via API parameters or UI sliders, with the model automatically adjusting reasoning depth based on prompt complexity. Gemini 2.5 Flash demonstrates strong performance on challenging prompts, ranking second only to Gemini 2.5 Pro on the LMArena benchmark, and offers a superior price-to-performance ratio. The model‚Äôs API and detailed reasoning guides are available in the developer documentation.\n","wordCount":"9494","inLanguage":"en","datePublished":"2025-04-22T21:51:27.909189Z","dateModified":"2025-04-22T21:51:27.909189Z","author":{"@type":"Person","name":"Alvaro Lopez Ortega"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://alobbs.com/posts/2025-04-22/"},"publisher":{"@type":"Organization","name":"Alvaro's Site","logo":{"@type":"ImageObject","url":"https://alobbs.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://alobbs.com/ accesskey=h title="Alvaro's Site (Alt + H)">Alvaro's Site</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://alobbs.com/archives title=Archive><span>Archive</span></a></li><li><a href=https://alobbs.com/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://alobbs.com/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://alobbs.com/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://alobbs.com/>Home</a>&nbsp;¬ª&nbsp;<a href=https://alobbs.com/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">2025-04-22 Briefing</h1><div class=post-meta><span title='2025-04-22 21:51:27.909189 +0000 UTC'>April 22, 2025</span>&nbsp;¬∑&nbsp;45 min&nbsp;¬∑&nbsp;Alvaro Lopez Ortega</div></header><div class=post-content><h2 id=-internet-infrastructure>‚ñ∂Ô∏è Internet Infrastructure<a hidden class=anchor aria-hidden=true href=#-internet-infrastructure>#</a></h2><h3 id=arch-linux---news-valkey-to-replace-redis-in-the-><a href=https://archlinux.org/news/valkey-to-replace-redis-in-the-extra-repository/>Arch Linux - News: Valkey to replace Redis in the [extra] Repository</a><a hidden class=anchor aria-hidden=true href=#arch-linux---news-valkey-to-replace-redis-in-the->#</a></h3><h3 id=key-facts>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts>#</a></h3><ul><li>Valkey will replace Redis in the Arch Linux [extra] repository.</li><li>The change is due to Redis licensing modifications on March 20th, 2024.</li><li>Arch Linux Package Maintainers will support the redis package for 14 days from April 17th, 2025, before moving it to the AUR.</li><li>After the 14-day transition, the redis package will not receive further updates.</li></ul><h3 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h3><p>Arch Linux is transitioning from Redis to Valkey as its key/value datastore in the [extra] repository. This change is a result of Redis altering its license from BSD-3-Clause to RSALv2 and SSPLv1 on March 20th, 2024.</p><p>The Arch Linux Package Maintainers will continue to support the redis package for approximately 14 days from April 17th, 2025, to facilitate a smooth transition for users. Following this period, the redis package will be moved to the Arch User Repository (AUR) and will no longer receive updates, effectively becoming deprecated. Users are advised to transition to Valkey as soon as possible to avoid potential issues after the 14-day transition window closes.</p><hr><h3 id=botnet-part-2-the-web-is-broken---jan-wildeboers-blog><a href=https://jan.wildeboer.net/2025/04/Web-is-Broken-Botnet-Part-2/>Botnet Part 2: The Web is Broken - Jan Wildeboer‚Äôs Blog</a><a hidden class=anchor aria-hidden=true href=#botnet-part-2-the-web-is-broken---jan-wildeboers-blog>#</a></h3><h3 id=key-facts-1>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-1>#</a></h3><ul><li>Companies inject SDKs into iOS, Android, MacOS, and Windows apps to monetize network bandwidth, creating botnets used for web crawling, brute-force attacks, and other malicious activities.</li><li>Infatica is one such company offering access to millions of residential, static, and mobile IP addresses via SDKs embedded in third-party apps.</li><li>Trend Micro&rsquo;s 2023 research confirms malicious actors repackaging freeware to conduct drive-by downloads of services like Infatica, fueling abusive web scraping.</li><li>The business model enables the sale of network access through infected devices, contributing to the explosion of bot traffic that hampers small web services.</li><li>The author advocates that all web scraping should be considered abusive and recommends blocking such traffic to protect web infrastructure.</li></ul><h3 id=summary-1>Summary<a hidden class=anchor aria-hidden=true href=#summary-1>#</a></h3><p>The article highlights the rise of shady business practices where companies recruit app developers to embed SDKs into mobile and desktop applications, monetizing users&rsquo; network bandwidth to create large-scale botnets. These botnets are used for aggressive web crawling, AI training data collection, and malicious activities, often causing DDoS-like traffic surges that overwhelm small web servers. Infatica, among other providers, offers access to millions of residential, static, and mobile IP addresses, claiming to carefully monitor command execution within infected apps, though the business model is inherently questionable.</p><p>Research by Trend Micro in 2023 supports concerns about malicious actors repacking freeware to facilitate drive-by downloads of such proxy services, exacerbating the problem. The widespread use of SDKs that infect user devices and generate illegitimate traffic makes it difficult for users and administrators to detect and block these activities. The author argues that web scraping should be universally regarded as abusive behavior, urging webmasters to implement blocking measures. The proliferation of these practices, driven by AI and the demand for web data, threatens the integrity of the web as originally intended.</p><hr><h3 id=bring-back-rss-feeds-to-browsers--jetgirlart><a href=https://jetgirl.art/bring-back-rss-feeds-to-browsers/>Bring Back RSS Feeds to Browsers | JetGirlArt</a><a hidden class=anchor aria-hidden=true href=#bring-back-rss-feeds-to-browsers--jetgirlart>#</a></h3><h3 id=key-facts-2>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-2>#</a></h3><ul><li>The author currently uses Thunderbird to manage RSS feeds.</li><li>Outlook previously supported RSS feeds but no longer does.</li><li>The author expresses a preference for native browser RSS feed support similar to bookmark functionality.</li><li>The author dislikes web-based feed readers due to account requirements and views browser extensions with suspicion.</li></ul><h3 id=summary-2>Summary<a hidden class=anchor aria-hidden=true href=#summary-2>#</a></h3><p>The author reflects on the decline of native RSS feed support in web browsers, currently utilizing Thunderbird as a workaround. Previously, Outlook offered native RSS functionality, which has since been removed. The author laments the loss of a streamlined experience where feeds could be added and accessed similarly to bookmarks, directly within the browser interface.</p><p>The author expresses dissatisfaction with current alternatives, citing the need for accounts with web-based readers and concerns about the security and installation complexities of browser extensions and open-source projects. The post highlights a desire for a simpler, more integrated solution for accessing and managing RSS feeds, reminiscent of earlier browser capabilities.</p><h2 id=-open-source>‚ñ∂Ô∏è Open Source<a hidden class=anchor aria-hidden=true href=#-open-source>#</a></h2><h3 id=attention-k-mart-shoppers--free-audio--free-download-borrow-and-streaming--internet-archive><a href=https://archive.org/details/attentionkmartshoppers>Attention K-Mart Shoppers : Free Audio : Free Download, Borrow and Streaming : Internet Archive</a><a hidden class=anchor aria-hidden=true href=#attention-k-mart-shoppers--free-audio--free-download-borrow-and-streaming--internet-archive>#</a></h3><h3 id=key-facts-3>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-3>#</a></h3><ul><li>The Internet Archive hosts an audio recording titled &ldquo;Attention K-Mart Shoppers.&rdquo;</li><li>The recording is a 6-minute, 58-second audio file.</li><li>It was uploaded on November 16, 2010.</li><li>The recording is available for free download, borrowing, and streaming.</li></ul><h3 id=summary-3>Summary<a hidden class=anchor aria-hidden=true href=#summary-3>#</a></h3><p>The Internet Archive provides access to an audio recording titled ‚ÄúAttention K-Mart Shoppers.‚Äù The 6 minute, 58 second recording was uploaded on November 16, 2010 and is freely available for <a href=https://archive.org/details/attentionkmartshoppers/Attention+K-Mart+Shoppers.mp3>download</a>, <a href=https://archive.org/details/attentionkmartshoppers>borrowing</a>, and <a href=https://archive.org/details/attentionkmartshoppers>streaming</a>. The recording&rsquo;s content is not further detailed on the archive page.</p><hr><h3 id=try-generating-video-in-gemini-powered-by-veo-2><a href=https://blog.google/products/gemini/video-generation/>Try generating video in Gemini, powered by Veo 2</a><a hidden class=anchor aria-hidden=true href=#try-generating-video-in-gemini-powered-by-veo-2>#</a></h3><h3 id=key-facts-4>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-4>#</a></h3><ul><li>Google introduced video generation in Gemini, powered by Veo 2, available to Google One AI Premium subscribers.</li><li>Users can transform text prompts into high-resolution, eight-second videos at 720p in MP4 format, landscape 16:9.</li><li>Gemini&rsquo;s Veo 2 model produces cinematic, detailed videos with realistic physics and human motion, supporting diverse styles.</li><li>Video creation is accessible via selecting Veo 2 from the model dropdown in Gemini; prompts describe scenes for automatic rendering.</li><li>Sharing is simplified on mobile, enabling quick uploads to TikTok and YouTube Shorts; a monthly creation limit applies.</li><li>Whisk Animate allows users to turn images into eight-second videos using Veo 2, available in over 60 countries.</li><li>Safety measures include extensive red teaming, content policies, and embedding SynthID digital watermarks in generated videos.</li><li>The rollout begins today for Gemini Advanced users globally, with ongoing expansion over the coming weeks.</li></ul><h3 id=summary-4>Summary<a hidden class=anchor aria-hidden=true href=#summary-4>#</a></h3><p>Google has launched new video generation capabilities within the Gemini AI platform, utilizing the Veo 2 model to convert text prompts into high-resolution, cinematic-quality videos. Available to Google One AI Premium subscribers, this feature enables users to create eight-second videos at 720p resolution in MP4 format, supporting a variety of visual styles from realism to fantasy. To generate a video, users select Veo 2 from the model dropdown in Gemini and describe their scene in detail, with the system producing a dynamic visual representation. The process emphasizes ease of sharing, allowing quick uploads to platforms like TikTok and YouTube Shorts.</p><p>In addition, Whisk Animate extends these capabilities by transforming images into animated videos, accessible in over 60 countries. The system emphasizes safety through rigorous testing, content moderation, and embedding SynthID watermarks in all videos to indicate AI origin. The feature rollout begins today for Gemini Advanced users worldwide, with gradual expansion planned over the next few weeks. More information is available at <a href=http://gemini.google.com/>gemini.google.com</a>.</p><hr><h3 id=a-new-form-of-verification-on-bluesky---bluesky><a href=https://bsky.social/about/blog/04-21-2025-verification>A New Form of Verification on Bluesky - Bluesky</a><a hidden class=anchor aria-hidden=true href=#a-new-form-of-verification-on-bluesky---bluesky>#</a></h3><h3 id=key-facts-5>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-5>#</a></h3><ul><li>Bluesky introduced a new blue check verification system on April 21, 2025.</li><li>Over 270,000 accounts have already linked their domain to their Bluesky username.</li><li>‚ÄúTrusted Verifiers,‚Äù such as The New York Times, can directly verify accounts, indicated by a scalloped blue check.</li><li>Bluesky is not currently accepting direct applications for verification but plans to launch a request form in the future.</li></ul><h3 id=summary-5>Summary<a hidden class=anchor aria-hidden=true href=#summary-5>#</a></h3><p>Bluesky has launched a new verification system featuring a standard blue checkmark to indicate authentic and notable accounts. This builds upon the existing system allowing users to link their domain as their username, with over 270,000 accounts already utilizing this feature. In addition to Bluesky-issued checks, the platform is introducing ‚ÄúTrusted Verifiers‚Äù ‚Äì independent organizations authorized to directly verify accounts, denoted by a scalloped blue check.</p><p>When a user taps on a verified account‚Äôs checkmark, they can view which organizations have granted verification. Users also have the option to hide verification signals within the app‚Äôs settings. Bluesky is not currently accepting direct verification requests but plans to introduce a request form as the feature stabilizes.</p><hr><h3 id=i-left-spotify-what-happened-next><a href=https://coppolaemilio.com/entries/i-left-spotify-what-happened-next/>I left Spotify. What happened next?</a><a hidden class=anchor aria-hidden=true href=#i-left-spotify-what-happened-next>#</a></h3><h3 id=key-facts-6>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-6>#</a></h3><ul><li>The author transitioned from Spotify to Jellyfin for music management and playback.</li><li>Initial attempts included local music files, web-based streaming with htmx, and using Apple Music.</li><li>Self-hosted Jellyfin was set up on an old computer, with apps like Finamp used for offline listening.</li><li>The author now runs Jellyfin and Immich on a mini PC for media hosting and management.</li><li>Several third-party clients support Jellyfin, including Finamp, Fintunes, and Manet, facilitating offline access and remote streaming.</li></ul><h3 id=summary-6>Summary<a hidden class=anchor aria-hidden=true href=#summary-6>#</a></h3><p>The author left Spotify and explored various music playback solutions, including traditional media players like Winamp, VLC, and foobar2000, but found them inadequate for browsing and managing large libraries. They built a web-based music player using htmx to stream music locally, which worked initially but lacked offline capabilities, prompting a switch to Apple Music for offline listening despite its storage inefficiencies.</p><p>Eventually, the author discovered Jellyfin, an open-source media server that can replace streaming services like Spotify. After a straightforward setup on an old computer, they began using apps such as Finamp and Fintunes to download music for offline use. Motivated by the benefits of self-hosting, they purchased a mini PC to run Jellyfin and other apps like Immich, enabling access to their entire media library from any device. The article emphasizes that self-hosting is accessible even for non-programmers and advocates for a future where users control their media without relying on third-party services, highlighting the open-source nature of these solutions.</p><hr><h3 id=using-><a href=https://dan.langille.org/2025/04/17/using-ssh-authorized-keys-to-decide-what-the-incoming-connection-can-do/>Using ~/.ssh/authorized keys to decide what the incoming connection can do ‚Äì Dan Langille&rsquo;s Other Diary</a><a hidden class=anchor aria-hidden=true href=#using->#</a></h3><h3 id=key-facts-7>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-7>#</a></h3><ul><li>The article discusses using ~/.ssh/authorized_keys to specify commands executed upon SSH connection based on the key used</li><li>It demonstrates restricting rsync access to specific directories with commands like <code>/usr/local/sbin/rrsync -ro /path/</code></li><li>The setup involves multiple SSH keys, each associated with different commands for different backup tasks</li><li>The author shows how to configure keys for push and pull backup operations, ensuring only authorized actions are performed</li><li>Highlights the importance of restricting SSH key capabilities for security when managing critical systems like database backups</li></ul><h3 id=summary-7>Summary<a hidden class=anchor aria-hidden=true href=#summary-7>#</a></h3><p>The article explains how ~/.ssh/authorized_keys can be utilized to control the actions permitted during SSH connections by associating specific commands with individual public keys. This approach allows for fine-grained access control, such as restricting rsync operations to read-only access within designated directories using commands like <code>/usr/local/sbin/rrsync -ro /path/</code>. The author provides practical examples, including configuring multiple SSH keys for different backup tasks‚Äîone for pushing database backups from a client to a server, and another for pulling backups from the server to a client‚Äîeach with tailored command restrictions. By assigning distinct keys and commands, the setup ensures that each SSH connection performs only its intended operation, enhancing security for sensitive systems like database backups. The article emphasizes the importance of restricting SSH key capabilities to prevent unauthorized actions while maintaining flexible automation workflows.</p><hr><h3 id=reflections-on-unikernels><a href=https://dave.recoil.org/unikernels/>Reflections on Unikernels</a><a hidden class=anchor aria-hidden=true href=#reflections-on-unikernels>#</a></h3><h3 id=key-facts-8>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-8>#</a></h3><ul><li>Unikernels are single-purpose appliances linking application, kernel drivers, and necessary components into one binary for cloud deployment</li><li>Based on MirageOS, utilizing OCaml libraries such as TCP/IP, DNS, Xen disk, and network drivers</li><li>Won the Influential Paper Award at the 2025 ASPLOS conference for their architecture</li><li>Enable exploration of new OS interfaces by removing legacy layers, leveraging common hypervisor interfaces like Xen <code>netfront</code>, <code>blkfront</code>, <code>virtio-net</code>, and <code>virtio-blk</code></li><li>Reduce attack surface due to minimal linking; example includes porting the xenstore service to MirageOS unikernel, increasing security and fault-tolerance with OCaml Irmin database</li><li>Used in high-security contexts such as QubesOS and MirageOS-based firewall components</li><li>Facilitate experimentation with OS interfaces, e.g., spawning unikernels in response to network requests with shared memory transfer of TCP state</li><li>Evolved into various projects like <a href=https://github.com/unikraft/unikraft>UniKraft</a>, <a href=https://github.com/nanovms/nanos>Nanos</a>, and upstream Linux approaches like <a href=https://dl.acm.org/doi/10.1145/3552326.3587458>Unikernel Linux (UKL)</a></li></ul><h3 id=summary-8>Summary<a hidden class=anchor aria-hidden=true href=#summary-8>#</a></h3><p>Unikernels are specialized, single-binary appliances that embed an application along with necessary kernel drivers and libraries, primarily designed for cloud environments. Originating from MirageOS and supported by OCaml libraries such as TCP/IP, DNS, and Xen device drivers, they enable a minimal attack surface and facilitate security enhancements, exemplified by porting critical services like xenstore into isolated unikernel VMs. Their architecture allows for flexible experimentation with OS interfaces, including dynamic spawning in response to network requests, by leveraging shared memory for state transfer. Unikernels have been recognized with awards and are increasingly adopted in high-assurance systems like QubesOS and in projects such as UniKraft, Nanos, and Linux-based unikernel approaches, supporting features like confidential computing and cross-application isolation.</p><hr><h3 id=python><a href=https://davepeck.org/2025/04/11/pythons-new-t-strings/>Python&rsquo;s new t-strings | Dave Peck</a><a hidden class=anchor aria-hidden=true href=#python>#</a></h3><h3 id=key-facts-9>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-9>#</a></h3><ul><li>Python 3.14 will officially include <a href=https://peps.python.org/pep-0750/>t-strings</a> in late 2025</li><li>T-strings are a generalization of f-strings, evaluating to <code>string.templatelib.Template</code> objects</li><li>Templates must be processed before use; <code>str()</code> on a Template does not return the template content</li><li><code>Template</code> objects provide <code>.strings</code> and <code>.values</code> properties as tuples, with one more string than value</li><li>Developers can access detailed interpolation info via <code>.interpolations</code>, including <code>value</code>, <code>expression</code>, <code>conversion</code>, and <code>format_spec</code></li></ul><h3 id=summary-9>Summary<a hidden class=anchor aria-hidden=true href=#summary-9>#</a></h3><p>Python&rsquo;s upcoming <a href=https://peps.python.org/pep-0750/>version 3.14</a> will introduce <a href=https://davepeck.org/2025/04/11/pythons-new-t-strings/>t-strings</a>, a new string templating feature that enhances safety and flexibility in string processing. Unlike f-strings, which are immediately evaluated into strings, t-strings evaluate to <code>string.templatelib.Template</code> objects that require explicit processing before use, enabling safer handling of user input and dynamic content.</p><p>Templates expose <code>.strings</code> and <code>.values</code> properties as tuples, with <code>.strings</code> containing the literal parts and <code>.values</code> holding interpolated expressions. Developers can also access detailed interpolation data through <code>.interpolations</code>, including the raw expression, conversion flags, and format specifications. T-strings support both literal (<code>t"foo"</code>) and programmatic instantiation via the <code>Template</code> constructor, which accepts strings and <code>Interpolation</code> objects in any order. The feature aims to improve string safety, especially in contexts like HTML and SQL, and is expected to influence library and framework development, with tooling support anticipated in formatters and IDEs.</p><hr><h3 id=defold---official-homepage---cross-platform-game-engine><a href=https://defold.com/>Defold - Official Homepage - Cross platform game engine</a><a hidden class=anchor aria-hidden=true href=#defold---official-homepage---cross-platform-game-engine>#</a></h3><h3 id=key-facts-10>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-10>#</a></h3><ul><li>Defold is a free, production-ready, cross-platform game engine with source code available under open source license.</li><li>Features include visual editor, code editor, Lua scripting, Lua debugger, scene, particle, tilemap editors, supporting both 2D and 3D development.</li><li>Supports publishing to major platforms: PlayStation¬Æ5, PlayStation¬Æ4, Nintendo Switch, Android, iOS, macOS, Linux, Windows, Steam, HTML5, Facebook, and Q3 2024 Xbox.</li><li>No setup required; fully featured out of the box with zero-config cloud build for native code.</li><li>Engine is actively developed with monthly releases; recent updates include Defold 1.9.8 (Mar 10, 2025), 1.9.7 (Feb 3, 2025), and 1.9.6 (Dec 18, 2024).</li></ul><h3 id=summary-10>Summary<a hidden class=anchor aria-hidden=true href=#summary-10>#</a></h3><p>Defold is a free, open-source, cross-platform game engine designed for high-performance game development across multiple platforms including PlayStation¬Æ5, PlayStation¬Æ4, Nintendo Switch, Android, iOS, macOS, Linux, Windows, Steam, HTML5, Facebook, and Xbox (Q3 2024). It offers a comprehensive suite of tools such as a visual editor, code editor, Lua scripting, Lua debugger, scene, particle, and tilemap editors, supporting both 2D and 3D game creation without requiring external setup or configuration. The engine enables developers to use a single code base for all supported platforms, with features like zero-config cloud native build system and native code extension capabilities via the Asset Portal and custom setup options.</p><p>Recent updates demonstrate active development, with monthly releases including version 1.9.8 released on March 10, 2025. Defold emphasizes accessibility, with no licensing fees, royalties, or runtime costs, supported by the Defold Foundation. The platform has a proven track record of game releases across mobile, web, desktop, and consoles, and provides integrated support for analytics, app economy, and game services. Additional resources include tutorials, manuals, API references, and a community forum.</p><hr><h3 id=gemma-3-qat-models-bringing-state-of-the-art-ai-to-consumer-gpus---google-developers-blog><a href=https://developers.googleblog.com/en/gemma-3-quantized-aware-trained-state-of-the-art-ai-to-consumer-gpus/>Gemma 3 QAT Models: Bringing state-of-the-Art AI to consumer GPUs - Google Developers Blog</a><a hidden class=anchor aria-hidden=true href=#gemma-3-qat-models-bringing-state-of-the-art-ai-to-consumer-gpus---google-developers-blog>#</a></h3><h3 id=key-facts-11>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-11>#</a></h3><ul><li>Google Research announced Gemma 3 models with Quantization-Aware Training (QAT) to enable efficient inference on consumer GPUs</li><li>Gemma 3 achieves state-of-the-art performance, capable of running on high-end GPUs like NVIDIA H100 using BF16 precision</li><li>Quantization reduces model VRAM requirements significantly: Gemma 3 27B from 54 GB (BF16) to 14.1 GB (int4); 12B from 24 GB to 6.6 GB; 4B from 8 GB to 2.6 GB; 1B from 2 GB to 0.5 GB</li><li>QAT involves training models with simulated low-precision operations (~5,000 steps), reducing perplexity drop by 54% at Q4_0 quantization</li><li>Quantized models are compatible with inference engines like Ollama, llama.cpp, MLX, Gemma.cpp, and can be accessed via Hugging Face and Kaggle</li></ul><h3 id=summary-11>Summary<a hidden class=anchor aria-hidden=true href=#summary-11>#</a></h3><p>Google Research&rsquo;s Gemma 3 models, launched last month, deliver state-of-the-art large language model performance optimized for deployment on consumer-grade GPUs through Quantization-Aware Training (QAT). These models maintain high accuracy while dramatically reducing memory requirements, enabling local inference on hardware such as NVIDIA RTX 3090 (24GB VRAM) for the 27B variant, and NVIDIA RTX 4060 Laptop GPU (8GB VRAM) for the 12B variant. Quantization reduces VRAM load by up to 4x, with the 27B model decreasing from 54 GB to 14.1 GB when using int4 precision. QAT involves training with simulated low-precision operations over approximately 5,000 steps, which reduces perplexity degradation by 54% at Q4_0 quantization, preserving model quality.</p><p>These advancements make powerful AI models accessible on a wide range of devices, including laptops, desktops, and phones. The models are compatible with popular inference tools such as Ollama, llama.cpp, MLX, Gemma.cpp, and are available on platforms like Hugging Face and Kaggle. The initiative aims to democratize AI by enabling high-performance models to run efficiently on hardware with limited VRAM, facilitating local deployment and development.</p><hr><h3 id=github---pilidarpilidar><a href=https://github.com/PiLiDAR/PiLiDAR>GitHub - PiLiDAR/PiLiDAR</a><a hidden class=anchor aria-hidden=true href=#github---pilidarpilidar>#</a></h3><h3 id=key-facts-12>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-12>#</a></h3><ul><li>PiLiDAR is a DIY 360¬∞ 3D panorama scanner project with ongoing development status.</li><li>Core features include a custom serial driver for LDRobot LD06, LD19, or STL27L LiDAR modules, with CRC package integrity check and hardware PWM calibration.</li><li>It provides 2D live visualization and export of LiDAR data in numpy or CSV formats.</li><li>The system captures fisheye photos stitched into a 6K spherical map using Hugin panorama stitcher, with automatic exposure and white balance optimization.</li><li>3D scene assembly is based on angle and offset data, sampling vertex colors from panorama, with Open3D visualization and export options (PCD, PLY, E57). Supports global registration, ICP fine-tuning, and Poisson surface meshing (slow on Pi4).</li></ul><h3 id=summary-12>Summary<a hidden class=anchor aria-hidden=true href=#summary-12>#</a></h3><p>PiLiDAR is an open-source project aimed at creating a 360¬∞ 3D panorama scanner utilizing a Raspberry Pi 4, Raspberry Pi HQ Camera with ArduCam M12 lens, and LDRobot LiDAR modules (LD06, LD19, STL27L). The system features a custom serial protocol implementation for LiDAR data acquisition at a sampling frequency of 4,500 Hz (LD06) or 21,600 Hz (STL27L), with detailed protocol specifications including packet structure, CRC check, and angle interpolation. Hardware components include a NEMA17 stepper motor with A4988 driver, a planetary reduction gearbox, and power options such as 2x 18650 batteries or a 10,000 mAh USB power bank.</p><p>The software supports live visualization, data export, and panoramic stitching using Hugin, with options for 3D scene assembly, vertex color sampling, and advanced registration techniques. Additional features include USB power switching via uhubctl, remote Jupyter notebook access, and hardware PWM control for motor and LED management. The LiDAR communication protocol involves a 48-byte package with start character 0x54, data length fixed at 12 points, and includes distance and luminance data per point. The project emphasizes technical precision, with detailed wiring, setup instructions, and troubleshooting guides for serial communication, GPIO, and performance optimization.</p><hr><h3 id=github---ericjenottevertop-e-ink-ibm-xt-clone-with-solar-power-ultra-low-power-consumption-and-ultra-long-battery-life><a href=https://github.com/ericjenott/Evertop>GitHub - ericjenott/Evertop: E-ink IBM XT clone with solar power, ultra low power consumption, and ultra long battery life.</a><a hidden class=anchor aria-hidden=true href=#github---ericjenottevertop-e-ink-ibm-xt-clone-with-solar-power-ultra-low-power-consumption-and-ultra-long-battery-life>#</a></h3><h3 id=key-facts-13>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-13>#</a></h3><ul><li>Evertop is a portable IBM XT clone powered by an 80186 microcontroller, with 1MB RAM</li><li>Uses an e-ink display with no power when not refreshing, supporting DOS, Minix, Windows up to 3.0</li><li>Incorporates a 6V, 6W solar panel, two 10,000mAh batteries, and supports simultaneous charging via solar, buck/boost circuit (2.5-20V), and micro USB</li><li>Power saving mode enables 200-500 hours of continuous use on a single charge; hibernate and power-off features extend battery life</li><li>Built-in peripherals include CGA, Hercules, MCGA graphics, serial ports, PS/2 ports, USB, Ethernet, WiFi, LoRA radio, headphone jack, volume control, and detachable keyboard</li><li>The device features a 5.83-inch 648x480 e-ink display, 3D-printed matte PETG enclosure, and supports external full-sized keyboards</li><li>Runs nearly all IBM PC/XT compatible DOS software from the 1980s and early 90s</li><li>The system supports multiple power options, automatic battery monitoring, and user-initiated or automatic hibernate modes</li><li>The project includes a minimal version (&ldquo;Evertop Min&rdquo;) removing keyboard, serial port, Ethernet, LoRA, and reducing battery capacity for lighter, cost-effective off-grid use</li><li>Powered by C++, C, and Assembly, with extensive peripheral and power management features</li></ul><h3 id=summary-13>Summary<a hidden class=anchor aria-hidden=true href=#summary-13>#</a></h3><p>Evertop is an ultra low-power, solar-powered IBM XT clone built around an ESP32 microcontroller, supporting vintage operating systems such as DOS, Minix, and Windows up to version 3.0. It features a 5.83-inch 648x480 e-ink display that consumes no power when idle, enabling extended battery life. The device includes a built-in keyboard, external PS/2 ports, full CGA, Hercules, and MCGA graphics support, along with serial, USB, Ethernet, WiFi, LoRA radio, headphone jack, and volume control. Its power system combines two 10,000mAh batteries with a 6V, 6W solar panel, and supports charging from solar, buck/boost circuits (2.5-20V), and micro USB simultaneously, with a built-in voltmeter for monitoring.</p><p>Power management features include a power-saving mode capable of running 200-500 hours continuously on a single charge, with options for hibernate and total power shutoff to maximize longevity. The device&rsquo;s enclosure is 3D-printed matte PETG plastic, designed for portability and ease of access to internal components. It supports emulation of floppy and hard disk images stored on a 256GB SD card, with emulated systems capable of mounting up to 8GB of storage. The system supports multiple emulated configurations, user-initiated hibernate, and automatic power-off, making it suitable for indefinite off-grid operation with proper sunlight. The project also offers a minimized version (&ldquo;Evertop Min&rdquo;) that omits the built-in keyboard, serial ports, Ethernet, LoRA, and reduces battery capacity, maintaining core functionalities for lightweight, cost-effective off-grid computing.</p><hr><h3 id=github---nari-labsdia-a-tts-model-capable-of-generating-ultra-realistic-dialogue-in-one-pass><a href=https://github.com/nari-labs/dia>GitHub - nari-labs/dia: A TTS model capable of generating ultra-realistic dialogue in one pass.</a><a hidden class=anchor aria-hidden=true href=#github---nari-labsdia-a-tts-model-capable-of-generating-ultra-realistic-dialogue-in-one-pass>#</a></h3><h3 id=key-facts-14>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-14>#</a></h3><ul><li>Dia is a 1.6 billion parameter text-to-speech (TTS) model developed by Nari Labs</li><li>Capable of generating ultra-realistic dialogue in a single pass, conditioned on transcripts and audio</li><li>Supports nonverbal expressions such as laughter, coughing, and clearing throat</li><li>Only supports English language generation; pretrained weights available on <a href=https://huggingface.co/nari-labs/Dia-1.6B>Hugging Face</a></li><li>Licensed under Apache 2.0; accessible via inference code and demo page</li></ul><h3 id=summary-14>Summary<a hidden class=anchor aria-hidden=true href=#summary-14>#</a></h3><p>Dia is a TTS model with 1.6 billion parameters designed to produce highly realistic dialogue from transcripts in one pass. Developed by Nari Labs, it allows conditioning on audio inputs to control emotion and tone, and can generate nonverbal cues like laughter and coughing. The model is optimized for English speech synthesis, with pretrained weights hosted on <a href=https://huggingface.co/nari-labs/Dia-1.6B>Hugging Face</a>. It supports dialogue generation with <code>[S1]</code> and <code>[S2]</code> tags, and features voice cloning capabilities demonstrated through example scripts. Inference on enterprise GPUs achieves real-time audio generation, with approximately 40 tokens per second on an A4000 GPU, requiring around 10GB VRAM. The project is licensed under Apache 2.0 and emphasizes ethical use, forbidding identity misuse, deceptive content, and illegal activities. Users can access a demo via <a href=https://huggingface.co/spaces/nari-labs/Dia-1.6B>Hugging Face Spaces</a> and join community discussions on <a href=https://discord.gg/pgdB5YRe>Discord</a>. Future plans include Docker support, inference speed optimization, and quantization for memory efficiency.</p><hr><h3 id=github---openaicodex-lightweight-coding-agent-that-runs-in-your-terminal><a href=https://github.com/openai/codex>GitHub - openai/codex: Lightweight coding agent that runs in your terminal</a><a hidden class=anchor aria-hidden=true href=#github---openaicodex-lightweight-coding-agent-that-runs-in-your-terminal>#</a></h3><h3 id=key-facts-15>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-15>#</a></h3><ul><li>OpenAI&rsquo;s Codex CLI is a lightweight coding agent designed to run in terminal environments</li><li>Licensed under Apache-2.0, with 18.9k stars and 1.7k forks on GitHub</li><li>Supports multiple models and providers, including openai, openrouter, gemini, ollama, mistral, deepseek, xai, groq</li><li>Implements security modes: Suggest, Auto-Edit, Full Auto, with network-disabled execution in Full Auto</li><li>Compatible with Node.js 22+, macOS 12+, Ubuntu 20.04+, Windows 11 via WSL2; RAM requirement is at least 4 GB</li><li>Provides CLI commands such as <code>codex</code>, <code>codex "prompt"</code>, <code>codex -q "prompt"</code>, and shell completion scripts</li><li>Supports configuration via <code>~/.codex/</code> in YAML or JSON, including model, approval mode, notifications, and custom instructions</li><li>Includes features like <code>/diff</code> command for git diffs, and tab completions for file paths</li><li>Uses Husky and lint-staged for code quality enforcement, with migration to pnpm for dependency management</li><li>Supports non-interactive/CI mode for headless operation, with verbose debugging via <code>DEBUG=true</code></li><li>Incorporates update check flow supporting npm, pnpm, bun, with registry-based version lookup and multi-manager support</li><li>Uses Nix flakes for reproducible development environments and detailed migration guides for dependency management</li><li>Active development with frequent commits, PRs, and community contributions</li></ul><h3 id=summary-15>Summary<a hidden class=anchor aria-hidden=true href=#summary-15>#</a></h3><p>OpenAI&rsquo;s Codex CLI is an open-source, terminal-based coding agent that enables AI-assisted development workflows. It integrates with various models and providers, offering flexible security modes‚Äîranging from suggestion to full auto-approval‚Äîallowing users to control the agent&rsquo;s autonomy. The CLI supports multiple operating systems, including macOS, Linux, and Windows via WSL2, requiring Node.js 22+ and at least 4 GB RAM.</p><p>Configuration is managed through YAML or JSON files located in <code>~/.codex/</code>, enabling customization of models, approval modes, notifications, and instructions. The tool provides commands such as <code>codex</code> for interactive use, with options for non-interactive mode suitable for CI pipelines. Features like <code>/diff</code> for git diffs and tab completions enhance usability.</p><p>The project emphasizes code quality and maintainability, utilizing Husky and lint-staged, with a migration to pnpm for dependency management. It includes an intelligent update check system that supports multiple package managers, fetching version info directly from registries to determine updates. Development is further streamlined with Nix flakes for reproducible environments and comprehensive documentation, including migration guides and usage instructions. The repository is actively maintained, with contributions from a broad community of developers.</p><hr><h3 id=15000-lines-of-verified-cryptography-now-in-python--jonathan-protzenko><a href=https://jonathan.protzenko.fr/2025/04/18/python.html>15,000 lines of verified cryptography now in Python | Jonathan Protzenko</a><a hidden class=anchor aria-hidden=true href=#15000-lines-of-verified-cryptography-now-in-python--jonathan-protzenko>#</a></h3><h3 id=key-facts-16>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-16>#</a></h3><ul><li>Python&rsquo;s hash and HMAC algorithms are now fully implemented using <a href=https://github.com/project-everest/hacl-star/>HACL*</a>, a verified cryptographic library, replacing previous implementations.</li><li>The transition involved integrating approximately 15,000 lines of verified C code into Python&rsquo;s repository, with automated updates from upstream HACL*.</li><li>The update enhances support for additional modes in Blake2, a new API for all Keccak variants of SHA3, and improved error management, including handling allocation failures.</li><li>The work spanned 2.5 years, involving contributions from developers Aymeric Fromherz, Son Ho, Gregory P. Smith, B√©n√©dikt Tran, and Chris Eibl, supported by the Python and cryptographic communities.</li><li>The implementation addresses complex streaming API requirements for block algorithms, ensuring safe, generic, and verified handling of various cryptographic modes and states, including optimized HMAC with dual hash states.</li></ul><h3 id=summary-16>Summary<a hidden class=anchor aria-hidden=true href=#summary-16>#</a></h3><p>In November 2022, Jonathan Protzenko proposed replacing Python‚Äôs hash infrastructure with verified cryptography after a CVE in SHA3. As of April 2025, Python now vendors <a href=https://github.com/project-everest/hacl-star/>HACL*</a>, a verified cryptographic library, for all default hash and HMAC algorithms, integrating 15,000 lines of verified C code. This transition was seamless for users and involved automating updates from upstream HACL*.</p><p>The integration required extensive technical work, including implementing new features such as additional Blake2 modes, a comprehensive SHA3 API covering all Keccak variants, and robust error handling, notably for memory allocation failures. The process involved developing a generic, formally verified streaming API for block algorithms using dependent types, accommodating diverse algorithm-specific requirements like pre-input data, variable output lengths, and state retention.</p><p>Handling build complexities, especially with platform-specific optimizations like AVX2 instructions, necessitated refactoring C code to use abstract structs, which was complicated by the auto-generated nature of the code from F* via krml. The project also incorporated rigorous CI testing across 50+ toolchains and architectures, revealing and addressing corner cases such as AVX2-dependent code in HMAC.</p><p>Memory failure propagation was achieved by integrating <code>option</code> types into the verification model, enabling failure detection and propagation through the cryptographic stack. Upstream updates from HACL* are now manageable via a shell script, simplifying maintenance.</p><p>This large-scale integration demonstrates that verified cryptography is mature enough for production use, providing enhanced security guarantees while meeting engineering standards in a major software project like Python.</p><hr><h3 id=getting-forked-by-microsoft--philip-laine><a href=https://philiplaine.com/posts/getting-forked-by-microsoft/>Getting Forked by Microsoft ‚Ä¢ Philip Laine</a><a hidden class=anchor aria-hidden=true href=#getting-forked-by-microsoft--philip-laine>#</a></h3><h3 id=key-facts-17>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-17>#</a></h3><ul><li>Philip Laine&rsquo;s open source project Spegel was developed to address container image registry downtime issues without stateful components</li><li>Microsoft contacted Laine to collaborate on Spegel; later, Microsoft developed Peerd, a forked project with significant code similarities and direct references to Spegel</li><li>Peerd is a fork of Spegel, maintained by Microsoft under the MIT license, with test cases and code snippets directly copied from Spegel</li><li>The copied code includes functions and comments identical to Spegel, causing confusion among users and raising concerns about proper attribution</li><li>Laine&rsquo;s Spegel has over 1,700 stars and 14.4 million pulls since its release, despite challenges posed by Microsoft&rsquo;s actions and open source licensing dynamics</li></ul><h3 id=summary-17>Summary<a hidden class=anchor aria-hidden=true href=#summary-17>#</a></h3><p>Philip Laine&rsquo;s open source project Spegel, designed as a lightweight, non-stateful container image registry solution, was positively received with over 1,700 stars and 14.4 million pulls. Microsoft engaged with Laine to collaborate on Spegel, but later developed Peerd, a peer-to-peer container content distributor for Kubernetes, which is a fork of Spegel. Peerd&rsquo;s codebase contains test cases, function signatures, comments, and configuration snippets that are directly copied from Spegel, with no attribution, despite being licensed under MIT, which permits modifications and forks without obligation to credit original authors.</p><p>This copying has led to confusion among users and challenges for Laine in maintaining an unbiased stance. The incident highlights issues around open source licensing, attribution, and the power imbalance between individual maintainers and large corporations. Laine has considered changing Spegel&rsquo;s license and has enabled GitHub Sponsors to support ongoing development, reflecting concerns about the sustainability and recognition of individual open source contributors.</p><hr><h3 id=claude-code-best-practices--anthropic><a href=https://www.anthropic.com/engineering/claude-code-best-practices>Claude Code Best Practices \ Anthropic</a><a hidden class=anchor aria-hidden=true href=#claude-code-best-practices--anthropic>#</a></h3><h3 id=key-facts-18>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-18>#</a></h3><ul><li>Claude Code is a command line tool for agentic coding, released by Anthropic on April 18, 2025</li><li>Designed as a low-level, unopinionated interface providing near-raw model access</li><li>Supports customization via <code>CLAUDE.md</code> files, environment tuning, and tool allowlists</li><li>Integrates with shell tools, MCP servers, slash commands, GitHub, Git, Jupyter notebooks, and headless automation</li><li>Recommends workflows including exploration, planning, testing, iteration, and multi-Claude setups for parallel tasks</li></ul><h3 id=summary-18>Summary<a hidden class=anchor aria-hidden=true href=#summary-18>#</a></h3><p>Claude Code is a research-developed command line utility for agentic coding, offering flexible, scriptable access to Anthropic&rsquo;s Claude models without enforcing specific workflows. It emphasizes environment customization through <code>CLAUDE.md</code> files, which document commands, style guidelines, and setup instructions, and can be tuned iteratively to optimize context gathering. Users can manage allowed tools via prompts, configuration files, or CLI flags, and integrate Claude with their shell environment, MCP servers, slash commands, GitHub, and Git for enhanced automation and collaboration.</p><p>Claude Code supports advanced workflows such as multi-instance parallel processing, code verification, and visual iteration with screenshots and mockups. Its headless mode enables automation in CI/CD pipelines, issue triage, and code review. Best practices include providing specific instructions, visual references, file paths, URLs, and frequent course corrections. The system also facilitates data input through copy-paste, piping, file reading, and URL fetching. Overall, Claude Code aims to streamline agentic programming, improve productivity, and enable complex automation across diverse environments.</p><hr><h3 id=new-research-backs-up-what-gamers-have-thought-for-years-cozy-video-games-can-be-an-antidote-to-stress-and-anxiety><a href=https://www.reuters.com/business/retail-consumer/cozy-video-games-can-quell-stress-anxiety-2025-01-27/>New research backs up what gamers have thought for years: cozy video games can be an antidote to stress and anxiety.</a><a hidden class=anchor aria-hidden=true href=#new-research-backs-up-what-gamers-have-thought-for-years-cozy-video-games-can-be-an-antidote-to-stress-and-anxiety>#</a></h3><h3 id=key-facts-19>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-19>#</a></h3><ul><li>Recent research supports that cozy video games can reduce stress and anxiety.</li><li>Studies indicate that playing such games can improve mental health and life satisfaction.</li><li>A Japanese study (2020‚Äì2022) found that owning a game console and increasing gameplay by one hour daily correlated with reduced psychological distress.</li><li>Playing casual games like Flower and engaging in mindfulness meditation showed similar stress reduction effects.</li><li>Video games are being explored as therapeutic tools for conditions like ADHD and end-of-life grief processing.</li></ul><h3 id=summary-19>Summary<a hidden class=anchor aria-hidden=true href=#summary-19>#</a></h3><p>Emerging research confirms that cozy video games, characterized by their relaxing and community-oriented nature, can serve as effective tools for alleviating stress and anxiety. These games, which often feature non-violent challenges, customizable avatars, and social interactions within virtual communities, have gained popularity especially after titles like Nintendo‚Äôs <a href=https://en.wikipedia.org/wiki/Animal_Crossing:_New_Horizons>Animal Crossing: New Horizons</a>, released in March 2020 during the COVID-19 pandemic, sold over 13 million units within six weeks. Studies, including a Japanese investigation (2020‚Äì2022), reveal that increased gameplay‚Äîspecifically, an extra hour per day‚Äîcan decrease psychological distress and enhance overall life satisfaction.</p><p>Research by Hiroyuki Egami suggests that owning a gaming console and engaging in regular gaming can positively impact mental health, countering earlier concerns about gaming disorder listed in the ICD-11. Additionally, a 2021 study by Michael Wong found no significant difference in stress reduction between playing casual games like Flower and practicing mindfulness meditation, indicating that gaming can be a viable form of self-care. Therapeutic applications are also expanding, with investigations into using video games to manage ADHD and process grief, exemplified by titles like Spiritfarer, which explores themes of death and compassion. Overall, the evidence underscores the potential of cozy games not only for entertainment but also as accessible mental health interventions.</p><hr><h3 id=building-a-website-fit-for-1999---wesley-moore><a href=https://www.wezm.net/v2/posts/2025/website-fit-for-1999/>Building a Website Fit for 1999 - Wesley Moore</a><a hidden class=anchor aria-hidden=true href=#building-a-website-fit-for-1999---wesley-moore>#</a></h3><h3 id=key-facts-20>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-20>#</a></h3><ul><li>Wesley Moore built a retro-themed website in April 2025, inspired by Ruben‚Äôs Retro Corner and Joel Humphries‚Äô Raspberry Pi hosting</li><li>The site was implemented in HTML4, served over plain HTTP, to ensure compatibility with old computers and browsers</li><li>Hosted on a Qotom mini PC running Chimera Linux, with dynamic content generated via minijinja, jaq, and make, updated every 5 minutes using a cron job</li><li>The website includes static pages with 88√ó31 buttons, server stats, and dynamic content such as server energy, uptime, memory info, and climate data</li><li>Experimented with hosting on an ESP32-C3 microcontroller using bare-metal Rust, and considered CGI with Axum but opted for a Rust server behind Nginx proxy for dynamic content</li></ul><h3 id=summary-20>Summary<a hidden class=anchor aria-hidden=true href=#summary-20>#</a></h3><p>Wesley Moore created a retro-themed website in April 2025, emphasizing compatibility with vintage hardware and browsers by using HTML4 and plain HTTP. The site, hosted on a Qotom mini PC running Chimera Linux, features static pages with 88√ó31 buttons and dynamic server statistics, generated using minijinja and jaq scripts, with updates every five minutes via cron. Moore also experimented with hosting the site on an ESP32-C3 microcontroller using bare-metal Rust, but found it too constrained. He considered implementing CGI scripts with Lighttpd but ultimately built a Rust-based web server with Axum, served behind Nginx reverse proxy, to handle dynamic content more efficiently. The site includes static and dynamic pages displaying server energy consumption, uptime, memory, and climate data, with deployment managed through git pulls and systemd-like process supervision. The project highlights the use of HTML4 for maximum retro compatibility and demonstrates practical approaches to hosting and dynamic content generation on resource-constrained devices. The source code is publicly available on <a href=https://github.com/wezm/wezm.net>GitHub</a>.</p><hr><h3 id=i-thought-i-bought-a-camera-but-no-dji-sold-me-a-license-to-use-their-camera----youtube><a href="https://www.youtube.com/watch?v=aUOnQ_boqCw">I thought I bought a camera, but no! DJI sold me a LICENSE to use their camera ü§¶‚Äç‚ôÇÔ∏è - YouTube</a><a hidden class=anchor aria-hidden=true href=#i-thought-i-bought-a-camera-but-no-dji-sold-me-a-license-to-use-their-camera----youtube>#</a></h3><h3 id=key-facts-21>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-21>#</a></h3><ul><li>YouTube video titled &ldquo;I thought I bought a camera, but no! DJI sold me a LICENSE to use their camera&rdquo; uploaded by Louis Rossmann on April 17, 2025</li><li>The creator emphasizes that purchasing DJI&rsquo;s product grants a license to use the camera, not ownership of the hardware</li><li>The video has 409,465 views, 28K likes, and discusses the licensing model DJI employs for their cameras</li><li>Content includes critique of DJI&rsquo;s licensing approach, implying users do not own the camera outright but are licensed to operate it</li><li>The video is auto-dubbed with generated audio tracks for some languages</li></ul><h3 id=summary-21>Summary<a hidden class=anchor aria-hidden=true href=#summary-21>#</a></h3><p>Louis Rossmann&rsquo;s YouTube video highlights that purchasing a DJI camera does not equate to owning the device outright; instead, customers are effectively buying a license to use the camera. The video, posted on April 17, 2025, with over 409K views, critiques DJI&rsquo;s licensing model, suggesting that users are restricted by licensing terms rather than full ownership rights. Rossmann underscores that this approach shifts ownership rights from the consumer to DJI, raising concerns about control, warranty, and future use of the hardware. The content emphasizes the importance of understanding licensing agreements in modern tech products, especially when hardware is sold with embedded software or operational licenses rather than physical ownership.</p><h2 id=-software-development>‚ñ∂Ô∏è Software Development<a hidden class=anchor aria-hidden=true href=#-software-development>#</a></h2><h3 id=pipelining-might-be-my-favorite-programming-language-feature--mondtech-magazine><a href=https://herecomesthemoon.net/2025/04/pipelining/>Pipelining might be my favorite programming language feature | MOND‚ÜêTECH MAGAZINE</a><a hidden class=anchor aria-hidden=true href=#pipelining-might-be-my-favorite-programming-language-feature--mondtech-magazine>#</a></h3><h3 id=key-facts-22>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-22>#</a></h3><ul><li>The article advocates for pipelining as a programming language feature that enhances code readability and maintainability.</li><li>Pipelining allows passing previous computation results as arguments to subsequent functions, reducing nested calls and parentheses.</li><li>Examples compare traditional nested function calls with pipelined syntax, highlighting improved clarity.</li><li>Pipelining benefits include easier code discovery, simplified editing, and better IDE/autocomplete support.</li><li>The article discusses pipelining&rsquo;s application in SQL, builder patterns, Haskell, and Rust, emphasizing its versatility across languages.</li></ul><h3 id=summary-22>Summary<a hidden class=anchor aria-hidden=true href=#summary-22>#</a></h3><p>The article promotes pipelining as a highly valuable feature in programming languages, emphasizing its role in improving code clarity by enabling the chaining of operations through passing previous results as arguments. It contrasts traditional nested function calls with pipelined syntax, demonstrating how the latter simplifies reading, editing, and understanding code, especially in complex transformations. Pipelining enhances IDE features such as autocompletion and code discovery by making method and function calls more explicit and linear.</p><p>The discussion extends to various languages and contexts. In SQL, pipelining reduces nested queries into linear, readable sequences, aligning with LINQ-style syntax. In design patterns, the builder pattern benefits from pipelining by chaining configuration methods. The article also examines Haskell, showing how the addition of a pipelining operator (<code>&</code>) improves readability by refactoring nested function compositions into a clear, top-to-bottom flow. Rust&rsquo;s trait system and type inference further exemplify how pipelining combines flexibility with ease of use, avoiding complex inheritance or monadic paradigms.</p><p>Overall, the author advocates for adopting pipelining to produce more understandable, maintainable, and editor-friendly code, emphasizing its advantages across multiple programming paradigms and languages.</p><h2 id=-management-and-leadership>‚ñ∂Ô∏è Management and Leadership<a hidden class=anchor aria-hidden=true href=#-management-and-leadership>#</a></h2><h3 id=android-phones-will-soon-reboot-themselves-after-sitting-unused-for-3-days---ars-technica><a href=https://arstechnica.com/gadgets/2025/04/android-phones-will-soon-reboot-themselves-after-sitting-unused-for-3-days/>Android phones will soon reboot themselves after sitting unused for 3 days - Ars Technica</a><a hidden class=anchor aria-hidden=true href=#android-phones-will-soon-reboot-themselves-after-sitting-unused-for-3-days---ars-technica>#</a></h3><h3 id=key-facts-23>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-23>#</a></h3><ul><li>Android devices will automatically reboot after being locked and unused for 3 consecutive days via a Google Play Services update (version 25.14), rolling out gradually from April 14, 2025.</li><li>The feature enhances device security by encrypting data in the &ldquo;Before First Unlock&rdquo; (BFU) state and restricting access to PIN or passcode, making data retrieval more difficult.</li><li>The update is silent, with no user notification, and is part of ongoing efforts to move system components to Google Play Services for faster updates.</li><li>Similar to Apple&rsquo;s Inactivity Reboot feature introduced in iOS 18.1, this limits device exposure by forcing reboots after periods of inactivity, reducing the risk of data extraction.</li><li>The feature is designed to improve security, especially in scenarios where devices are left unattended for extended periods, and is expected to be available on most Android devices within a week or more.</li></ul><h3 id=summary-23>Summary<a hidden class=anchor aria-hidden=true href=#summary-23>#</a></h3><p>A forthcoming Google Play Services update (version 25.14), officially released on April 14, 2025, will enable Android phones to automatically reboot after being locked and unused for three days. This silent feature aims to bolster security by encrypting user data in the &ldquo;Before First Unlock&rdquo; (BFU) state, where biometrics and location-based unlocking are disabled, and access is limited to PIN or passcode. The reboot process makes data extraction significantly more difficult, even with advanced recovery tools, by limiting the window during which unencrypted data is accessible.</p><p>This functionality mirrors Apple&rsquo;s Inactivity Reboot introduced in iOS 18.1, which also aims to enhance device security by forcing reboots after inactivity, though it has caused concerns among law enforcement regarding data access. The update is part of Google&rsquo;s strategy to shift system components to Google Play Services, allowing for automatic background updates without user intervention. The rollout is gradual, and most Android devices are expected to receive the feature within a week or more, contributing to improved device security and data protection.</p><hr><h3 id=deciphering-glyph--stop-writing><a href=https://blog.glyph.im/2025/04/stop-writing-init-methods.html>Deciphering Glyph :: Stop Writing <code>__init__</code> Methods</a><a hidden class=anchor aria-hidden=true href=#deciphering-glyph--stop-writing>#</a></h3><h3 id=key-facts-24>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-24>#</a></h3><ul><li>The article critiques the traditional use of <code>__init__</code> methods in Python classes, especially for data structures, highlighting their drawbacks.</li><li>Before Python 3.7, <code>__init__</code> was essential for constructing data classes, but alternatives had significant limitations.</li><li>Overusing <code>__init__</code> for side-effects or complex initialization links object creation with side-effects, leading to design issues.</li><li>The proposed solution involves using <code>@dataclass</code>, <code>@classmethod</code> factory methods, and <code>typing.NewType</code> for better design.</li><li>Implementing these techniques improves code robustness, testability, discoverability, and future-proofing.</li></ul><h3 id=summary-24>Summary<a hidden class=anchor aria-hidden=true href=#summary-24>#</a></h3><p>The article argues that writing custom <code>__init__</code> methods for data classes in Python introduces unnecessary complexity and side-effects, especially when handling more complex data structures like a <code>FileReader</code>. Historically, <code>__init__</code> was necessary before Python 3.7&rsquo;s introduction of <code>dataclasses</code>, but its overuse often leads to tightly coupled object creation and side-effects, complicating testing and maintenance. For example, a <code>FileReader</code> class that opens a file descriptor in <code>__init__</code> becomes difficult to extend or modify, especially when dealing with asynchronous operations or external dependencies.</p><p>The author advocates replacing <code>__init__</code> with <code>@dataclass</code> for attribute management, <code>@classmethod</code> factory methods for flexible object creation, and <code>typing.NewType</code> to enforce constraints on primitive types like <code>int</code>. This approach ensures objects are always valid upon creation, simplifies testing, and enhances discoverability of construction methods. For instance, <code>FileReader.open()</code> as a class method replaces direct constructor calls, allowing for asynchronous or alternative instantiation patterns without side-effects in <code>__init__</code>.</p><p>By adopting these patterns, developers can create robust, flexible, and future-proof classes that avoid the pitfalls of arbitrary code execution during object instantiation, leading to clearer, more maintainable, and testable codebases.</p><hr><h3 id=librarians-are-dangerous---the-enthusiast-by-brad-montague><a href=https://bradmontague.substack.com/p/librarians-are-dangerous>Librarians are dangerous. - The Enthusiast by Brad Montague</a><a hidden class=anchor aria-hidden=true href=#librarians-are-dangerous---the-enthusiast-by-brad-montague>#</a></h3><h3 id=key-facts-25>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-25>#</a></h3><ul><li>The article is a satirical public service announcement claiming &ldquo;Librarians are dangerous.&rdquo;</li><li>Librarians are depicted as activists, educators, tech-savvy, myth-slayers, and organizers of rebellion.</li><li>They actively combat misinformation, censorship, outdated practices, apathy, and loneliness.</li><li>Modern librarians are described as capable of coding, curating, and influencing worldview through access to stories.</li><li>The message emphasizes librarians&rsquo; role in fostering empathy, curiosity, and societal change, especially through empowering youth with meaningful books.</li></ul><h3 id=summary-25>Summary<a hidden class=anchor aria-hidden=true href=#summary-25>#</a></h3><p>Brad Montague&rsquo;s article presents a provocative, humorous perspective on librarians, portraying them as &ldquo;dangerous&rdquo; agents of societal transformation. Far from being mere custodians of quiet and dusty books, modern librarians are depicted as multifaceted activists‚Äîpart educators, part tech experts, part myth-slayers‚Äîwho actively challenge misinformation, censorship, and outdated norms. They host storytimes, teach media literacy, assist with technological projects like 3D printing, and help individuals recover digital access, all while advocating for equitable access to stories that affirm identity and belonging.</p><p>Montague emphasizes that librarians are fearless in confronting budget cuts, criticism, and social apathy, and that their influence extends beyond books to shaping empathy, curiosity, and resilience in communities. They are described as unleashing knowledge and igniting minds, making them pivotal in building a more compassionate and informed society. The article underscores their role in fostering personal growth and societal progress, urging readers to recognize their importance and &ldquo;stay dangerous.&rdquo;</p><hr><h3 id=fossil-fuels-fall-below-50-of-us-electricity-for-the-first-month-on-record--ember><a href=https://ember-energy.org/latest-updates/fossil-fuels-fall-below-50-of-us-electricity-for-the-first-month-on-record/>Fossil fuels fall below 50% of US electricity for the first month on record | Ember</a><a hidden class=anchor aria-hidden=true href=#fossil-fuels-fall-below-50-of-us-electricity-for-the-first-month-on-record--ember>#</a></h3><h3 id=key-facts-26>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-26>#</a></h3><ul><li>In March 2025, fossil fuels accounted for 49.2% of US electricity generation, the first month below 50%, surpassing the previous record of 51% in April 2024</li><li>Clean energy sources generated 50.8% of US electricity in March 2025, driven by record-high wind and solar power</li><li>Wind and solar reached a combined 24.4% of US electricity, with solar increasing by 37% (+8.3 TWh) and wind by 12% (+5.7 TWh) compared to March 2024</li><li>Wind and solar generated a total of 83 TWh, an 11% increase over the previous record of 75 TWh set in April 2024</li><li>Fossil fuel generation decreased by 2.5% (-4.3 TWh) compared to March 2024</li></ul><h3 id=summary-26>Summary<a hidden class=anchor aria-hidden=true href=#summary-26>#</a></h3><p>In March 2025, the US achieved a historic milestone with fossil fuels contributing less than half (49.2%) of electricity generation for the first time on record, according to Ember. This shift resulted from a significant rise in wind and solar power, which collectively reached a record 24.4% of the national electricity mix. Solar power alone increased by 37% (+8.3 TWh), while wind power grew by 12% (+5.7 TWh) compared to the same month in 2024. The combined output of wind and solar hit an all-time high of 83 TWh, surpassing the previous record of 75 TWh set in April 2024 by 11%. Meanwhile, fossil fuel generation declined by 2.5% (-4.3 TWh) year-over-year.</p><p>This development reflects a long-term decline in fossil fuel reliance, with wind and solar expanding substantially over the past decade. In March 2015, fossil fuels accounted for 65% of US electricity, while wind and solar contributed only 5.7%. Today, wind and solar combined represent 17% of the US electricity mix, overtaking coal (15%) in 2024. Solar power&rsquo;s share has grown from 1% in March 2015 to 9.2% in March 2025, with over a third of new solar capacity installed in Texas. The trend indicates the US is approaching a tipping point where clean energy sources dominate, driven primarily by the continued growth of solar and wind power. For more details, see <a href=https://ember-energy.org/latest-insights/us-electricity-2025-special-report>Ember&rsquo;s report</a>.</p><hr><h3 id=gitgo-away-self-hosted-abuse-detection-and-rule-enforcement-against-low-effort-mass-ai-scraping-and-bots---gammaspectralive-git><a href=https://git.gammaspectra.live/git/go-away#why>git/go-away: Self-hosted abuse detection and rule enforcement against low-effort mass AI scraping and bots. - GammaSpectra.Live Git</a><a hidden class=anchor aria-hidden=true href=#gitgo-away-self-hosted-abuse-detection-and-rule-enforcement-against-low-effort-mass-ai-scraping-and-bots---gammaspectralive-git>#</a></h3><h3 id=key-facts-27>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-27>#</a></h3><ul><li>The repository <code>git/go-away</code> provides a self-hosted abuse detection and rule enforcement system targeting low-effort AI scraping and bots.</li><li>Implements flexible request filtering using CEL (Common Expression Language) for rule matching based on client properties such as IP, host, method, user agent, path, headers, TLS fingerprints, and DNSBL status.</li><li>Supports multiple rule actions: PASS, CHALLENGE, DENY, CHECK, and POISON, with configurable challenge options including non-JavaScript and JavaScript/WASM challenges.</li><li>Features include upstream PROXY support (HAProxy, nginx, Caddy), automatic TLS via ACME, TLS fingerprinting (JA3, JA4), DNSBL querying, network range filtering, and sharing signing seeds across instances.</li><li>The system is designed for high configurability, supporting multiple backends, dynamic policy reloading, custom templates, and challenge themes, with active development and planned enhancements.</li></ul><h3 id=summary-27>Summary<a hidden class=anchor aria-hidden=true href=#summary-27>#</a></h3><p><code>git/go-away</code> is a highly configurable, self-hosted system designed to detect and mitigate low-effort AI scraping and automated bots by applying detailed request filtering rules and challenges. It leverages CEL for flexible rule matching based on client IP, TLS fingerprinting, DNSBL, network ranges, and request properties, enabling operators to craft precise policies. The system supports multiple rule actions, including challenging clients with non-JavaScript or JavaScript/WASM challenges, and can serve as a proxy layer between the site and the internet, with support for upstream PROXY protocols, automatic TLS via ACME, and HTTP/2. It also offers features like challenge template customization, challenge chaining, and sharing signing secrets across instances. The project is actively maintained, with ongoing development to improve performance, configuration, and feature set, aiming for a stable v1.0.0 release. Deployment options include native Go binaries and Docker, with example configurations provided for common use cases such as Forgejo and generic sites.</p><hr><h3 id=github---humanlayer12-factor-agents-what-are-the-principles-we-can-use-to-build-llm-powered-software-that-is-actually-good-enough-to-put-in-the-hands-of-production-customers><a href=https://github.com/humanlayer/12-factor-agents>GitHub - humanlayer/12-factor-agents: What are the principles we can use to build LLM-powered software that is actually good enough to put in the hands of production customers?</a><a hidden class=anchor aria-hidden=true href=#github---humanlayer12-factor-agents-what-are-the-principles-we-can-use-to-build-llm-powered-software-that-is-actually-good-enough-to-put-in-the-hands-of-production-customers>#</a></h3><h3 id=key-facts-28>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-28>#</a></h3><ul><li>The project outlines principles for building production-ready LLM-powered software, inspired by the 12-factor app methodology</li><li>Emphasizes core engineering techniques to improve reliability, scalability, and maintainability of AI applications</li><li>Presents 12 factors including natural language to tool calls, prompt ownership, context window management, structured tool outputs, unified state, control flow ownership, error handling, and modular agent design</li><li>Advocates integrating modular agent concepts into existing products rather than full framework rewrites</li><li>The principles are documented in a public GitHub repository <a href=https://github.com/humanlayer/12-factor-agents>humanlayer/12-factor-agents</a>, with detailed content on each factor and related resources</li></ul><h3 id=summary-28>Summary<a hidden class=anchor aria-hidden=true href=#summary-28>#</a></h3><p>The <a href=https://github.com/humanlayer/12-factor-agents>12-factor agents</a> project defines a set of engineering principles for developing reliable, scalable, and maintainable LLM-powered software suitable for production deployment. Drawing from the 12-factor app methodology, it identifies 12 core factors‚Äîsuch as converting natural language to tool calls, owning prompts and context windows, structuring tool outputs, unifying execution and business state, and controlling flow ownership‚Äîthat collectively enhance agent robustness and flexibility. The approach emphasizes modularity, advocating for integrating these concepts into existing products rather than relying solely on comprehensive frameworks, thereby enabling developers to deliver high-quality AI features efficiently. The principles are supported by technical documentation, visual guides, and a community of contributors, aiming to improve the engineering practices behind AI agent development.</p><hr><h3 id=steve-blank-how-the-us-became-a-science-superpower><a href=https://steveblank.com/2025/04/15/how-the-u-s-became-a-science-superpower/>Steve Blank How the U.S. Became A Science Superpower</a><a hidden class=anchor aria-hidden=true href=#steve-blank-how-the-us-became-a-science-superpower>#</a></h3><h3 id=key-facts-29>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-29>#</a></h3><ul><li>Prior to WWII, the U.S. was a distant second in science and engineering; by war&rsquo;s end, it surpassed Britain and led the world for 85 years.</li><li>British wartime science efforts focused on defense and intelligence tech, led by Professor Frederick Lindemann, with centralized government labs and no university involvement.</li><li>U.S. wartime research was driven by Vannevar Bush, who established university-based, civilian-run labs through the Office of Scientific Research and Development (OSRD), funding $9 billion (2025 dollars) from 1941-1945.</li><li>U.S. government funding post-WWII, notably via NSF, NASA, and DARPA, fostered a decentralized, collaborative innovation ecosystem centered around universities like MIT, Harvard, and Stanford.</li><li>Britain‚Äôs post-war austerity and economic policies hindered scaling and commercialization of wartime innovations; the U.S. built a global leadership in science, technology, and industry, with university-industry-government partnerships fueling Silicon Valley and related sectors.</li><li>In 2025, U.S. government support for university research declines, raising concerns about future global technological leadership.</li></ul><h3 id=summary-29>Summary<a hidden class=anchor aria-hidden=true href=#summary-29>#</a></h3><p>Before WWII, the U.S. lagged behind Britain in science and engineering, but the war catalyzed a transformation driven by contrasting approaches to research and development. Britain‚Äôs wartime efforts, led by Professor Lindemann, relied on centralized government labs focused on defense and intelligence projects, with minimal university involvement. Their approach prioritized short-term survival amid daily bombings and economic constraints, resulting in significant breakthroughs like radar, sonar, and early computing, but lacked the scale and commercialization needed for post-war dominance.</p><p>In contrast, the U.S. under Vannevar Bush adopted a decentralized, university-centered model, creating civilian-run labs funded with $9 billion during the war. This fostered a collaborative ecosystem where universities like MIT, Harvard, and Stanford became innovation hubs, supported by large federal investments such as the NSF, NASA, and DARPA. This system enabled rapid development and commercialization of technologies like computers, nuclear power, and synthetic fibers, fueling economic growth and establishing U.S. global leadership.</p><p>Post-war, Britain‚Äôs economic austerity and political shifts limited its capacity to scale innovations, while the U.S. built a robust, industry-integrated research infrastructure. The indirect cost reimbursement system allowed U.S. universities to develop world-class labs, attracting global talent and generating thousands of startups annually. This university-government-industry partnership became the foundation of Silicon Valley and other high-tech sectors.</p><p>Today, China has invested heavily to surpass U.S. technological dominance, and concerns grow as U.S. federal support for university research diminishes in 2025, potentially ending decades of American leadership in science and technology.</p><hr><h3 id=tls-certificate-lifetimes-will-officially-reduce-to-47-days--digicert><a href=https://www.digicert.com/blog/tls-certificate-lifetimes-will-officially-reduce-to-47-days>TLS Certificate Lifetimes Will Officially Reduce to 47 Days | DigiCert</a><a hidden class=anchor aria-hidden=true href=#tls-certificate-lifetimes-will-officially-reduce-to-47-days--digicert>#</a></h3><h3 id=key-facts-30>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-30>#</a></h3><ul><li>The CA/Browser Forum voted to reduce TLS certificate lifetime to 47 days, with implementation phases starting March 15, 2026</li><li>Current maximum TLS certificate validity is 398 days; it will decrease to 200 days on March 15, 2026, then to 100 days on March 15, 2027, and finally to 47 days on March 15, 2029</li><li>Domain and IP address validation reuse periods will also decrease: from 398 days now to 10 days by March 15, 2029</li><li>Validations of Subject Identity Information (SII) in OV/EV certificates will be limited to 398 days from the current 825 days after March 15, 2026</li><li>The change emphasizes automation, as manual revalidation becomes impractical with shorter certificate lifetimes</li></ul><h3 id=summary-30>Summary<a hidden class=anchor aria-hidden=true href=#summary-30>#</a></h3><p>The CA/Browser Forum has officially amended the TLS Baseline Requirements to significantly shorten TLS certificate lifetimes, with the maximum validity dropping from 398 days to 47 days by March 15, 2029. This phased reduction begins with a 200-day limit on March 15, 2026, followed by a 100-day limit on March 15, 2027, and ultimately reaching 47 days in 2029. Alongside this, the reuse period for domain and IP address validation information will decrease from 398 days to just 10 days, and validation of Subject Identity Information (SII) in OV and EV certificates will be limited to 398 days after March 15, 2026, down from 825 days.</p><p>The move aims to enhance security by mitigating risks associated with outdated certificate data and unreliable revocation systems, advocating for automation in certificate lifecycle management. Apple‚Äôs justification highlights that shorter lifetimes necessitate automation, with DigiCert supporting this transition through solutions like <a href=https://www.digicert.com/trust-lifecycle-manager>Trust Lifecycle Manager</a> and <a href=https://www.digicert.com/tls-ssl/certcentral-tls-ssl-manager>CertCentral</a>, including ACME support for automated issuance and renewal. The new schedule underscores the importance of adopting automation tools to prevent outages and streamline certificate management processes.</p><hr><h3 id=decreased-co2-saturation-during-circular-breathwork-supports-emergence-of-altered-states-of-consciousness--communications-psychology><a href=https://www.nature.com/articles/s44271-025-00247-0>Decreased CO2 saturation during circular breathwork supports emergence of altered states of consciousness | Communications Psychology</a><a hidden class=anchor aria-hidden=true href=#decreased-co2-saturation-during-circular-breathwork-supports-emergence-of-altered-states-of-consciousness--communications-psychology>#</a></h3><h3 id=key-facts-31>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-31>#</a></h3><ul><li>Study tracked physiological and experiential dynamics during circular breathwork (Holotropic and Conscious-Connected) in 61 experienced practitioners.</li><li>End-tidal CO2 pressure (etCO2) decreased significantly in active-breathing groups, reaching levels below 20 mmHg, correlating with onset of altered states of consciousness (ASCs).</li><li>Subjective ASC scores (MEQ-30 and 11D-ASC) during breathwork resembled those induced by psychedelics, with depth predicting improvements in well-being and reductions in depressive symptoms.</li><li>Physiological changes included a significant drop in etCO2 during active breathwork, with trajectories highly similar across breathwork styles; reductions in CO2 were strongly linked to deeper ASCs.</li><li>Post-session, active breathers showed decreased salivary Œ±-amylase (sympathetic activity marker) and increased IL-1Œ≤ (inflammatory marker), with subjective ASC depth moderating inflammation; follow-up assessments indicated improved mental health and well-being.</li></ul><h3 id=summary-31>Summary<a hidden class=anchor aria-hidden=true href=#summary-31>#</a></h3><p>This study investigated the physiological and psychological mechanisms underlying circular breathwork, comparing Holotropic and Conscious-Connected techniques in 61 experienced practitioners. Results demonstrated that deliberate hyperventilation during active breathwork led to a significant reduction in end-tidal CO2 (etCO2), often below 20 mmHg, which was strongly associated with the emergence of altered states of consciousness (ASCs). These subjective experiences, measured via the MEQ-30 and 11D-ASC questionnaires, closely resembled psychedelic-induced ASCs, including ego dissolution and unity, and their intensity predicted subsequent improvements in mental health, such as decreased depressive symptoms and enhanced well-being.</p><p>Physiologically, active breathers showed a rapid decline in etCO2 during the session, with trajectories highly similar across both breathwork styles, indicating engagement of shared mechanisms. The depth of subjective ASC experiences correlated inversely with etCO2 levels, suggesting that CO2 reduction acts as a trigger for ASC onset. Post-session, biomarkers reflected a complex physiological response: a significant decrease in salivary Œ±-amylase indicated reduced sympathetic activity, while IL-1Œ≤ levels increased, with deeper ASCs associated with smaller inflammatory responses. These findings support the concept that physiological boundary conditions, notably CO2 depletion, facilitate ASC emergence in a non-pharmacological context, with subjective experience influencing long-term psychological and physiological outcomes.</p><p>The similarity in outcomes across both breathwork styles, despite differences in session duration, suggests that core physiological mechanisms‚Äîparticularly CO2 modulation‚Äîare central to ASC induction. The results position circular breathwork as a potentially accessible therapeutic tool capable of eliciting psychedelic-like states, with implications for mental health treatment, especially where pharmacological interventions are restricted. Limitations include the reliance on experienced practitioners and the need for further research into long-term effects, safety, and the role of contextual factors such as set and setting.</p><hr><h3 id=synology-lost-the-plot-with-hard-drive-locking-move---servethehome><a href=https://www.servethehome.com/synology-lost-the-plot-with-hard-drive-locking-move/>Synology Lost the Plot with Hard Drive Locking Move - ServeTheHome</a><a hidden class=anchor aria-hidden=true href=#synology-lost-the-plot-with-hard-drive-locking-move---servethehome>#</a></h3><h3 id=key-facts-32>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-32>#</a></h3><ul><li>Synology plans to restrict its 2025 Plus NAS models to using only Synology-branded hard drives</li><li>This move enforces vendor lock-in, disabling features like volume-wide deduplication, lifespan analysis, and automatic firmware updates for third-party drives</li><li>Synology‚Äôs Plus series currently supports drives up to 16TB (e.g., HAT3310-16T), whereas competitors like WD Red Pro support 26TB</li><li>The restriction limits raw capacity to 128TB in an 8-bay NAS, compared to 208TB with other brands</li><li>The move is viewed as a margin-boosting strategy that negatively impacts customer flexibility, data security, and long-term drive availability</li></ul><h3 id=summary-32>Summary<a hidden class=anchor aria-hidden=true href=#summary-32>#</a></h3><p>Synology is reportedly moving towards locking its upcoming 2025 Plus NAS models to only support its own branded hard drives, as reported by <a href=https://www.hardwareluxx.de/index.php/news/hardware/festplatten/65949-synology-weitet-den-zwang-zur-eignen-oder-zertifizierten-festplatte-auf-die-plus-modelle-aus.html>HardwareLuxx</a>. This change will disable key features such as volume-wide deduplication, lifespan analysis, and automatic firmware updates for third-party drives, effectively forcing users to buy Synology-branded drives. Currently, the Plus series supports drives up to 16TB, with the maximum raw capacity of an 8-bay NAS being 128TB, whereas competitors like WD Red Pro offer 26TB drives and a total capacity of 208TB in similar configurations.</p><p>The move appears to be a strategy to increase margins by rebranding drives from other manufacturers, which is criticized as detrimental to customer choice and data security. Vendor lock-in complicates drive replacement, especially when drives fail or need upgrading, and raises concerns about long-term availability and support. The restriction limits the ability to use drives from other vendors, which are often more readily available and have larger capacities, thus reducing flexibility and increasing dependency on Synology‚Äôs supply chain. Critics argue this approach is a step back from previous hardware standards and could harm Synology‚Äôs reputation, as many users prefer open hardware configurations like <a href=https://www.servethehome.com/category/server-parts/server-systems/>TrueNAS</a> or custom-built solutions.</p><h2 id=-technology>‚ñ∂Ô∏è Technology<a hidden class=anchor aria-hidden=true href=#-technology>#</a></h2><h3 id=start-building-with-gemini-25-flash---google-developers-blog><a href=https://developers.googleblog.com/en/start-building-with-gemini-25-flash/>Start building with Gemini 2.5 Flash - Google Developers Blog</a><a hidden class=anchor aria-hidden=true href=#start-building-with-gemini-25-flash---google-developers-blog>#</a></h3><h3 id=key-facts-33>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-33>#</a></h3><ul><li>Google announced Gemini 2.5 Flash in early preview on April 17, 2025</li><li>Major upgrade over Gemini 2.0 Flash, enhancing reasoning capabilities while maintaining speed and cost efficiency</li><li>First fully hybrid reasoning model, allowing developers to toggle &ldquo;thinking&rdquo; on or off and set reasoning budgets</li><li>Supports reasoning through complex tasks with adjustable token budgets (0 to 24,576 tokens)</li><li>Performs strongly on complex prompts, second only to Gemini 2.5 Pro on Hard Prompts in LMArena</li><li>Adds fine-grained control over reasoning, enabling tradeoffs between quality, cost, and latency</li><li>Available via <a href="https://aistudio.google.com/prompts/new_chat?model=gemini-2.5-flash-preview-04-17">Google AI Studio</a>, <a href="https://console.cloud.google.com/vertex-ai/studio/multimodal?model=gemini-2.5-flash-preview-04-17">Vertex AI</a>, and Gemini app</li><li>API parameter <code>thinking_budget</code> controls reasoning depth; ranges from 0 to 24,576 tokens</li><li>Demonstrates improved reasoning for tasks like math problem solving and code generation</li><li>Model pricing based on Google‚Äôs analysis and documentation</li></ul><h3 id=summary-33>Summary<a hidden class=anchor aria-hidden=true href=#summary-33>#</a></h3><p>Google has introduced Gemini 2.5 Flash in early preview, available through the <a href=https://ai.google.dev/gemini-api/docs/thinking>Gemini API</a> via <a href="https://aistudio.google.com/prompts/new_chat?model=gemini-2.5-flash-preview-04-17">Google AI Studio</a>, <a href="https://console.cloud.google.com/vertex-ai/studio/multimodal?model=gemini-2.5-flash-preview-04-17">Vertex AI</a>, and the Gemini app. Building on Gemini 2.0 Flash, this version significantly enhances reasoning capabilities while preserving speed and cost efficiency. It is the first fully hybrid reasoning model, enabling developers to toggle &ldquo;thinking&rdquo; on or off and set reasoning budgets to balance quality, latency, and cost.</p><p>The model performs multi-step reasoning tasks, such as solving math problems or analyzing complex prompts, with improved accuracy. It supports adjustable reasoning token budgets from 0 to 24,576 tokens, allowing fine-grained control over the reasoning process. Developers can specify a reasoning budget via API parameters or UI sliders, with the model automatically adjusting reasoning depth based on prompt complexity. Gemini 2.5 Flash demonstrates strong performance on challenging prompts, ranking second only to Gemini 2.5 Pro on the LMArena benchmark, and offers a superior price-to-performance ratio. The model&rsquo;s API and detailed reasoning guides are available in the <a href=https://ai.google.dev/gemini-api/docs/thinking#set-budget>developer documentation</a>.</p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=next href=https://alobbs.com/posts/my-first-post/><span class=title>Next ¬ª</span><br><span>Investment Strategies: Intro to Building Wealth with Precision</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share 2025-04-22 Briefing on x" href="https://x.com/intent/tweet/?text=2025-04-22%20Briefing&amp;url=https%3a%2f%2falobbs.com%2fposts%2f2025-04-22%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 2025-04-22 Briefing on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2falobbs.com%2fposts%2f2025-04-22%2f&amp;title=2025-04-22%20Briefing&amp;summary=2025-04-22%20Briefing&amp;source=https%3a%2f%2falobbs.com%2fposts%2f2025-04-22%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 2025-04-22 Briefing on reddit" href="https://reddit.com/submit?url=https%3a%2f%2falobbs.com%2fposts%2f2025-04-22%2f&title=2025-04-22%20Briefing"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 2025-04-22 Briefing on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2falobbs.com%2fposts%2f2025-04-22%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 2025-04-22 Briefing on whatsapp" href="https://api.whatsapp.com/send?text=2025-04-22%20Briefing%20-%20https%3a%2f%2falobbs.com%2fposts%2f2025-04-22%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 2025-04-22 Briefing on telegram" href="https://telegram.me/share/url?text=2025-04-22%20Briefing&amp;url=https%3a%2f%2falobbs.com%2fposts%2f2025-04-22%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 2025-04-22 Briefing on ycombinator" href="https://news.ycombinator.com/submitlink?t=2025-04-22%20Briefing&u=https%3a%2f%2falobbs.com%2fposts%2f2025-04-22%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://alobbs.com/>Alvaro's Site</a></span> ¬∑
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><a rel=me href=https://mastodon.social/@alobbs style=display:none>Mastodon</a>
<script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>