<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>2025-04-23 Briefing | Alvaro's Site</title>
<meta name=keywords content><meta name=description content="▶️ Internet Infrastructure
Arch Linux - News: Valkey to replace Redis in the [extra] Repository
Key Facts

Valkey, a high-performance key/value datastore, will replace Redis in the extra repository
The change follows Redis&rsquo;s license modification from BSD-3-Clause to RSALv2 and SSPLv1 on March 20, 2024
Arch Linux package maintainers will support the redis package for approximately 14 days from April 17, 2025, after which it will be moved to the AUR and no longer receive updates

Summary
Arch Linux announced that Valkey, a high-performance key/value datastore, will replace Redis in the extra repository due to Redis changing its license to RSALv2 and SSPLv1 on March 20, 2024. The redis package will be supported for about 14 days from April 17, 2025, allowing users to transition smoothly to Valkey; after this period, the redis package will be moved to the AUR and considered deprecated with no further updates. Users are advised to begin migrating their Redis usage promptly to avoid issues post-transition."><meta name=author content="Alvaro Lopez Ortega"><link rel=canonical href=https://alobbs.com/posts/2025-04-23/><link crossorigin=anonymous href=/assets/css/stylesheet.f49d66caae9ea0fd43f21f29e71a8d3e284517ed770f2aa86fa012953ad3c9ef.css integrity="sha256-9J1myq6eoP1D8h8p5xqNPihFF+13Dyqob6ASlTrTye8=" rel="preload stylesheet" as=style><link rel=icon href=https://alobbs.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://alobbs.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://alobbs.com/favicon-32x32.png><link rel=apple-touch-icon href=https://alobbs.com/apple-touch-icon.png><link rel=mask-icon href=https://alobbs.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://alobbs.com/posts/2025-04-23/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://alobbs.com/posts/2025-04-23/"><meta property="og:site_name" content="Alvaro's Site"><meta property="og:title" content="2025-04-23 Briefing"><meta property="og:description" content="▶️ Internet Infrastructure Arch Linux - News: Valkey to replace Redis in the [extra] Repository Key Facts Valkey, a high-performance key/value datastore, will replace Redis in the extra repository The change follows Redis’s license modification from BSD-3-Clause to RSALv2 and SSPLv1 on March 20, 2024 Arch Linux package maintainers will support the redis package for approximately 14 days from April 17, 2025, after which it will be moved to the AUR and no longer receive updates Summary Arch Linux announced that Valkey, a high-performance key/value datastore, will replace Redis in the extra repository due to Redis changing its license to RSALv2 and SSPLv1 on March 20, 2024. The redis package will be supported for about 14 days from April 17, 2025, allowing users to transition smoothly to Valkey; after this period, the redis package will be moved to the AUR and considered deprecated with no further updates. Users are advised to begin migrating their Redis usage promptly to avoid issues post-transition."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-04-23T15:34:31+00:00"><meta property="article:modified_time" content="2025-04-23T15:34:31+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="2025-04-23 Briefing"><meta name=twitter:description content="▶️ Internet Infrastructure
Arch Linux - News: Valkey to replace Redis in the [extra] Repository
Key Facts

Valkey, a high-performance key/value datastore, will replace Redis in the extra repository
The change follows Redis&rsquo;s license modification from BSD-3-Clause to RSALv2 and SSPLv1 on March 20, 2024
Arch Linux package maintainers will support the redis package for approximately 14 days from April 17, 2025, after which it will be moved to the AUR and no longer receive updates

Summary
Arch Linux announced that Valkey, a high-performance key/value datastore, will replace Redis in the extra repository due to Redis changing its license to RSALv2 and SSPLv1 on March 20, 2024. The redis package will be supported for about 14 days from April 17, 2025, allowing users to transition smoothly to Valkey; after this period, the redis package will be moved to the AUR and considered deprecated with no further updates. Users are advised to begin migrating their Redis usage promptly to avoid issues post-transition."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://alobbs.com/posts/"},{"@type":"ListItem","position":2,"name":"2025-04-23 Briefing","item":"https://alobbs.com/posts/2025-04-23/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"2025-04-23 Briefing","name":"2025-04-23 Briefing","description":"▶️ Internet Infrastructure Arch Linux - News: Valkey to replace Redis in the [extra] Repository Key Facts Valkey, a high-performance key/value datastore, will replace Redis in the extra repository The change follows Redis\u0026rsquo;s license modification from BSD-3-Clause to RSALv2 and SSPLv1 on March 20, 2024 Arch Linux package maintainers will support the redis package for approximately 14 days from April 17, 2025, after which it will be moved to the AUR and no longer receive updates Summary Arch Linux announced that Valkey, a high-performance key/value datastore, will replace Redis in the extra repository due to Redis changing its license to RSALv2 and SSPLv1 on March 20, 2024. The redis package will be supported for about 14 days from April 17, 2025, allowing users to transition smoothly to Valkey; after this period, the redis package will be moved to the AUR and considered deprecated with no further updates. Users are advised to begin migrating their Redis usage promptly to avoid issues post-transition.\n","keywords":[],"articleBody":"▶️ Internet Infrastructure Arch Linux - News: Valkey to replace Redis in the [extra] Repository Key Facts Valkey, a high-performance key/value datastore, will replace Redis in the extra repository The change follows Redis’s license modification from BSD-3-Clause to RSALv2 and SSPLv1 on March 20, 2024 Arch Linux package maintainers will support the redis package for approximately 14 days from April 17, 2025, after which it will be moved to the AUR and no longer receive updates Summary Arch Linux announced that Valkey, a high-performance key/value datastore, will replace Redis in the extra repository due to Redis changing its license to RSALv2 and SSPLv1 on March 20, 2024. The redis package will be supported for about 14 days from April 17, 2025, allowing users to transition smoothly to Valkey; after this period, the redis package will be moved to the AUR and considered deprecated with no further updates. Users are advised to begin migrating their Redis usage promptly to avoid issues post-transition.\nMan who built ISP instead of paying Comcast $50K expands to hundreds of homes - Ars Technica Key Facts Jared Mauch received $2.6 million in government funding to expand his fiber-to-the-home ISP in rural Michigan. The project will extend his network by 38 miles, serving approximately 596 addresses, with a total funding of $2,618,958.03. Mauch’s service offers 100 Mbps symmetrical internet for $55/month and 1 Gbps for $79/month, with installation fees around $199; he participates in the FCC’s Affordable Connectivity Program. Summary Jared Mauch, who built a fiber-to-the-home ISP after being dissatisfied with poor broadband from AT\u0026T and Comcast, is expanding his network in rural Michigan with $2.6 million from the US government’s American Rescue Plan funds. The project, contracted by Washtenaw County, aims to wire approximately 596 addresses across four townships, adding 38 miles of fiber to his existing 14-mile network. The county allocated $71 million for broadband infrastructure, with Mauch’s project being one of four selected through a competitive RFP process that prioritized wireline speeds of at least 100 Mbps symmetrical. Mauch’s service costs $55/month for 100 Mbps unlimited, and $79/month for 1 Gbps, with a typical installation fee of $199. The project must be completed by the end of 2026, but Mauch aims to finish half by the end of 2023. Previously, he faced high costs from major providers—Comcast demanded $50,000 for line extension, and AT\u0026T offered only 1.5 Mbps DSL. Mauch also provides free 250 Mbps service to a local church and fiber backhaul to cell towers. His network management has been stable, with traffic around 500 Mbps, scalable to 4 Gbps. The expansion is part of a broader effort by Washtenaw County to connect over 3,000 households, with a focus on underserved, lower-income areas.\nAtuin Desktop: Runbooks that Run Key Facts Atuin Desktop is a local-first, executable runbook editor designed to streamline terminal workflows, integrating script blocks, embedded terminals, database clients, and Prometheus charts. Features include chaining shell commands, executing and maintaining relevant documentation, reusable automation with Jinja-style templating, autocomplete from shell history, and synchronization via Atuin Hub. Currently used for releasing Atuin CLI, migrating infrastructure, deploying environments, and managing live database queries; upcoming features include team accounts and auto-generated runbooks from shell history. Summary Atuin Desktop, launched on April 22, 2025, is a local-first runbook editor that combines terminal-like execution with document-like organization, enabling repeatable, shareable workflows. It supports chaining commands, executing embedded scripts, and dynamic templating, with real-time autocomplete from shell history. The platform syncs across devices via Atuin Hub and is used for deploying CLI tools, infrastructure migration, environment setup, and database management. Future updates include team collaboration and automated runbook generation from shell history.\nBotnet Part 2: The Web is Broken - Jan Wildeboer’s Blog Key Facts Companies like Infatica monetize network bandwidth via SDKs embedded in iOS, Android, MacOS, and Windows apps, enabling web crawling and scraping using infected devices. These SDKs sell access to millions of residential, static, and mobile IP addresses, facilitating large-scale AI web scraping and brute-force attacks. Trend Micro’s 2023 research confirms malicious repacking of freeware/shareware to conduct drive-by downloads of these proxy services, contributing to increased bot traffic and server overloads. Summary Jan Wildeboer highlights the proliferation of shady business models where app developers embed SDKs, such as Infatica’s, into their applications to monetize users’ network bandwidth. These SDKs enable the creation of botnets that leverage infected devices to perform web crawling, brute-force mail server attacks, and AI-driven web scraping, often using millions of residential, static, and mobile IP addresses. Trend Micro’s 2023 investigation confirms malicious repacking of freeware/shareware to facilitate drive-by downloads of these proxy services, exacerbating botnet activity. Wildeboer argues that this business model effectively causes DDoS-like traffic surges, impacting small web services and increasing the difficulty for server administrators to detect and block such malicious activity. He advocates that all web scraping should be considered abusive and recommends blocking such traffic, emphasizing that inclusion of SDKs for profit in apps makes developers complicit in malware distribution and botnet formation. The market for residential proxies, driven by AI web scraping demands, is expanding rapidly, with many providers relying on SDK injection into third-party apps. Wildeboer concludes that this trend undermines the web’s foundational integrity, urging webmasters and admins to remain vigilant against these evolving threats.\nWhistleblower: DOGE Siphoned NLRB Case Data – Krebs on Security Key Facts A whistleblower from the National Labor Relations Board (NLRB) alleges that DOGE employees transferred gigabytes of sensitive case data in early March using short-lived accounts with restricted logging. The data exfiltration involved approximately 10 GB from the NxGen case management system, with suspicious activity including creation of high-privilege accounts and use of containers to obfuscate activity. The whistleblower reports blocked login attempts from a Russian IP address shortly after account creation, along with unauthorized download of code libraries used for web scraping and brute-force attacks. The NLRB investigated but was reportedly instructed to cease reporting to US-CERT, and the agency’s control was later removed from all employee accounts. Summary A security architect at the NLRB, Daniel J. Berulis, alleges that employees from Elon Musk’s Department of Government Efficiency (DOGE) siphoned sensitive case data in early March by creating high-privilege accounts with logging restrictions and using containerized environments to conceal activities. The incident involved transferring approximately 10 GB of data from the NxGen system, which contains confidential information on unions, legal cases, and corporate secrets. Suspicious activity included multiple blocked login attempts from a Russian IP address (83.149.30.186) shortly after account creation, with attempts to use valid credentials, and the download of code libraries designed for web scraping and brute-force attacks from GitHub. Berulis observed that network logs for recent resources went missing, and Microsoft Azure monitoring was turned off during the incident. Despite raising alarms and reporting to US-CERT, the NLRB was allegedly ordered to halt further investigation, and control over its systems was later revoked from staff. The whistleblower’s disclosures, supported by internal documentation and expert review, highlight potential security breaches involving high-level access and covert data exfiltration, amid broader political tensions and legal disputes involving Musk’s companies and government agencies.\nAn Intro to DeepSeek’s Distributed File System | Some blog Key Facts 3FS (Fire-Flyer File System) is an open-source distributed filesystem released by DeepSeek on April 15, 2025 Core components include Meta, Mgmtd, Storage, and Client nodes, with Mgmtd managing node registration and cluster configuration Utilizes CRAQ (Chain Replication with Apportioned Queries) protocol for strong consistency, with write throughput limited by the slowest node in the chain and performance affected by workload types Summary DeepSeek introduced 3FS, a distributed filesystem designed to abstract data across multiple machines, enabling applications to interact with it as if it were a local filesystem. It consists of four primary node types: Meta (manages file metadata stored in inodes and DirEntries within FoundationDB), Mgmtd (controls cluster configuration and node health via heartbeats and node discovery), Storage (handles physical data chunks using a Rust-based ChunkEngine, with metadata stored in LevelDB), and Client (interfaces with other nodes for file operations and data transfer). The system employs CRAQ for fault-tolerant, strongly consistent data replication, where write operations propagate from head to tail nodes, marking entries as “dirty” until committed, with read operations querying the tail for the most recent clean data. CRAQ’s performance varies with workload, offering scalable, low-latency reads but higher write latency, especially under zipfian access patterns. The architecture emphasizes fault tolerance, scalability, and simplicity, with detailed design notes available here. Future analyses aim to benchmark 3FS performance, evaluate bottlenecks, and compare it with other distributed filesystems.\nUpdate on Spain and LALIGA blocks of the internet - Vercel Key Facts Spanish court authorized LALIGA to block IP addresses associated with unauthorized football streaming, affecting Vercel infrastructure since December 2024. IP addresses 66.33.60.129 and 76.76.21.142 are no longer blocked as of April 18, 2025, following Vercel’s cooperation to remove illegal content. Broad IP-wide blocks are enforced during LALIGA matchdays, impacting legitimate websites and services that share IP addresses, with no distinction between infringing and lawful content. Summary A Spanish court granted LALIGA authority in December 2024 to require ISPs, including Movistar, Vodafone, and Orange, to block IP addresses linked to unauthorized football streaming, a ruling upheld in March 2025. Enforcement has expanded to affect Vercel-hosted sites, resulting in indiscriminate IP blocking that impacts legitimate services such as Tinybird and Hello Magazine, which operate on shared IPs like 66.33.60.129 and 76.76.21.142. Unlike targeted domain blocking via SNI inspection, ISPs are blocking entire IP ranges without differentiation, causing collateral damage to infrastructure, developers, and businesses during LALIGA matchdays. Vercel actively monitors and removes illegal content, maintaining a zero-tolerance policy, and is working with LALIGA to mitigate the impact. The company advocates for targeted, transparent enforcement and is exploring strategies to restore access for affected users in Spain, emphasizing the importance of an open, permissionless web.\n▶️ Open Source 15,000 lines of verified cryptography now in Python | Jonathan Protzenko Key Facts Python’s hash and HMAC algorithms are now fully implemented using HACL*, a verified cryptographic library, replacing previous implementations. The transition, completed after 2.5 years of work, includes 15,000 lines of verified C code integrated into Python without loss of functionality. Upstream updates from HACL* are automated via a script, ensuring maintainability and consistency. Summary Python has integrated HACL*, a verified cryptographic library, to implement all default hash and HMAC algorithms, following a GitHub issue opened in November 2022 addressing cryptographic verification after a SHA3 CVE. This integration adds approximately 15,000 lines of verified C code, enabling features such as additional Blake2 modes, a comprehensive SHA3 API covering all Keccak variants, strict abstraction patterns for build system compatibility, proper error handling including allocation failures, and optimized HMAC implementations maintaining two hash states simultaneously. The process involved extensive low-level technical work, including generic streaming API verification using dependent types, handling complex buffer management, and refactoring C code to abstract structs for compatibility with older compilers. The build system’s CI coverage uncovered corner cases, notably with AVX2-specific code, requiring careful refactoring to ensure cross-platform compatibility. Memory allocation failures are now propagated through the verification framework via option types, enhancing robustness. Upstream HACL* updates are managed through a shell script that fetches, refines, and integrates code changes, simplifying maintenance. This milestone demonstrates verified cryptography’s maturity and readiness for real-world deployment in critical software like Python.\nA New Form of Verification on Bluesky - Bluesky Key Facts Bluesky introduces a new, user-friendly blue check for verified accounts, launched on April 21, 2025 Over 270,000 accounts linked their domain as their username since the 2023 launch of domain handle verification Verified accounts now display a blue check, with additional verification via trusted verifiers—organizations that can directly issue blue checks, marked by scalloped blue checks Summary Bluesky announced a new verification layer on April 21, 2025, featuring a recognizable blue check to indicate authentic and notable accounts. Building on the initial domain handle verification launched in 2023, which linked over 270,000 accounts to their websites, the platform now offers a visual trust signal through a blue check. This check is issued either automatically for verified accounts or through trusted verifiers—organizations like The New York Times that can directly verify accounts within the app, with moderation reviews ensuring authenticity. When users tap on a verified account’s blue check, they can see which organization granted the verification. Users can also hide verification signals via Settings. Self-verification remains encouraged by setting a domain as the username; however, Bluesky is not accepting direct verification applications during this phase. Future plans include a request form for notable accounts and trusted verifiers once the feature stabilizes. The initiative aims to enhance trust and authenticity in decentralized social conversations, aligning with Bluesky’s broader goal of transitioning the social web from platform-centric to protocol-based systems.\nI left Spotify. What happened next? Key Facts The author transitioned from Spotify to self-hosted Jellyfin for music management Built a web-based music player using htmx to stream music locally and remotely Uses apps like Finamp (link) to download music for offline listening Purchased a mini PC to self-host Jellyfin and other apps like Immich for photo management Emphasizes ease of setup without advanced technical skills, using existing hardware like an old computer Highlights potential for future digital autonomy by self-hosting media services Summary After leaving Spotify, the author explored various local music players such as Winamp, VLC, and foobar2000, but found them inadequate for browsing and managing large libraries. They developed a web-based music streaming solution using htmx, enabling remote access to their library via a local server, though this approach lacked offline functionality. Switching to Apple’s Music app provided reliable offline access but required managing storage across devices, which was cumbersome. Inspired by a YouTube video, they adopted Jellyfin, an open-source media server that can replace Spotify, Netflix, and other streaming services. Self-hosting Jellyfin on an old computer or mini PC allows full control over media libraries and offline access through apps like Finamp and Fintunes. The author now runs Jellyfin and Immich for photo management, emphasizing that self-hosting is accessible without advanced technical skills and promotes digital autonomy. They advocate for open-source solutions to reduce dependence on commercial platforms and envision a future where users fully control their media content.\nUsing ~/.ssh/authorized keys to decide what the incoming connection can do – Dan Langille’s Other Diary Key Facts Demonstrates using ~/.ssh/authorized_keys to assign specific commands for incoming SSH connections on FreeBSD 14.2 Configures SSH keys with command restrictions, such as running rrsync in read-only mode for backups Shows how multiple SSH keys can be used for different tasks by specifying distinct commands in authorized_keys Summary The article explains how to leverage the ~/.ssh/authorized_keys file to control the actions of incoming SSH connections by associating specific commands with SSH keys on FreeBSD 14.2. It illustrates configuring a key to run /usr/local/sbin/rrsync -ro /path/ to restrict an SSH session to a read-only rsync operation, enhancing security for database backup transfers. The author demonstrates managing multiple tasks by adding separate SSH keys with different command directives, such as initiating a script to pull database backups from another host. The setup involves specifying the from attribute to restrict access to particular hosts and embedding the command directly within the authorized_keys entry. The approach ensures that each SSH key is tied to a precise, limited operation, preventing unauthorized actions. The article emphasizes the importance of using distinct SSH keys for different tasks, simplifying access management and maintaining security boundaries for critical systems like database backups.\nPython’s new t-strings | Dave Peck Key Facts Python 3.14, featuring t-strings, will be released in late 2025 T-strings are a generalization of f-strings, evaluating to string.templatelib.Template objects T-strings improve safety by requiring explicit processing before use, preventing injection vulnerabilities Summary Python’s PEP 750 introduces t-strings (template strings) as a new feature in Python 3.14, arriving in late 2025. T-strings extend the capabilities of f-strings by evaluating to string.templatelib.Template objects rather than immediate strings, enabling safer and more flexible string processing. Unlike f-strings, which can be dangerously misused with user input (e.g., SQL injection or XSS), t-strings require explicit processing before conversion to a string, allowing developers to implement safe escaping functions such as html(). T-strings provide access to their components via .strings and .values properties, supporting complex manipulations and custom processing, including iteration and detailed interpolation analysis. They can be instantiated with literal syntax (t\"...\") or directly through the Template constructor with Interpolation objects. An example demonstrates converting a template into pig Latin by processing each interpolation. The feature aims to enhance string safety and flexibility in Python libraries and frameworks, especially those handling user input, and is expected to influence tooling support like code formatters and IDEs.\nDefold - Official Homepage - Cross platform game engine Key Facts Defold is a free, production-ready, cross-platform game engine supporting major platforms including PlayStation®5, PlayStation®4, Nintendo Switch, Android, iOS, macOS, Linux, Windows, Steam, HTML5, Facebook, and Q3 2024 XBox. Comes fully featured out of the box with visual editor, code editor, Lua scripting, Lua debugger, scene, particle, tilemap editors, supporting both 2D and 3D development. No setup required; offers zero-config cloud build, native code extension, and integration with tools like Atom, VS Code, Spine, TexturePacker, and Tiled. Summary Defold is a free, open-source, cross-platform game engine designed for high-performance game development, supporting platforms such as PlayStation®5, PlayStation®4, Nintendo Switch, Android, iOS, macOS, Linux, Windows, Steam, HTML5, Facebook, and expected Q3 2024 XBox release. It provides a comprehensive, ready-to-use environment with features including a visual editor, code editor, Lua scripting, Lua debugger, scene, particle, and tilemap editors, supporting both 2D and 3D game creation. The engine requires no initial setup, offering a zero-configuration cloud build system and native code extension capabilities. It integrates with popular development tools like Atom, VS Code, Spine, TexturePacker, and Tiled, enabling customization and extension. Defold is supported by a broad community, with active development releasing updates approximately monthly, and includes support contracts for enterprise use. Notable projects include the game Family Island, which has over 50 million downloads on Google Play as of September 2023. The engine emphasizes accessibility, with no licensing fees, royalties, or runtime costs, and is governed by the Defold Foundation (source).\nGitHub - The-Pocket/Tutorial-Codebase-Knowledge: Turns Codebase into Easy Tutorial with AI Key Facts The project automates turning codebases into beginner-friendly tutorials using AI analysis. It crawls GitHub repositories or local directories, analyzing core abstractions and interactions. Built on Pocket Flow, a 100-line LLM framework, it generates tutorials in multiple languages, with commands like python main.py --repo URL --include \"*.py\" \"*.js\". Summary The system enables AI-driven transformation of complex codebases into easy-to-understand tutorials by crawling GitHub repositories or local directories. It leverages Pocket Flow, a minimalistic 100-line LLM framework, to analyze code structure, identify core abstractions, and visualize interactions. Users can specify repositories, file inclusion/exclusion patterns, maximum file size, and output language via command-line arguments such as --repo, --include, --exclude, and --language. The tool then generates comprehensive tutorials that explain how the code works, suitable for beginners. It supports setup with API keys for models like Gemini Pro 2.5 and can be customized for different models and languages. The project has gained notable attention, including front-page Hacker News coverage in April 2025, and provides example tutorials for repositories like AutoGen Core, Browser Use, and Celery. It emphasizes rapid development through agentic coding paradigms and visualizes code interactions, making complex repositories accessible. The system is open-source under the MIT license, with detailed setup instructions and resource links for further learning.\nGitHub - alexykn/sapphire: Rust based package manager for macOS Key Facts Sapphire is an alpha-stage, Rust-based package manager for macOS, focusing on ARM architecture. Manages formulae (command-line tools, libraries, languages) and casks (macOS applications). Core library handles fetching, dependency resolution, archive extraction, artifact management; CLI wraps core functionalities. Supports bottle and cask installation/uninstallation, parallel downloads, automatic dependency resolution, and early-stage source build capabilities. Project structure includes sapphire-core and sapphire-cli; currently under active development with planned features like upgrade, cleanup, reinstallation, and prefix support. Uses BSD-3-Clause license; inspired by Homebrew; heavily experimental and unstable.\nRepository link Summary Sapphire is an experimental, Rust-powered package manager for macOS, supporting ARM architecture. It manages formulae and casks, with core functions for fetching, dependency resolution, and artifact handling, and a CLI interface. Features include bottle and cask installation/uninstallation, parallel downloads, and early support for building from source. Its architecture comprises sapphire-core and sapphire-cli. The project is under active development with future plans for upgrade, cleanup, reinstallation commands, and prefix support. It is licensed under BSD-3-Clause and inspired by Homebrew, but remains unstable and alpha software.\nGitHub - ericjenott/Evertop: E-ink IBM XT clone with solar power, ultra low power consumption, and ultra long battery life. Key Facts Evertop is a portable IBM XT clone powered by an 80186 microcontroller, with 1MB RAM, running DOS, Minix, Windows up to 3.0, and other 1980s OS. Features include a 5.83-inch 648x480 e-ink display, built-in keyboard, external PS/2 ports, full graphics support (CGA, Hercules, MCGA, partial EGA/VGA), audio outputs, serial ports, USB flash drive, Ethernet, WiFi, LoRA radio, Bluetooth (planned), and multiple charging options. Power management enables 200-500 hours of continuous use on a single charge in power-saving mode, with a 6V, 6W solar panel capable of producing up to 700mA in full sunlight, supporting indefinite off-grid operation. Summary Evertop is an ultra low-power, solar-powered portable PC emulating an IBM XT with an 80186 processor, 1MB RAM, and a 648x480 e-ink display, capable of running DOS, Minix, Windows 3.0, and other 1980s operating systems. It integrates extensive peripherals, including a built-in keyboard, external PS/2 ports, full CGA, Hercules, MCGA graphics, partial EGA/VGA support, audio outputs (PC speaker, Adlib, Covox, Disney Sound Source), serial ports, USB flash drive, Ethernet, WiFi, and LoRA radio, with Bluetooth planned. Power options include a detachable 6V, 6W solar panel, internal buck/boost circuit accepting 2.5-20V DC, and micro USB, with simultaneous charging capability. Power management techniques, such as hibernate, automatic shutdown, and physical switches, enable 200-500 hours of active use per charge, with longer durations possible for dedicated applications like e-readers. The solar panel can generate up to 700mA under full sunlight, providing 10-50 hours of use per hour of sunlight, supporting indefinite off-grid use. Storage is handled via a 256GB SD card, supporting multiple emulated systems with up to 8GB total. The system is based on an Espressif ESP32 microcontroller, with a custom firmware derived from Fabrizio Di Vittorio’s PCEmulator, housed in a 3D-printed matte PETG enclosure. A minimal version, “Evertop Min,” removes the built-in keyboard, serial ports, Ethernet, LoRA, voltmeter, and reduces battery capacity, maintaining core features for lightweight, off-grid computing.\nGitHub - nari-labs/dia: A TTS model capable of generating ultra-realistic dialogue in one pass. Key Facts Dia is a 1.6 billion parameter text-to-speech (TTS) model developed by Nari Labs, capable of generating ultra-realistic dialogue in a single pass. Supports conditioning on audio for emotion and tone control, and can produce nonverbal sounds like laughter, coughing, and clearing throat. Model weights are available on Hugging Face, supporting only English at present; inference code and pretrained checkpoints are provided for research use. Summary Dia is an open-weight, 1.6B parameter TTS model designed to generate highly realistic dialogue directly from transcripts, with the ability to condition output on audio inputs for emotion and tone modulation. It can also synthesize nonverbal cues such as laughter and coughing, enhancing dialogue authenticity. The model is hosted on Hugging Face and supports English language generation exclusively. Researchers can access pretrained checkpoints and inference code to facilitate development, with the model capable of real-time audio synthesis on enterprise GPUs, requiring approximately 10GB VRAM. The model supports dialogue generation via [S1] and [S2] tags, voice cloning, and nonverbal sound production. It is intended for research and educational purposes, with restrictions against identity misuse, deceptive content, and illegal activities. The project is licensed under Apache-2.0, with ongoing plans for Docker support, inference speed optimization, and quantization for memory efficiency. Users can run the Gradio UI for testing or integrate the model as a Python library, with hardware acceleration via torch.compile expected to improve speed.\nGitHub - openai/codex: Lightweight coding agent that runs in your terminal Key Facts OpenAI’s Codex CLI is a lightweight coding agent designed to run in terminal environments. It supports multiple providers, including OpenAI, OpenRouter, Gemini, Ollama, Mistral, DeepSeek, XAI, and Groq, with configurable API keys. The project is licensed under Apache-2.0, with over 19,300 stars and active development on GitHub. Summary OpenAI’s Codex CLI is an open-source, terminal-based coding agent enabling developers to generate, modify, and execute code through natural language prompts. It supports various models and providers, allowing flexible integration with different AI backends, such as OpenAI, Gemini, Ollama, and others, with configurable API keys via environment variables or config files. The CLI offers interactive and non-interactive modes, including full auto-approval for code changes, with safety features like sandboxing on macOS (via Apple Seatbelt) and Linux (using Docker). It requires Node.js 22+, operates across macOS 12+, Ubuntu 20.04+, and Windows 11 with WSL2, and manages dependencies with package managers like pnpm, migrated from npm for efficiency. The project includes comprehensive documentation, CLI commands, and configuration options, along with development workflows emphasizing high-quality contributions, testing, and code standards. Recent updates include support for pnpm workspace management, a Nix flake for reproducible environments, a new /diff command, and version check improvements supporting multiple package managers. The repository is actively maintained, with over 80 contributors, and emphasizes responsible AI use, security, and community engagement.\n15,000 lines of verified cryptography now in Python | Jonathan Protzenko Key Facts Python’s hash and HMAC algorithms are now fully implemented using HACL*, a verified cryptographic library, replacing previous implementations. The transition, completed after 2.5 years of development, includes 15,000 lines of verified C code integrated into Python without functionality loss. Upstream updates from HACL* are automated via a script, ensuring maintainability and synchronization with the verified library. Summary Python has integrated HACL*, a verified cryptographic library, to implement all default hash and HMAC algorithms, replacing previous code after a GitHub issue was opened in November 2022. This move enhances security by ensuring cryptographic primitives are formally verified, covering algorithms such as SHA3, Blake2, and their variants, with support for additional modes and error management. The integration involved adapting HACL*’s generic streaming API, which uses dependent types for block algorithms, to handle Python’s diverse cryptographic needs, including pre-input handling, variable output lengths, and state retention. Technical challenges included refactoring C code generated from F* to use abstract structs for compatibility across compilers and architectures, and propagating memory allocation failures through the verification model. The process also required refining build systems to handle multiple toolchains, architectures, and compiler behaviors, especially for AVX2-specific code in implementations like Blake2b-256. Upstream updates from HACL* are managed via a shell script, simplifying maintenance. This large-scale adoption demonstrates verified cryptography’s maturity for real-world applications, ensuring cryptographic correctness and robustness in Python’s core infrastructure.\nNLnet; 42 Free and Open Source Projects Receive Funding to Reclaim the Public Nature of the Internet Key Facts 42 projects funded by NLnet’s October NGI Zero Commons Fund call, the largest in NGI Zero history Projects aim to reclaim the public nature of the internet, focusing on open hardware, privacy, decentralization, and open source tools Notable projects include open hardware (Solar FemtoTX motherboard, FuseSoC Web Catalog), privacy tools (LiberaForms, Alaveteli GDPR search), decentralized identity (DID SASL), and AI/data workflows (Livebook, LLM2FPGA) Summary NLnet announced funding for 42 open source projects through the October NGI Zero Commons Fund, supporting initiatives that promote the public, privacy-respecting internet. Projects span open hardware (e.g., Solar FemtoTX low-power motherboard, FuseSoC Web Catalog), privacy-enhancing forms (LiberaForms), decentralized identity (DID SASL), and AI workflows (Livebook, LLM2FPGA). Additional efforts include improving web standards for print, developing federated XR content, and enhancing open communication protocols. This funding aims to strengthen digital commons, transparency, and user control over online infrastructure and data.\nNine Reasons to Use OSH Key Facts OSH is a modern, compatible Unix shell implementation focused on scripting and interactive features, part of the Oils project It supports running existing shell scripts, including POSIX and Bash, with nine years of increasing compatibility Provides precise error messages, pretty printing for debugging, strict mode for error checking, and additional runtime features Small size (~2MB), minimal dependencies, suitable for building GUIs and headless protocols Upgradable to YSH, a versatile language combining shell, Python, JSON, and YAML Open for contributions, with high-level, statically typed code, and performance advantages over C-based shells Summary OSH is a high-compatibility, script-focused Unix shell that runs existing scripts from POSIX and Bash, with nine years of incremental improvements. It offers precise error reporting, debugging tools like pretty printing, and a strict mode to enhance script reliability. The lightweight design (~2MB, minimal dependencies) facilitates system building and GUI development. Users can upgrade to YSH, a flexible language integrating shell, Python, and data formats. The project encourages contributions, featuring high-level, statically typed code and superior performance benchmarks compared to C shells.\nGetting Forked by Microsoft • Philip Laine Key Facts Philip Laine’s open source project Spegel, a P2P container image sharing tool, was forked and maintained by Microsoft under an MIT license. Microsoft’s Peerd project contains code, test cases, and comments directly copied from Spegel without attribution, leading to confusion among users. Laine’s Spegel has over 1,700 stars and 14.4 million pulls since its release over two years ago; he questions the implications of corporate forks on individual maintainers and open source sustainability. Summary Philip Laine’s open source project Spegel, a peer-to-peer container image sharing solution designed to improve scalability and reduce downtime caused by registry outages, was acknowledged by Microsoft during a conference talk. Subsequently, Microsoft developed Peerd, a fork of Spegel, incorporating code, test cases, and comments directly copied from Laine’s project without attribution, despite Spegel being licensed under MIT. This has caused confusion among users and raised concerns about intellectual property and community trust. Laine, who maintained Spegel with community support and over 1,700 stars, felt marginalized as his work was effectively appropriated by a corporate entity. The incident highlights challenges faced by individual open source maintainers when collaborating with or being forked by large corporations, especially amid declining open source investment and licensing complexities. Laine has considered changing Spegel’s license and has enabled GitHub Sponsors to fund ongoing development.\nEx-OpenAI staff and top AI experts seek to block proposed for-profit restructure Key Facts Ex-OpenAI employees and leading AI experts, including Geoffrey Hinton, Margaret Mitchell, and Stuart Russell, oppose the company’s proposed transition from a non-profit to a for-profit public benefit corporation (PBC). The joint letter to California and Delaware attorneys-general warns that the restructuring would transfer control of artificial general intelligence (AGI) development to a profit-driven entity, contradicting OpenAI’s founding mission to benefit all humanity. OpenAI aims to complete the for-profit conversion by the end of 2023 to secure additional investments, including a $30bn funding round from SoftBank, amid concerns that the change could undermine its mission and increase societal risks associated with AGI. Summary Former OpenAI staff and top AI researchers, including Geoffrey Hinton and Stuart Russell, have petitioned US authorities to block the company’s plan to convert from a non-profit to a for-profit public benefit corporation (PBC). They argue this shift would prioritize profit over the company’s original mission to ensure AGI benefits all humanity, risking increased societal and safety concerns. OpenAI, valued at $300bn and seeking to secure further investments, contends that the restructuring will strengthen its non-profit and align with its mission, but critics warn it could lead to loss of control over AGI development and heightened risks of misuse.\nA new, open source text-to-speech model called Dia has arrived to challenge ElevenLabs, OpenAI and more | VentureBeat Key Facts Nari Labs released Dia, a 1.6 billion parameter open-source TTS model, designed for naturalistic dialogue generation from text prompts Supports nuanced features like emotional tone, speaker tagging, nonverbal cues, and audio conditioning for voice cloning Model runs on PyTorch 2.0+ with CUDA 12.6, requiring ~10GB VRAM; inference on enterprise GPUs (~NVIDIA A4000) achieves ~40 tokens/sec Code and weights available on GitHub and Hugging Face; accessible via demo on Hugging Face Spaces Fully open source under Apache 2.0 license, permitting commercial use with restrictions on impersonation and misinformation; development supported by Google TPU Cloud and Hugging Face grants Summary Nari Labs introduced Dia, an open-source 1.6B parameter TTS model capable of producing highly natural dialogue with features like emotional tone, speaker tags, and nonverbal cues. Built with PyTorch 2.0+ and CUDA 12.6, it requires approximately 10GB VRAM, delivering around 40 tokens/sec on enterprise GPUs such as NVIDIA A4000. The model’s code and weights are available on GitHub and Hugging Face, with a demo accessible via Hugging Face Spaces. Distributed under Apache 2.0 license, Dia aims to support diverse applications from content creation to assistive tech, emphasizing responsible use.\nClaude Code Best Practices \\ Anthropic Key Facts Claude Code is a command line tool for agentic coding, released by Anthropic on April 18, 2025 Designed as a low-level, unopinionated, flexible, and scriptable system for integrating Claude into coding workflows Emphasizes environment customization via CLAUDE.md files, tool allowlist management, and integration with MCP, GitHub, and other tools Summary Claude Code is a research-developed command line tool aimed at agentic coding, providing raw model access without enforcing specific workflows. It allows users to customize their setup through CLAUDE.md files, which document commands, style, and environment specifics, and can be placed in various directory levels or the home folder. Users can tune these files for efficiency and clarity, and curate Claude’s allowed tools, including file system actions, bash commands, MCP tools, and GitHub interactions. Claude Code inherits the user’s shell environment, enabling integration with custom scripts, MCP servers, slash commands, and GitHub. It supports common workflows such as exploration, planning, coding, testing, and iteration, with best practices emphasizing specificity, visual aids, early course correction, and context management via /clear. The system also offers headless mode for automation in CI/CD pipelines, issue triage, linting, and large-scale data processing, with features like -p flag and --output-format stream-json. Multi-Claude workflows involve parallel instances for code generation, verification, and repository management, utilizing git worktrees or multiple checkouts for efficiency. Overall, Claude Code aims to enhance agentic coding productivity through flexible, customizable, and multi-instance workflows, with detailed documentation available at claude.ai/code.\nOpenAI Would Buy Google’s Chrome Browser, ChatGPT Chief Says - Bloomberg Key Facts OpenAI’s ChatGPT chief, Nick Turley, stated in court that OpenAI would be interested in acquiring Google’s Chrome browser if a federal court orders its spin-off Many other parties would also seek to buy Chrome under such circumstances The statement was made during a court hearing related to antitrust actions against Google Summary OpenAI’s ChatGPT chief, Nick Turley, indicated in a court hearing that OpenAI would pursue purchasing Google’s Chrome browser if a federal court mandates its divestment, with many other entities also interested. This statement aligns with ongoing antitrust litigation efforts aimed at breaking up Google’s dominance in search and advertising. The potential sale of Chrome is linked to broader antitrust cases, including DOJ efforts to force Google to sell off parts of its business to reduce monopoly power.\nNew research backs up what gamers have thought for years: cozy video games can be an antidote to stress and anxiety. Key Facts Recent research supports that cozy video games can reduce stress and anxiety. Studies indicate that playing such games increases mental well-being, with an extra hour of gameplay linked to higher life satisfaction. The genre, originating with titles like Harvest Moon (1996) and popularized by Animal Crossing: New Horizons (2020), emphasizes relaxation, community-building, and non-violent challenges. Summary New research confirms that cozy video games can serve as effective tools for alleviating stress and anxiety. These games, characterized by their relaxing gameplay, community focus, and non-violent mechanics, attract both long-time gamers and newcomers. The genre gained prominence with titles like Harvest Moon (1996) and surged in popularity after Animal Crossing: New Horizons launched on March 20, 2020, coinciding with COVID-19 lockdowns, and sold over 13 million units within six weeks. Studies, including Hiroyuki Egami’s 2022 research in Japan, show that owning a game console and increasing gameplay time by one hour daily correlates with reduced psychological distress and enhanced life satisfaction. Other research, such as Michael Wong’s 2021 survey at McMaster University, found no significant difference in stress reduction between casual gaming and mindfulness meditation. Therapeutic applications are emerging, with video games being explored as interventions for ADHD and as tools for emotional processing, exemplified by Spiritfarer, which helps players explore death and grief. These games often feature inclusive design, customizable characters, and tasks like gardening and house decoration, fostering a sense of comfort and community. Developers like Dorian Signargout emphasize promoting inclusivity through diverse character representations. Overall, cozy games are increasingly recognized for their mental health benefits, offering a sanctuary of simplicity and connection amid a complex world.\nBuilding a Website Fit for 1999 - Wesley Moore Key Facts Wesley Moore built a retro-themed website in HTML4, hosted at home.wezm.net/~wmoore/, inspired by Ruben’s Retro Corner and a Raspberry Pi project. The site was developed to run on old hardware and browsers, tested in IE 4.01 and Netscape Navigator 3.01 in Mac OS 8.1 via Basilisk II emulator. The website uses static HTML generated with MiniJinja, jaq, and make, with dynamic content updated every 5 minutes via a Rust server using Axum, served through Nginx reverse proxy with Tailscale for network access. Static content is stored on a Qotom mini PC running Chimera Linux, with deployment via git pull and make, and the Rust binary managed as a systemd-like process with Dinit. The site includes static pages with animated GIFs, pixel icons, and server stats, emphasizing minimal CSS and table-based layouts for compatibility with legacy browsers. Summary Wesley Moore created a retro-styled website emulating the 1999 web aesthetic, implemented entirely in HTML4 and served over plain HTTP to ensure compatibility with outdated browsers like IE 4.01 and Netscape 3.01, tested in Mac OS 8.1 emulation. The site features static pages generated with MiniJinja, jaq, and make, with dynamic content such as server uptime, memory, and energy data refreshed every five minutes via a Rust server built with Axum, reverse-proxied through Nginx and connected over Tailscale. Hosting is on a Qotom mini PC running Chimera Linux, with deployment managed through git and a custom package build process. The website emphasizes minimal CSS, table-based layouts, and direct HTML4 coding, including animated GIFs and pixel icons, to preserve authenticity and functionality on legacy hardware. The project code is available on GitHub, with future plans to expand content on additional pages.\nI thought I bought a camera, but no! DJI sold me a LICENSE to use their camera 🤦‍♂️ - YouTube Key Facts The YouTube video titled “I thought I bought a camera, but no! DJI sold me a LICENSE to use their camera” was uploaded by Louis Rossmann on April 17, 2025, with 416,986 views. The content highlights that DJI sells users a license to operate their cameras rather than the camera hardware itself. The video has received 28K likes and discusses issues related to proprietary licensing models in consumer electronics. Summary Louis Rossmann’s video critiques DJI’s business model, revealing that purchasing a DJI camera does not grant ownership of the hardware but instead provides a license to use the camera. This licensing approach implies that users do not own the device outright, but are granted permission to operate it under specific terms. The video emphasizes the technical and legal implications of this model, suggesting it shifts ownership rights and control from consumers to the manufacturer. The discussion underscores broader concerns about proprietary licensing in consumer electronics, where users may face restrictions on repair, modification, or resale. The video’s key data points include the date of upload (April 17, 2025), view count (416,986), and the focus on the distinction between hardware ownership and licensing rights in DJI products.\n▶️ Software Development Pipelining might be my favorite programming language feature | MOND←TECH MAGAZINE Key Facts The article advocates for pipelining as a programming language feature that allows passing previous values as arguments, enhancing code readability and editing ease. Pipelining enables method chaining and function composition, exemplified by Rust’s trait-based approach, Haskell’s \u0026 and $ operators, and SQL’s pipe syntax. Benefits include improved code discovery, simplified refactoring, better IDE support, and clearer data flow, reducing nested parentheses and complex variable tracking. Summary The article presents pipelining as a highly valued programming language feature that simplifies code structure by passing previous values directly into subsequent functions or methods. It contrasts traditional nested or imperative code with pipelined syntax, emphasizing how the latter improves readability, ease of editing, and IDE support, especially in languages like Rust, Haskell, and SQL. Rust’s trait-based method calls exemplify effective pipelining without requiring higher-kinded types, while Haskell’s \u0026 and $ operators demonstrate functional composition that enhances clarity. SQL’s proposed pipe syntax reduces nested queries into linear, line-by-line transformations, aligning with LINQ-style readability. The article also discusses the builder pattern as a form of pipelining for object construction, and critiques Haskell’s complex operator ecosystem, advocating for more approachable, pipeline-friendly syntax. Overall, pipelining is praised for enabling modular, understandable, and maintainable code, with a focus on top-to-bottom data flow and minimal parentheses or variable tracking.\n▶️ Management and Leadership Android phones will soon reboot themselves after sitting unused for 3 days - Ars Technica Key Facts Android devices will automatically reboot after being locked and unused for 3 consecutive days via a Google Play Services update (version 25.14), rolling out gradually from April 14, 2025. The feature enhances security by encrypting data in the “Before First Unlock” (BFU) state, making data retrieval more difficult, especially after prolonged inactivity. The update is part of a broader set of improvements, including UI enhancements, better connectivity with cars and watches, and content previews in Quick Share. Summary A silent update to Google Play Services (version 25.14), released on April 14, 2025, introduces an auto-restart feature for Android devices, activating after three days of inactivity when the device remains locked. This feature aims to improve security by encrypting all data in the BFU state, where biometrics and location-based unlocking are disabled, and access is limited to PIN or passcode. The automatic reboot makes data extraction significantly more difficult, aligning with similar features like Apple’s Inactivity Reboot introduced in iOS 18.1. The update is delivered automatically to certified devices over the next week or more, with no user intervention required. This mechanism leverages Google Play Services’ background update system, which has been central to Android’s system update strategy, allowing Google to enhance security and functionality remotely. The broader update includes UI improvements, enhanced device connectivity, and content sharing features.\nDeciphering Glyph :: Stop Writing __init__ Methods Key Facts The article critiques the widespread use of custom __init__ methods in Python classes, especially before dataclasses (introduced in Python 3.7), highlighting their drawbacks. It presents a structured approach: using @dataclass for attribute management, @classmethod factory methods for object creation, and typing.NewType for enforcing constraints on primitive types. This methodology ensures valid object construction, improves discoverability, and enhances testability without relying on side-effect-laden __init__ methods. Summary The article argues that defining custom __init__ methods in Python classes is an anti-pattern, historically used to facilitate object instantiation with attributes like x and y in data structures such as 2DCoordinate. Prior to Python 3.7’s @dataclass, developers resorted to complex alternatives like factory functions or attribute defaults, which had significant drawbacks. Using __init__ for side-effect-laden setup, such as opening file descriptors in a FileReader class, introduces problems including reduced testability, increased complexity, and potential invalid object states. The proposed solution involves leveraging @dataclass to automatically generate attribute assignment, replacing __init__ with @classmethod factory methods (e.g., FileReader.open) for flexible, asynchronous, or context-specific instantiation, and employing typing.NewType to enforce constraints on primitive types like file descriptors. This approach ensures objects are always valid, simplifies testing, and makes object creation more discoverable and future-proof, aligning with Python’s idiomatic practices.\nFossil fuels fall below 50% of US electricity for the first month on record | Ember Key Facts In March 2025, fossil fuels accounted for 49.2% of US electricity, the first month below 50%, surpassing the previous record low of 51% in April 2024 Clean energy sources generated 50.8% of US electricity in March 2025, driven by record-high wind and solar power Wind and solar reached a combined 24.4% of US electricity, with solar increasing 37% (+8.3 TWh) and wind increasing 12% (+5.7 TWh) compared to March 2024; total wind and solar generation hit 83 TWh, an 11% rise over April 2024 Summary In March 2025, the US experienced a historic shift as fossil fuels contributed only 49.2% to electricity generation, marking the first month on record below the 50% threshold, according to Ember. This milestone reflects a long-term decline in fossil fuel reliance, with wind and solar power reaching a record 24.4% of the energy mix—solar alone increased by 37% (+8.3 TWh) and wind by 12% (+5.7 TWh) year-over-year. Combined, wind and solar generated 83 TWh, an 11% increase from April 2024, while fossil fuel generation decreased by 2.5% (-4.3 TWh). Since March 2015, when fossil fuels accounted for 65% and solar just 1%, the share of wind and solar has more than quadrupled, with solar expected to constitute over half of new US capacity in 2025. The rapid growth of renewable energy signifies a pivotal transition toward a cleaner power system, with wind and solar increasingly displacing coal and gas, and signals a potential approaching tipping point where clean energy surpasses fossil fuel reliance in the US electricity sector. More details are available at Ember.\nGitHub - LukasOgunfeitimi/TikTok-ReverseEngineering Key Facts The project reverse engineers TikTok’s custom virtual machine (VM) used for obfuscation and security. It includes tools for deobfuscating webmssdk.js, decompiling VM bytecode, injecting deobfuscated scripts, and generating signed URLs. Bytecode is stored as a long string XOR-encrypted with a key embedded within the string; decryption involves base64 decoding, XOR with a key derived from the string, and leb128 decoding for compression. Summary The repository provides tools to reverse engineer TikTok’s sophisticated VM-based obfuscation system. It deobfuscates heavily encrypted webmssdk.js by reversing bracket notation encoding, replacing function arrays with standard functions, and reconstructing the VM’s bytecode execution logic. The bytecode is stored as a long base64-encoded string, XOR-encrypted with an embedded key, and leb128 compressed, which is decoded to extract functions, strings, and metadata. The VM supports scopes, nested functions, and exception handling, indicating a complex, custom bytecode interpreter. Decompilation involves analyzing VM switch-case structures, with manual and AI-assisted efforts to produce readable code snippets. The system also includes a URL signing mechanism that replicates TikTok’s request signatures (msToken, X-Bogus, _signature) by executing specific VM functions, enabling authenticated requests such as posting comments. The project emphasizes that TikTok’s VM is subject to frequent updates, requiring ongoing reverse engineering efforts. It leverages browser extensions like Tampermonkey and CSP bypasses for debugging and script injection, facilitating real-time analysis and testing.\nNative visionOS platform support by rsanchezsaez · Pull Request #105628 · godotengine/godot · GitHub Key Facts This PR adds native support for the visionOS platform to Godot, using iOS as the base and reusing code via drivers/apple_embedded. Implements platform-specific logic, including app instantiation, display server, and export plugin, with no OpenGL support due to visionOS limitations. Renames export plugin files and shared configuration to accommodate both iOS and visionOS, and moves shared code into drivers/apple_embedded. Tests confirm native execution on visionOS with Metal renderer; DPI metrics are hardcoded, and icon asset support is pending. The PR is part of a three-step plan, with subsequent PRs planned for SwiftUI integration and immersive VR plugin support. Summary This PR introduces a native visionOS platform support for Godot, closely aligned with the existing iOS implementation. It refactors shared code into drivers/apple_embedded, renames relevant export and platform files, and disables OpenGL due to visionOS constraints. Testing with the Platformer demo confirms native execution using Metal, though DPI metrics are static and icon asset generation is incomplete. The contribution lays the groundwork for future immersive VR support and SwiftUI integration, with ongoing discussions about platform maintenance and compatibility considerations.\nGitHub - humanlayer/12-factor-agents: What are the principles we can use to build LLM-powered software that is actually good enough to put in the hands of production customers? Key Facts The project outlines 12 core principles for building reliable, scalable, and maintainable LLM-powered software suitable for production deployment. These principles include owning prompts, controlling context windows, structuring tool outputs, unifying execution and business state, and owning control flow. The guide emphasizes integrating modular concepts from agent design into existing products rather than relying solely on frameworks, to accelerate deployment of high-quality AI features. Summary The “12-factor agents” framework, inspired by the 12 Factor Apps, provides foundational principles for developing production-ready LLM-powered software. It addresses common challenges such as managing prompts, context windows, structured tool outputs, and control flow, advocating for ownership and modularity in these areas. The approach discourages building from scratch with full frameworks; instead, it recommends incorporating small, modular agent concepts into existing products to improve reliability, scalability, and maintainability. The twelve factors include transforming natural language into tool calls, owning prompts and context, unifying execution and business state, enabling simple APIs for launching and pausing agents, and designing small, focused agents that can trigger from anywhere and act as stateless reducers. The guide also discusses the historical evolution from DAG-based orchestrators to agent loops, highlighting the limitations of purely loop-based agent architectures. It emphasizes that core engineering techniques, rather than frameworks alone, are key to deploying effective AI agents in customer-facing environments. Additional resources and related frameworks are linked throughout, supporting developers in applying these principles across various languages and platforms.\nThings Zig comptime Won’t Do Key Facts Zig’s comptime is designed to be restrictive, preventing host architecture leakage, dynamic code evaluation (#eval), DSL creation, RTTI, API extension, and IO operations Comptime code observes the target architecture, not the host, ensuring cross-compilation correctness Zig lacks facilities for dynamic source code generation, custom syntax extension, runtime type information, and input/output during compilation Summary Zig’s compile-time evaluation (comptime) features are intentionally limited to maintain safety, portability, and simplicity. Comptime code executes in the target environment, not on the host machine, preventing host architecture leakage, as demonstrated by examples showing architecture-dependent behavior during cross-compilation. Zig does not support dynamic source code injection or evaluation (#eval), relying instead on partial evaluation and specialization, such as marking function parameters with comptime to generate optimized code paths. It also lacks support for custom syntax DSLs, as all code operates on Zig values, with embedded DSLs like print using format strings. Zig does not include runtime type information (RTTI), requiring users to implement manual reflection for dynamic data handling, exemplified by a custom RTTI union and print_dyn function. Additionally, Zig types cannot be extended with methods post-generation; API modifications are limited to reflection-based internal logic. Finally, Zig’s compile-time evaluation is hermetic, with no I/O capabilities, ensuring reproducibility and safety, though build systems can invoke external programs for code generation.\nmatthewsinclair.com · Intelligence. Innovation. Leadership. Influence. Key Facts Author used Claude Code to develop approximately 30,000 lines of code across backend and frontend projects within weeks AI tools function as amplifiers (“mech suit”) rather than replacements, requiring human oversight and architectural judgment Vigilance is necessary due to AI’s tendency to make bewildering or inappropriate decisions, necessitating constant review and control Summary The article argues that LLM-powered programming tools like Claude Code serve as amplifiers—“mech suits”—that enhance developer capabilities rather than replace humans. The author built two applications totaling around 30,000 lines of code in weeks, demonstrating significant acceleration in development speed. These tools provide tremendous “lifting power,” but require developers to maintain constant vigilance, guiding the AI and correcting its often bewildering or biased decisions, similar to piloting an aircraft. The process shifts focus from coding to designing, reviewing, and maintaining architectural integrity, emphasizing the importance of experience and domain knowledge to recognize when AI output is flawed. The author highlights the “centaur effect,” where human-AI collaboration outperforms either alone, with AI handling pattern recognition and tactical execution, and humans providing strategic oversight. The article stresses that effective use of these tools demands new skills—particularly in delegation, ruthless discarding of suboptimal solutions, and clear problem understanding. While some fear AI will replace programmers, the author sees the future as augmentation, where mastery of AI tools becomes a core skill, transforming the role of developers into strategic operators of powerful systems. The key to success lies in balancing delegation with control, leveraging AI for speed while applying human judgment to ensure quality and direction.\nMicrosoft just launched powerful AI ‘agents’ that could completely transform your workday – and challenge Google’s workplace dominance | VentureBeat Key Facts Microsoft announced the “Microsoft 365 Copilot Wave 2 Spring release” introducing AI agents with deep reasoning capabilities for workplace tasks. New AI agents, Researcher and Analyst, leverage OpenAI’s reasoning models to handle complex research and data analysis, connecting information across sources. These agents will be accessible via an “Agent Store” featuring integrations with Jira, Monday.com, Miro, and custom organizational agents; rollout begins in late May 2025. Summary Microsoft launched the “Microsoft 365 Copilot Wave 2 Spring release,” expanding AI tools with agents capable of deep reasoning for enterprise tasks. The Researcher and Analyst agents, powered by OpenAI’s reasoning models, perform complex research and data analysis, aiding in activities like preparing business reviews. They will be available through an “Agent Store” with integrations from partners such as Jira, Monday.com, and Miro, with deployment starting in late May 2025. Microsoft envisions AI as an organizational collaborator, transforming workplace productivity and structure.\nIntel (INTC) to Announce Plans This Week to Lay Off More Than 20% of Staff - Bloomberg Key Facts Intel plans to announce a reduction of over 20% of its staff this week The restructuring aims to eliminate bureaucracy and rebuild an engineering-driven culture This is the first major restructuring under CEO Lip-Bu Tan, who assumed leadership last month Summary Intel Corporation is set to disclose plans this week to cut more than 20% of its workforce, targeting organizational streamlining and management restructuring, according to sources. The initiative seeks to reduce bureaucracy and foster an engineering-focused culture. This marks the first significant restructuring under new CEO Lip-Bu Tan, who took over leadership last month. The move reflects Intel’s efforts to address its financial struggles and improve operational efficiency.\nTLS Certificate Lifetimes Will Officially Reduce to 47 Days | DigiCert Key Facts CA/Browser Forum voted to reduce TLS certificate lifetime to 47 days, with implementation starting March 15, 2026 Maximum TLS certificate validity decreases from 398 days (current) to 200 days (2026), then to 100 days (2027), and finally to 47 days (2029) Reuse period for domain/IP validation info drops from 398 days to 10 days by March 15, 2029; SII reuse in OV/EV certificates reduces from 825 days to 398 days Summary The CA/Browser Forum has officially amended the TLS Baseline Requirements to progressively shorten TLS certificate lifetimes, with the maximum validity decreasing from 398 days to 47 days by March 15, 2029. The schedule begins with a reduction to 200 days in 2026, then to 100 days in 2027, and finally to 47 days in 2029. Concurrently, the reuse period for domain and IP address validation information will decline from 398 days to 10 days, and validation of Subject Identity Information (SII) in OV and EV certificates will be limited to 398 days, down from 825 days. The rationale emphasizes increased trustworthiness of certificate data and mitigates reliance on unreliable revocation systems like CRLs and OCSP. Apple justified the change by highlighting the necessity of automation for managing shorter-lived certificates, with DigiCert supporting this transition through solutions like Trust Lifecycle Manager and CertCentral, including ACME support for automated issuance and renewal. The move aims to enhance security, reduce outages, and promote widespread adoption of automation in certificate management.\nWhy I Cannot Be Technical Key Facts Author asserts she cannot be truly “Technical” due to structural and social barriers within tech systems “Technical” is a legitimacy-based, exclusionary label rooted in social hierarchies and identity policing The essay critiques how tech systems perpetuate dehumanization, inequality, and reinforce boundaries based on gender, race, class, and ideology Summary Cat Hicks argues that she cannot genuinely be “Technical” because the label functions as a structural designation that enforces legitimacy through social exclusion, rather than problem-solving ability. She emphasizes that “Technical” operates outside of actual problem-solving, serving to uphold hierarchies, policing boundaries, and perpetuating systemic inequalities based on gender, race, class, and social location. Hicks highlights how the social and political context of tech creates unearned privileges and unwarranted exclusions, making it impossible for her to be recognized as legitimate within that framework despite her expertise in psychology and her impactful work. She critiques the hierarchical, hierarchical, and geo-located nature of tech, which maintains a culture of dehumanization and marginalization of those deemed “not-Technical.” Hicks advocates for recognizing the full humanity of all individuals, emphasizing that true work involves caring, safety, and community rather than perpetuating the “hamster wheel” of performance and exclusion. She calls for a space of rehumanization and honest conversation about the systemic issues within tech, emphasizing that “Technical” is a social construct that cannot be earned or bestowed but is maintained through systemic reinforcement. Hicks urges a focus on collective healing, shared storytelling, and challenging the structural barriers that devalue human connection in technology.\nAmerica Underestimates the Difficulty of Bringing Manufacturing Back — Molson Hart Key Facts President announced tariffs on imports ranging from 10% to 49% on April 2, 2025, aiming to revive U.S. manufacturing The article presents 14 reasons why these tariffs will not succeed in bringing manufacturing back and may worsen economic decline Expert with 15 years in manufacturing argues that tariffs are insufficient due to supply chain weaknesses, high costs, lack of knowhow, infrastructure deficits, and complex policy environment Summary The April 2025 U.S. tariff policy, imposing import taxes between 10% and 49%, aims to restore domestic manufacturing but is fundamentally flawed. Key issues include tariffs being too low to offset higher U.S. production costs, as manufacturing in the U.S. remains more expensive than in Asia even with tariffs. The U.S. supply chain for industrial components is weak, relying heavily on Asian factories, making local production uncompetitive. The country lacks essential manufacturing knowhow, such as moldmaking and semiconductor fabrication, which cannot be quickly or easily developed. Labor costs in the U.S. are higher not only due to wages but also because of lower productivity, work ethic, and infrastructure quality compared to China. Infrastructure deficits, including electricity generation and transportation networks, further hinder manufacturing revival. The lengthy process to build and operationalize new factories (minimum two years) and the uncertainty caused by fluctuating tariffs discourage investment. Complex and inconsistent tariff enforcement, along with a litigious business environment, exacerbate risks. The policy risks causing a recession, as supply chain disruptions and increased costs lead to inflation or deflation. The article predicts that unless policies change, globalization will bypass the U.S., with manufacturing shifting to countries like Vietnam and Mexico. To genuinely rebuild manufacturing, the U.S. must address fundamental social, infrastructural, and educational issues, implement gradual tariff increases, and incentivize high-end production, rather than relying solely on tariffs.\nDecreased CO2 saturation during circular breathwork supports emergence of altered states of consciousness | Communications Psychology Key Facts Circular breathwork induces significant reductions in end-tidal CO2 pressure (etCO2), with active participants reaching levels as low as 10–20 mmHg, compared to 36.7 ± 1.5 mmHg in non-hyperventilating controls Decreases in etCO2 are significantly correlated with the onset and depth of altered states of consciousness (ASCs), resembling psychedelic experiences across domains such as ego dissolution and unity Both Holotropic and Conscious-Connected breathwork produce similar physiological and experiential outcomes, with session durations of approximately 3 hours and 1.5 hours respectively, engaging the same mechanisms Summary This study demonstrates that circular breathwork, involving deliberate hyperventilation, can reliably trigger altered states of consciousness (ASCs) akin to those produced by psychedelics, with experience scales (MEQ30 and 11D-ASC) reaching levels comparable to moderate doses of psilocybin, LSD, and MDMA. Physiologically, active participants exhibited a marked reduction in end-tidal CO2 (etCO2), dropping to as low as 10–20 mmHg, which was strongly associated with ASC onset (r = -0.46 to -0.47, p \u003c 0.01). These reductions in CO2 pressure, supported by prior hyperventilation research, appear to serve as a physiological trigger for ASC emergence, with lower etCO2 levels (\u003c35 mmHg) virtually guaranteeing profound experiences when falling below 20 mmHg. The experiential depth correlated with physiological changes and persisted even as etCO2 levels normalized, suggesting a transition into a neuronal state of heightened perception, consistent with the concept of Pivotal Mental States. Both breathwork styles—Holotropic and Conscious-Connected—elicited similar physiological and subjective effects despite differences in session length, indicating shared underlying mechanisms. Post-session assessments revealed sustained improvements in well-being and reductions in depressive symptoms, with deeper ASC experiences predicting greater long-term benefits. Physiologically, breathwork caused a decrease in salivary alpha-amylase (sympathetic activity marker) and an increase in IL-1β (inflammatory marker), with subjective ASC depth inversely related to inflammation. These findings position breathwork as a non-pharmacological tool capable of inducing profound, psychedelic-like states through physiological modulation, with potential therapeutic applications supported by its safety profile and accessibility.\nSynology Lost the Plot with Hard Drive Locking Move - ServeTheHome Key Facts Synology plans to restrict its 2025 Plus NAS models to use only its own branded hard drives This move disables features like volume-wide deduplication, lifespan analysis, and automatic firmware updates for third-party drives The new policy limits maximum raw capacity to 128TB in an 8-bay NAS, compared to 208TB with competing brands like QNAP and TrueNAS using third-party drives Summary Synology is moving towards vendor-locking its 2025 Plus series NAS devices to only support Synology-branded hard drives, a shift that restricts compatibility with third-party drives and disables features such as volume-wide deduplication, lifespan analysis, and automatic firmware updates for non-branded drives. This strategy appears to be driven by a desire to increase margins, as Synology’s current drives max out at 16TB (e.g., HAT3310-16T), whereas competitors like WD Red Pro and Toshiba N300 Pro offer drives up to 26TB. Locking drives limits data security options, as users cannot quickly replace failed drives with models from other vendors, which can delay rebuild times and compromise data safety. Additionally, long-term availability of Synology drives is uncertain, raising concerns about future replacements and ecosystem dependence. The move is viewed as a step back in hardware flexibility and could harm Synology’s reputation, especially given the aging hardware and limited hardware refresh cycles. Critics argue that this lock-in reduces consumer choice and may lead to higher costs, as Synology’s drives are priced higher and have slower delivery times compared to third-party options. The industry context shows that vendor lock-in strategies often backfire, and many users prefer building custom solutions or switching to open-source NAS platforms like TrueNAS.\nApple, Meta Fined by EU, Ordered to Comply With Tech Competition Rules - WSJ Key Facts The EU fined Apple €500 million (approximately $570 million) for breaching the Digital Markets Act and Meta €200 million. The fines target violations related to Apple’s App Store rules and Meta’s personalized ad practices. The European Commission issued cease-and-desist orders requiring changes to Apple’s app store transparency and Meta’s ad personalization, with a 60-day compliance window before additional penalties up to 5% of daily revenue. Summary The European Union imposed €500 million ($570 million) fine on Apple and €200 million on Meta for violations of the Digital Markets Act, focusing on App Store transparency and personalized advertising practices. The EU ordered Apple to allow app developers to inform users of alternative purchase options and directed Meta to cease requiring user consent for personalized ads on Facebook and Instagram. Both companies plan to appeal. The enforcement aims to promote competition, with potential fines up to 10% of global revenue, and comes amid heightened US-EU trade tensions over digital regulations.\n▶️ Technology Try generating video in Gemini, powered by Veo 2 Key Facts Google has introduced video generation features in Gemini, powered by Veo 2, available to Google One AI Premium subscribers. Users can transform text prompts into high-resolution, eight-second videos at 720p in a 16:9 aspect ratio, with a monthly creation limit. Additionally, Whisk Animate enables turning images into eight-second videos using Veo 2, accessible in over 60 countries. Summary Google has launched new video generation capabilities within the Gemini AI platform, utilizing the advanced Veo 2 model to produce high-resolution, cinematic-quality videos from text prompts. Users can generate eight-second videos at 720p resolution in a 16:9 landscape format by selecting Veo 2 from the model dropdown in Gemini. The process involves describing a scene or concept, with more detailed prompts offering greater control over the output. Examples include scenes of a glacial cavern, animated mice reading, coastal landscapes, and voxel-style melting ice cream. The feature is rolling out globally to Gemini Advanced users and Google One AI Premium subscribers, with a monthly limit on video creation. Additionally, Whisk Animate allows users to animate images into short videos, expanding creative possibilities. All generated videos are marked with SynthID watermarks for safety and transparency. The platform emphasizes safety through extensive red teaming and content moderation policies. Users can share videos directly to platforms like TikTok and YouTube Shorts, and the service continues to evolve with UI adjustments and ongoing improvements. More information is available at gemini.google.\nGemma 3 QAT Models: Bringing state-of-the-Art AI to consumer GPUs - Google Developers Blog Key Facts Google announced quantization-aware training (QAT) optimized versions of Gemma 3 models on April 18, 2025 QAT reduces VRAM requirements by up to 4x, enabling deployment of large models like Gemma 3 27B on consumer GPUs such as NVIDIA RTX 3090 Quantized models (int4, Q4_0) maintain high accuracy, with perplexity drop reduced by 54% using QAT during training Summary Google introduced optimized Gemma 3 models with Quantization-Aware Training (QAT) to enhance accessibility of large language models on consumer-grade GPUs. The original Gemma 3 models deliver state-of-the-art performance, capable of running on high-end GPUs like NVIDIA H100 with BF16 precision. QAT enables these models to be quantized to lower-precision formats such as int4 and Q4_0, significantly reducing VRAM usage—e.g., Gemma 3 27B’s weights decrease from 54 GB (BF16) to 14.1 GB (int4). This reduction allows deployment on GPUs with limited memory, such as NVIDIA RTX 3090 (24GB VRAM) for the 27B model, and NVIDIA RTX 4060 Laptop GPU (8GB VRAM) for the 12B variant. The models are trained with QAT over approximately 5,000 steps, using non-quantized checkpoints as targets, which reduces perplexity degradation by 54%. The quantized models are compatible with inference engines like Ollama, llama.cpp, MLX, Gemma.cpp, and support integration via Hugging Face and Kaggle. Additional community-driven quantization options are available through the Gemmaverse, often utilizing Post-Training Quantization (PTQ). These advancements democratize access to powerful AI models, enabling local deployment on desktops, laptops, and mobile devices.\n","wordCount":"11143","inLanguage":"en","datePublished":"2025-04-23T15:34:31.809738Z","dateModified":"2025-04-23T15:34:31.809738Z","author":{"@type":"Person","name":"Alvaro Lopez Ortega"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://alobbs.com/posts/2025-04-23/"},"publisher":{"@type":"Organization","name":"Alvaro's Site","logo":{"@type":"ImageObject","url":"https://alobbs.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://alobbs.com/ accesskey=h title="Alvaro's Site (Alt + H)">Alvaro's Site</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://alobbs.com/archives title=Archive><span>Archive</span></a></li><li><a href=https://alobbs.com/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://alobbs.com/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://alobbs.com/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://alobbs.com/>Home</a>&nbsp;»&nbsp;<a href=https://alobbs.com/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">2025-04-23 Briefing</h1><div class=post-meta><span title='2025-04-23 15:34:31.809738 +0000 UTC'>April 23, 2025</span>&nbsp;·&nbsp;53 min&nbsp;·&nbsp;Alvaro Lopez Ortega</div></header><div class=post-content><h2 id=-internet-infrastructure>▶️ Internet Infrastructure<a hidden class=anchor aria-hidden=true href=#-internet-infrastructure>#</a></h2><h3 id=arch-linux---news-valkey-to-replace-redis-in-the-><a href=https://archlinux.org/news/valkey-to-replace-redis-in-the-extra-repository/>Arch Linux - News: Valkey to replace Redis in the [extra] Repository</a><a hidden class=anchor aria-hidden=true href=#arch-linux---news-valkey-to-replace-redis-in-the->#</a></h3><h3 id=key-facts>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts>#</a></h3><ul><li>Valkey, a high-performance key/value datastore, will replace Redis in the <a href=https://archlinux.org/packages/extra/>extra</a> repository</li><li>The change follows Redis&rsquo;s license modification from BSD-3-Clause to RSALv2 and SSPLv1 on March 20, 2024</li><li>Arch Linux package maintainers will support the redis package for approximately 14 days from April 17, 2025, after which it will be moved to the AUR and no longer receive updates</li></ul><h3 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h3><p>Arch Linux announced that <a href=https://github.com/valkey>Valkey</a>, a high-performance key/value datastore, will replace Redis in the <a href=https://archlinux.org/packages/extra/>extra</a> repository due to Redis changing its license to RSALv2 and SSPLv1 on March 20, 2024. The redis package will be supported for about 14 days from April 17, 2025, allowing users to transition smoothly to Valkey; after this period, the redis package will be moved to the AUR and considered deprecated with no further updates. Users are advised to begin migrating their Redis usage promptly to avoid issues post-transition.</p><hr><h3 id=man-who-built-isp-instead-of-paying-comcast-50k-expands-to-hundreds-of-homes---ars-technica><a href=https://arstechnica.com/tech-policy/2022/08/man-who-built-isp-instead-of-paying-comcast-50k-expands-to-hundreds-of-homes/>Man who built ISP instead of paying Comcast $50K expands to hundreds of homes - Ars Technica</a><a hidden class=anchor aria-hidden=true href=#man-who-built-isp-instead-of-paying-comcast-50k-expands-to-hundreds-of-homes---ars-technica>#</a></h3><h3 id=key-facts-1>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-1>#</a></h3><ul><li>Jared Mauch received $2.6 million in government funding to expand his fiber-to-the-home ISP in rural Michigan.</li><li>The project will extend his network by 38 miles, serving approximately 596 addresses, with a total funding of $2,618,958.03.</li><li>Mauch&rsquo;s service offers 100 Mbps symmetrical internet for $55/month and 1 Gbps for $79/month, with installation fees around $199; he participates in the FCC&rsquo;s Affordable Connectivity Program.</li></ul><h3 id=summary-1>Summary<a hidden class=anchor aria-hidden=true href=#summary-1>#</a></h3><p>Jared Mauch, who built a fiber-to-the-home ISP after being dissatisfied with poor broadband from AT&amp;T and Comcast, is expanding his network in rural Michigan with $2.6 million from the US government’s American Rescue Plan funds. The project, contracted by Washtenaw County, aims to wire approximately 596 addresses across four townships, adding 38 miles of fiber to his existing 14-mile network. The county allocated $71 million for broadband infrastructure, with Mauch’s project being one of four selected through a competitive RFP process that prioritized wireline speeds of at least 100 Mbps symmetrical. Mauch’s service costs $55/month for 100 Mbps unlimited, and $79/month for 1 Gbps, with a typical installation fee of $199. The project must be completed by the end of 2026, but Mauch aims to finish half by the end of 2023. Previously, he faced high costs from major providers—Comcast demanded $50,000 for line extension, and AT&amp;T offered only 1.5 Mbps DSL. Mauch also provides free 250 Mbps service to a local church and fiber backhaul to cell towers. His network management has been stable, with traffic around 500 Mbps, scalable to 4 Gbps. The expansion is part of a broader effort by Washtenaw County to connect over 3,000 households, with a focus on underserved, lower-income areas.</p><hr><h3 id=atuin-desktop-runbooks-that-run><a href=https://blog.atuin.sh/atuin-desktop-runbooks-that-run/>Atuin Desktop: Runbooks that Run</a><a hidden class=anchor aria-hidden=true href=#atuin-desktop-runbooks-that-run>#</a></h3><h3 id=key-facts-2>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-2>#</a></h3><ul><li>Atuin Desktop is a local-first, executable runbook editor designed to streamline terminal workflows, integrating script blocks, embedded terminals, database clients, and Prometheus charts.</li><li>Features include chaining shell commands, executing and maintaining relevant documentation, reusable automation with Jinja-style templating, autocomplete from shell history, and synchronization via Atuin Hub.</li><li>Currently used for releasing Atuin CLI, migrating infrastructure, deploying environments, and managing live database queries; upcoming features include team accounts and auto-generated runbooks from shell history.</li></ul><h3 id=summary-2>Summary<a hidden class=anchor aria-hidden=true href=#summary-2>#</a></h3><p>Atuin Desktop, launched on April 22, 2025, is a local-first runbook editor that combines terminal-like execution with document-like organization, enabling repeatable, shareable workflows. It supports chaining commands, executing embedded scripts, and dynamic templating, with real-time autocomplete from shell history. The platform syncs across devices via Atuin Hub and is used for deploying CLI tools, infrastructure migration, environment setup, and database management. Future updates include team collaboration and automated runbook generation from shell history.</p><hr><h3 id=botnet-part-2-the-web-is-broken---jan-wildeboers-blog><a href=https://jan.wildeboer.net/2025/04/Web-is-Broken-Botnet-Part-2/>Botnet Part 2: The Web is Broken - Jan Wildeboer’s Blog</a><a hidden class=anchor aria-hidden=true href=#botnet-part-2-the-web-is-broken---jan-wildeboers-blog>#</a></h3><h3 id=key-facts-3>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-3>#</a></h3><ul><li>Companies like Infatica monetize network bandwidth via SDKs embedded in iOS, Android, MacOS, and Windows apps, enabling web crawling and scraping using infected devices.</li><li>These SDKs sell access to millions of residential, static, and mobile IP addresses, facilitating large-scale AI web scraping and brute-force attacks.</li><li>Trend Micro&rsquo;s 2023 research confirms malicious repacking of freeware/shareware to conduct drive-by downloads of these proxy services, contributing to increased bot traffic and server overloads.</li></ul><h3 id=summary-3>Summary<a hidden class=anchor aria-hidden=true href=#summary-3>#</a></h3><p>Jan Wildeboer highlights the proliferation of shady business models where app developers embed SDKs, such as Infatica’s, into their applications to monetize users’ network bandwidth. These SDKs enable the creation of botnets that leverage infected devices to perform web crawling, brute-force mail server attacks, and AI-driven web scraping, often using millions of residential, static, and mobile IP addresses. Trend Micro’s 2023 investigation confirms malicious repacking of freeware/shareware to facilitate drive-by downloads of these proxy services, exacerbating botnet activity. Wildeboer argues that this business model effectively causes DDoS-like traffic surges, impacting small web services and increasing the difficulty for server administrators to detect and block such malicious activity. He advocates that all web scraping should be considered abusive and recommends blocking such traffic, emphasizing that inclusion of SDKs for profit in apps makes developers complicit in malware distribution and botnet formation. The market for residential proxies, driven by AI web scraping demands, is expanding rapidly, with many providers relying on SDK injection into third-party apps. Wildeboer concludes that this trend undermines the web’s foundational integrity, urging webmasters and admins to remain vigilant against these evolving threats.</p><hr><h3 id=whistleblower-doge-siphoned-nlrb-case-data--krebs-on-security><a href=https://krebsonsecurity.com/2025/04/whistleblower-doge-siphoned-nlrb-case-data/>Whistleblower: DOGE Siphoned NLRB Case Data – Krebs on Security</a><a hidden class=anchor aria-hidden=true href=#whistleblower-doge-siphoned-nlrb-case-data--krebs-on-security>#</a></h3><h3 id=key-facts-4>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-4>#</a></h3><ul><li>A whistleblower from the National Labor Relations Board (NLRB) alleges that DOGE employees transferred gigabytes of sensitive case data in early March using short-lived accounts with restricted logging.</li><li>The data exfiltration involved approximately 10 GB from the NxGen case management system, with suspicious activity including creation of high-privilege accounts and use of containers to obfuscate activity.</li><li>The whistleblower reports blocked login attempts from a Russian IP address shortly after account creation, along with unauthorized download of code libraries used for web scraping and brute-force attacks. The NLRB investigated but was reportedly instructed to cease reporting to US-CERT, and the agency&rsquo;s control was later removed from all employee accounts.</li></ul><h3 id=summary-4>Summary<a hidden class=anchor aria-hidden=true href=#summary-4>#</a></h3><p>A security architect at the NLRB, Daniel J. Berulis, alleges that employees from Elon Musk’s Department of Government Efficiency (DOGE) siphoned sensitive case data in early March by creating high-privilege accounts with logging restrictions and using containerized environments to conceal activities. The incident involved transferring approximately 10 GB of data from the NxGen system, which contains confidential information on unions, legal cases, and corporate secrets. Suspicious activity included multiple blocked login attempts from a Russian IP address (83.149.30.186) shortly after account creation, with attempts to use valid credentials, and the download of code libraries designed for web scraping and brute-force attacks from GitHub. Berulis observed that network logs for recent resources went missing, and Microsoft Azure monitoring was turned off during the incident. Despite raising alarms and reporting to US-CERT, the NLRB was allegedly ordered to halt further investigation, and control over its systems was later revoked from staff. The whistleblower’s disclosures, supported by internal documentation and expert review, highlight potential security breaches involving high-level access and covert data exfiltration, amid broader political tensions and legal disputes involving Musk’s companies and government agencies.</p><hr><h3 id=an-intro-to-deepseek><a href=https://maknee.github.io/blog/2025/3FS-Performance-Journal-1/>An Intro to DeepSeek&rsquo;s Distributed File System | Some blog</a><a hidden class=anchor aria-hidden=true href=#an-intro-to-deepseek>#</a></h3><h3 id=key-facts-5>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-5>#</a></h3><ul><li>3FS (Fire-Flyer File System) is an open-source distributed filesystem released by DeepSeek on April 15, 2025</li><li>Core components include Meta, Mgmtd, Storage, and Client nodes, with Mgmtd managing node registration and cluster configuration</li><li>Utilizes CRAQ (Chain Replication with Apportioned Queries) protocol for strong consistency, with write throughput limited by the slowest node in the chain and performance affected by workload types</li></ul><h3 id=summary-5>Summary<a hidden class=anchor aria-hidden=true href=#summary-5>#</a></h3><p>DeepSeek introduced <a href=https://github.com/deepseek-ai/3FS>3FS</a>, a distributed filesystem designed to abstract data across multiple machines, enabling applications to interact with it as if it were a local filesystem. It consists of four primary node types: Meta (manages file metadata stored in inodes and DirEntries within FoundationDB), Mgmtd (controls cluster configuration and node health via heartbeats and node discovery), Storage (handles physical data chunks using a Rust-based ChunkEngine, with metadata stored in LevelDB), and Client (interfaces with other nodes for file operations and data transfer). The system employs CRAQ for fault-tolerant, strongly consistent data replication, where write operations propagate from head to tail nodes, marking entries as &ldquo;dirty&rdquo; until committed, with read operations querying the tail for the most recent clean data. CRAQ&rsquo;s performance varies with workload, offering scalable, low-latency reads but higher write latency, especially under zipfian access patterns. The architecture emphasizes fault tolerance, scalability, and simplicity, with detailed design notes available <a href=https://github.com/deepseek-ai/3FS/blob/ee9a5cee0a85c64f4797bf380257350ca1becd36/docs/design_notes.md>here</a>. Future analyses aim to benchmark 3FS performance, evaluate bottlenecks, and compare it with other distributed filesystems.</p><hr><h3 id=update-on-spain-and-laliga-blocks-of-the-internet---vercel><a href=https://vercel.com/blog/update-on-spain-and-laliga-blocks-of-the-internet>Update on Spain and LALIGA blocks of the internet - Vercel</a><a hidden class=anchor aria-hidden=true href=#update-on-spain-and-laliga-blocks-of-the-internet---vercel>#</a></h3><h3 id=key-facts-6>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-6>#</a></h3><ul><li>Spanish court authorized LALIGA to block IP addresses associated with unauthorized football streaming, affecting Vercel infrastructure since December 2024.</li><li>IP addresses <code>66.33.60.129</code> and <code>76.76.21.142</code> are no longer blocked as of April 18, 2025, following Vercel&rsquo;s cooperation to remove illegal content.</li><li>Broad IP-wide blocks are enforced during LALIGA matchdays, impacting legitimate websites and services that share IP addresses, with no distinction between infringing and lawful content.</li></ul><h3 id=summary-6>Summary<a hidden class=anchor aria-hidden=true href=#summary-6>#</a></h3><p>A Spanish court granted LALIGA authority in December 2024 to require ISPs, including Movistar, Vodafone, and Orange, to block IP addresses linked to unauthorized football streaming, a ruling upheld in March 2025. Enforcement has expanded to affect Vercel-hosted sites, resulting in indiscriminate IP blocking that impacts legitimate services such as Tinybird and Hello Magazine, which operate on shared IPs like <code>66.33.60.129</code> and <code>76.76.21.142</code>. Unlike targeted domain blocking via SNI inspection, ISPs are blocking entire IP ranges without differentiation, causing collateral damage to infrastructure, developers, and businesses during LALIGA matchdays. Vercel actively monitors and removes illegal content, maintaining a zero-tolerance policy, and is working with LALIGA to mitigate the impact. The company advocates for targeted, transparent enforcement and is exploring strategies to restore access for affected users in Spain, emphasizing the importance of an open, permissionless web.</p><h2 id=-open-source>▶️ Open Source<a hidden class=anchor aria-hidden=true href=#-open-source>#</a></h2><h3 id=15000-lines-of-verified-cryptography-now-in-python--jonathan-protzenko><a href=http://jonathan.protzenko.fr/2025/04/18/python.html>15,000 lines of verified cryptography now in Python | Jonathan Protzenko</a><a hidden class=anchor aria-hidden=true href=#15000-lines-of-verified-cryptography-now-in-python--jonathan-protzenko>#</a></h3><h3 id=key-facts-7>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-7>#</a></h3><ul><li>Python&rsquo;s hash and HMAC algorithms are now fully implemented using <a href=https://github.com/project-everest/hacl-star/>HACL*</a>, a verified cryptographic library, replacing previous implementations.</li><li>The transition, completed after 2.5 years of work, includes 15,000 lines of verified C code integrated into Python without loss of functionality.</li><li>Upstream updates from HACL* are automated via a script, ensuring maintainability and consistency.</li></ul><h3 id=summary-7>Summary<a hidden class=anchor aria-hidden=true href=#summary-7>#</a></h3><p>Python has integrated <a href=https://github.com/project-everest/hacl-star/>HACL*</a>, a verified cryptographic library, to implement all default hash and HMAC algorithms, following a GitHub issue opened in November 2022 addressing cryptographic verification after a SHA3 CVE. This integration adds approximately 15,000 lines of verified C code, enabling features such as additional Blake2 modes, a comprehensive SHA3 API covering all Keccak variants, strict abstraction patterns for build system compatibility, proper error handling including allocation failures, and optimized HMAC implementations maintaining two hash states simultaneously. The process involved extensive low-level technical work, including generic streaming API verification using dependent types, handling complex buffer management, and refactoring C code to abstract structs for compatibility with older compilers. The build system&rsquo;s CI coverage uncovered corner cases, notably with AVX2-specific code, requiring careful refactoring to ensure cross-platform compatibility. Memory allocation failures are now propagated through the verification framework via <code>option</code> types, enhancing robustness. Upstream HACL* updates are managed through a shell script that fetches, refines, and integrates code changes, simplifying maintenance. This milestone demonstrates verified cryptography&rsquo;s maturity and readiness for real-world deployment in critical software like Python.</p><hr><h3 id=a-new-form-of-verification-on-bluesky---bluesky><a href=https://bsky.social/about/blog/04-21-2025-verification>A New Form of Verification on Bluesky - Bluesky</a><a hidden class=anchor aria-hidden=true href=#a-new-form-of-verification-on-bluesky---bluesky>#</a></h3><h3 id=key-facts-8>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-8>#</a></h3><ul><li>Bluesky introduces a new, user-friendly blue check for verified accounts, launched on April 21, 2025</li><li>Over 270,000 accounts linked their domain as their username since the 2023 launch of domain handle verification</li><li>Verified accounts now display a blue check, with additional verification via trusted verifiers—organizations that can directly issue blue checks, marked by scalloped blue checks</li></ul><h3 id=summary-8>Summary<a hidden class=anchor aria-hidden=true href=#summary-8>#</a></h3><p>Bluesky announced a new verification layer on April 21, 2025, featuring a recognizable blue check to indicate authentic and notable accounts. Building on the initial domain handle verification launched in 2023, which linked over 270,000 accounts to their websites, the platform now offers a visual trust signal through a blue check. This check is issued either automatically for verified accounts or through trusted verifiers—organizations like The New York Times that can directly verify accounts within the app, with moderation reviews ensuring authenticity. When users tap on a verified account&rsquo;s blue check, they can see which organization granted the verification. Users can also hide verification signals via Settings. Self-verification remains encouraged by setting a domain as the username; however, Bluesky is not accepting direct verification applications during this phase. Future plans include a request form for notable accounts and trusted verifiers once the feature stabilizes. The initiative aims to enhance trust and authenticity in decentralized social conversations, aligning with Bluesky’s broader goal of transitioning the social web from platform-centric to protocol-based systems.</p><hr><h3 id=i-left-spotify-what-happened-next><a href=https://coppolaemilio.com/entries/i-left-spotify-what-happened-next/>I left Spotify. What happened next?</a><a hidden class=anchor aria-hidden=true href=#i-left-spotify-what-happened-next>#</a></h3><h3 id=key-facts-9>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-9>#</a></h3><ul><li>The author transitioned from Spotify to self-hosted Jellyfin for music management</li><li>Built a web-based music player using htmx to stream music locally and remotely</li><li>Uses apps like Finamp (<a href=https://github.com/jmshrv/finamp>link</a>) to download music for offline listening</li><li>Purchased a mini PC to self-host Jellyfin and other apps like Immich for photo management</li><li>Emphasizes ease of setup without advanced technical skills, using existing hardware like an old computer</li><li>Highlights potential for future digital autonomy by self-hosting media services</li></ul><h3 id=summary-9>Summary<a hidden class=anchor aria-hidden=true href=#summary-9>#</a></h3><p>After leaving Spotify, the author explored various local music players such as Winamp, VLC, and foobar2000, but found them inadequate for browsing and managing large libraries. They developed a web-based music streaming solution using <a href=https://htmx.org>htmx</a>, enabling remote access to their library via a local server, though this approach lacked offline functionality. Switching to Apple&rsquo;s Music app provided reliable offline access but required managing storage across devices, which was cumbersome. Inspired by a <a href="https://www.youtube.com/watch?v=7CDKvdlD6uQ">YouTube video</a>, they adopted <a href=https://jellyfin.org/>Jellyfin</a>, an open-source media server that can replace Spotify, Netflix, and other streaming services. Self-hosting Jellyfin on an old computer or mini PC allows full control over media libraries and offline access through apps like <a href=https://github.com/jmshrv/finamp>Finamp</a> and <a href=https://www.fintunes.app/>Fintunes</a>. The author now runs Jellyfin and <a href=https://immich.app/>Immich</a> for photo management, emphasizing that self-hosting is accessible without advanced technical skills and promotes digital autonomy. They advocate for open-source solutions to reduce dependence on commercial platforms and envision a future where users fully control their media content.</p><hr><h3 id=using-><a href=https://dan.langille.org/2025/04/17/using-ssh-authorized-keys-to-decide-what-the-incoming-connection-can-do/>Using ~/.ssh/authorized keys to decide what the incoming connection can do – Dan Langille&rsquo;s Other Diary</a><a hidden class=anchor aria-hidden=true href=#using->#</a></h3><h3 id=key-facts-10>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-10>#</a></h3><ul><li>Demonstrates using ~/.ssh/authorized_keys to assign specific commands for incoming SSH connections on FreeBSD 14.2</li><li>Configures SSH keys with command restrictions, such as running rrsync in read-only mode for backups</li><li>Shows how multiple SSH keys can be used for different tasks by specifying distinct commands in authorized_keys</li></ul><h3 id=summary-10>Summary<a hidden class=anchor aria-hidden=true href=#summary-10>#</a></h3><p>The article explains how to leverage the <code>~/.ssh/authorized_keys</code> file to control the actions of incoming SSH connections by associating specific commands with SSH keys on FreeBSD 14.2. It illustrates configuring a key to run <code>/usr/local/sbin/rrsync -ro /path/</code> to restrict an SSH session to a read-only rsync operation, enhancing security for database backup transfers. The author demonstrates managing multiple tasks by adding separate SSH keys with different command directives, such as initiating a script to pull database backups from another host. The setup involves specifying the <code>from</code> attribute to restrict access to particular hosts and embedding the command directly within the authorized_keys entry. The approach ensures that each SSH key is tied to a precise, limited operation, preventing unauthorized actions. The article emphasizes the importance of using distinct SSH keys for different tasks, simplifying access management and maintaining security boundaries for critical systems like database backups.</p><hr><h3 id=python><a href=https://davepeck.org/2025/04/11/pythons-new-t-strings/>Python&rsquo;s new t-strings | Dave Peck</a><a hidden class=anchor aria-hidden=true href=#python>#</a></h3><h3 id=key-facts-11>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-11>#</a></h3><ul><li>Python 3.14, featuring t-strings, will be released in late 2025</li><li>T-strings are a generalization of f-strings, evaluating to <code>string.templatelib.Template</code> objects</li><li>T-strings improve safety by requiring explicit processing before use, preventing injection vulnerabilities</li></ul><h3 id=summary-11>Summary<a hidden class=anchor aria-hidden=true href=#summary-11>#</a></h3><p>Python&rsquo;s <a href=https://peps.python.org/pep-0750/>PEP 750</a> introduces t-strings (template strings) as a new feature in Python 3.14, arriving in late 2025. T-strings extend the capabilities of f-strings by evaluating to <code>string.templatelib.Template</code> objects rather than immediate strings, enabling safer and more flexible string processing. Unlike f-strings, which can be dangerously misused with user input (e.g., SQL injection or XSS), t-strings require explicit processing before conversion to a string, allowing developers to implement safe escaping functions such as <code>html()</code>. T-strings provide access to their components via <code>.strings</code> and <code>.values</code> properties, supporting complex manipulations and custom processing, including iteration and detailed interpolation analysis. They can be instantiated with literal syntax (<code>t"..."</code>) or directly through the <code>Template</code> constructor with <code>Interpolation</code> objects. An example demonstrates converting a template into pig Latin by processing each interpolation. The feature aims to enhance string safety and flexibility in Python libraries and frameworks, especially those handling user input, and is expected to influence tooling support like code formatters and IDEs.</p><hr><h3 id=defold---official-homepage---cross-platform-game-engine><a href=https://defold.com/>Defold - Official Homepage - Cross platform game engine</a><a hidden class=anchor aria-hidden=true href=#defold---official-homepage---cross-platform-game-engine>#</a></h3><h3 id=key-facts-12>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-12>#</a></h3><ul><li>Defold is a free, production-ready, cross-platform game engine supporting major platforms including PlayStation®5, PlayStation®4, Nintendo Switch, Android, iOS, macOS, Linux, Windows, Steam, HTML5, Facebook, and Q3 2024 XBox.</li><li>Comes fully featured out of the box with visual editor, code editor, Lua scripting, Lua debugger, scene, particle, tilemap editors, supporting both 2D and 3D development.</li><li>No setup required; offers zero-config cloud build, native code extension, and integration with tools like Atom, VS Code, Spine, TexturePacker, and Tiled.</li></ul><h3 id=summary-12>Summary<a hidden class=anchor aria-hidden=true href=#summary-12>#</a></h3><p>Defold is a free, open-source, cross-platform game engine designed for high-performance game development, supporting platforms such as PlayStation®5, PlayStation®4, Nintendo Switch, Android, iOS, macOS, Linux, Windows, Steam, HTML5, Facebook, and expected Q3 2024 XBox release. It provides a comprehensive, ready-to-use environment with features including a visual editor, code editor, Lua scripting, Lua debugger, scene, particle, and tilemap editors, supporting both 2D and 3D game creation. The engine requires no initial setup, offering a zero-configuration cloud build system and native code extension capabilities. It integrates with popular development tools like Atom, VS Code, Spine, TexturePacker, and Tiled, enabling customization and extension. Defold is supported by a broad community, with active development releasing updates approximately monthly, and includes support contracts for enterprise use. Notable projects include the game Family Island, which has over 50 million downloads on Google Play as of September 2023. The engine emphasizes accessibility, with no licensing fees, royalties, or runtime costs, and is governed by the Defold Foundation (<a href=https://defold.com/>source</a>).</p><hr><h3 id=github---the-pockettutorial-codebase-knowledge-turns-codebase-into-easy-tutorial-with-ai><a href=https://github.com/The-Pocket/Tutorial-Codebase-Knowledge>GitHub - The-Pocket/Tutorial-Codebase-Knowledge: Turns Codebase into Easy Tutorial with AI</a><a hidden class=anchor aria-hidden=true href=#github---the-pockettutorial-codebase-knowledge-turns-codebase-into-easy-tutorial-with-ai>#</a></h3><h3 id=key-facts-13>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-13>#</a></h3><ul><li>The project automates turning codebases into beginner-friendly tutorials using AI analysis.</li><li>It crawls GitHub repositories or local directories, analyzing core abstractions and interactions.</li><li>Built on Pocket Flow, a 100-line LLM framework, it generates tutorials in multiple languages, with commands like <code>python main.py --repo URL --include "*.py" "*.js"</code>.</li></ul><h3 id=summary-13>Summary<a hidden class=anchor aria-hidden=true href=#summary-13>#</a></h3><p>The system enables AI-driven transformation of complex codebases into easy-to-understand tutorials by crawling GitHub repositories or local directories. It leverages Pocket Flow, a minimalistic 100-line LLM framework, to analyze code structure, identify core abstractions, and visualize interactions. Users can specify repositories, file inclusion/exclusion patterns, maximum file size, and output language via command-line arguments such as <code>--repo</code>, <code>--include</code>, <code>--exclude</code>, and <code>--language</code>. The tool then generates comprehensive tutorials that explain how the code works, suitable for beginners. It supports setup with API keys for models like Gemini Pro 2.5 and can be customized for different models and languages. The project has gained notable attention, including front-page Hacker News coverage in April 2025, and provides example tutorials for repositories like AutoGen Core, Browser Use, and Celery. It emphasizes rapid development through agentic coding paradigms and visualizes code interactions, making complex repositories accessible. The system is open-source under the MIT license, with detailed setup instructions and resource links for further learning.</p><hr><h3 id=github---alexyknsapphire-rust-based-package-manager-for-macos><a href=https://github.com/alexykn/sapphire>GitHub - alexykn/sapphire: Rust based package manager for macOS</a><a hidden class=anchor aria-hidden=true href=#github---alexyknsapphire-rust-based-package-manager-for-macos>#</a></h3><h3 id=key-facts-14>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-14>#</a></h3><ul><li>Sapphire is an alpha-stage, Rust-based package manager for macOS, focusing on ARM architecture.</li><li>Manages formulae (command-line tools, libraries, languages) and casks (macOS applications).</li><li>Core library handles fetching, dependency resolution, archive extraction, artifact management; CLI wraps core functionalities.</li><li>Supports bottle and cask installation/uninstallation, parallel downloads, automatic dependency resolution, and early-stage source build capabilities.</li><li>Project structure includes <code>sapphire-core</code> and <code>sapphire-cli</code>; currently under active development with planned features like upgrade, cleanup, reinstallation, and prefix support.</li><li>Uses BSD-3-Clause license; inspired by Homebrew; heavily experimental and unstable.<br><a href=https://github.com/alexykn/sapphire>Repository link</a></li></ul><h3 id=summary-14>Summary<a hidden class=anchor aria-hidden=true href=#summary-14>#</a></h3><p>Sapphire is an experimental, Rust-powered package manager for macOS, supporting ARM architecture. It manages formulae and casks, with core functions for fetching, dependency resolution, and artifact handling, and a CLI interface. Features include bottle and cask installation/uninstallation, parallel downloads, and early support for building from source. Its architecture comprises <code>sapphire-core</code> and <code>sapphire-cli</code>. The project is under active development with future plans for upgrade, cleanup, reinstallation commands, and prefix support. It is licensed under BSD-3-Clause and inspired by Homebrew, but remains unstable and alpha software.</p><hr><h3 id=github---ericjenottevertop-e-ink-ibm-xt-clone-with-solar-power-ultra-low-power-consumption-and-ultra-long-battery-life><a href=https://github.com/ericjenott/Evertop>GitHub - ericjenott/Evertop: E-ink IBM XT clone with solar power, ultra low power consumption, and ultra long battery life.</a><a hidden class=anchor aria-hidden=true href=#github---ericjenottevertop-e-ink-ibm-xt-clone-with-solar-power-ultra-low-power-consumption-and-ultra-long-battery-life>#</a></h3><h3 id=key-facts-15>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-15>#</a></h3><ul><li>Evertop is a portable IBM XT clone powered by an 80186 microcontroller, with 1MB RAM, running DOS, Minix, Windows up to 3.0, and other 1980s OS.</li><li>Features include a 5.83-inch 648x480 e-ink display, built-in keyboard, external PS/2 ports, full graphics support (CGA, Hercules, MCGA, partial EGA/VGA), audio outputs, serial ports, USB flash drive, Ethernet, WiFi, LoRA radio, Bluetooth (planned), and multiple charging options.</li><li>Power management enables 200-500 hours of continuous use on a single charge in power-saving mode, with a 6V, 6W solar panel capable of producing up to 700mA in full sunlight, supporting indefinite off-grid operation.</li></ul><h3 id=summary-15>Summary<a hidden class=anchor aria-hidden=true href=#summary-15>#</a></h3><p>Evertop is an ultra low-power, solar-powered portable PC emulating an IBM XT with an 80186 processor, 1MB RAM, and a 648x480 e-ink display, capable of running DOS, Minix, Windows 3.0, and other 1980s operating systems. It integrates extensive peripherals, including a built-in keyboard, external PS/2 ports, full CGA, Hercules, MCGA graphics, partial EGA/VGA support, audio outputs (PC speaker, Adlib, Covox, Disney Sound Source), serial ports, USB flash drive, Ethernet, WiFi, and LoRA radio, with Bluetooth planned. Power options include a detachable 6V, 6W solar panel, internal buck/boost circuit accepting 2.5-20V DC, and micro USB, with simultaneous charging capability. Power management techniques, such as hibernate, automatic shutdown, and physical switches, enable 200-500 hours of active use per charge, with longer durations possible for dedicated applications like e-readers. The solar panel can generate up to 700mA under full sunlight, providing 10-50 hours of use per hour of sunlight, supporting indefinite off-grid use. Storage is handled via a 256GB SD card, supporting multiple emulated systems with up to 8GB total. The system is based on an Espressif ESP32 microcontroller, with a custom firmware derived from Fabrizio Di Vittorio&rsquo;s PCEmulator, housed in a 3D-printed matte PETG enclosure. A minimal version, &ldquo;Evertop Min,&rdquo; removes the built-in keyboard, serial ports, Ethernet, LoRA, voltmeter, and reduces battery capacity, maintaining core features for lightweight, off-grid computing.</p><hr><h3 id=github---nari-labsdia-a-tts-model-capable-of-generating-ultra-realistic-dialogue-in-one-pass><a href=https://github.com/nari-labs/dia>GitHub - nari-labs/dia: A TTS model capable of generating ultra-realistic dialogue in one pass.</a><a hidden class=anchor aria-hidden=true href=#github---nari-labsdia-a-tts-model-capable-of-generating-ultra-realistic-dialogue-in-one-pass>#</a></h3><h3 id=key-facts-16>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-16>#</a></h3><ul><li>Dia is a 1.6 billion parameter text-to-speech (TTS) model developed by Nari Labs, capable of generating ultra-realistic dialogue in a single pass.</li><li>Supports conditioning on audio for emotion and tone control, and can produce nonverbal sounds like laughter, coughing, and clearing throat.</li><li>Model weights are available on <a href=https://huggingface.co/nari-labs/Dia-1.6B>Hugging Face</a>, supporting only English at present; inference code and pretrained checkpoints are provided for research use.</li></ul><h3 id=summary-16>Summary<a hidden class=anchor aria-hidden=true href=#summary-16>#</a></h3><p>Dia is an open-weight, 1.6B parameter TTS model designed to generate highly realistic dialogue directly from transcripts, with the ability to condition output on audio inputs for emotion and tone modulation. It can also synthesize nonverbal cues such as laughter and coughing, enhancing dialogue authenticity. The model is hosted on <a href=https://huggingface.co/nari-labs/Dia-1.6B>Hugging Face</a> and supports English language generation exclusively. Researchers can access pretrained checkpoints and inference code to facilitate development, with the model capable of real-time audio synthesis on enterprise GPUs, requiring approximately 10GB VRAM. The model supports dialogue generation via <code>[S1]</code> and <code>[S2]</code> tags, voice cloning, and nonverbal sound production. It is intended for research and educational purposes, with restrictions against identity misuse, deceptive content, and illegal activities. The project is licensed under Apache-2.0, with ongoing plans for Docker support, inference speed optimization, and quantization for memory efficiency. Users can run the Gradio UI for testing or integrate the model as a Python library, with hardware acceleration via <code>torch.compile</code> expected to improve speed.</p><hr><h3 id=github---openaicodex-lightweight-coding-agent-that-runs-in-your-terminal><a href=https://github.com/openai/codex>GitHub - openai/codex: Lightweight coding agent that runs in your terminal</a><a hidden class=anchor aria-hidden=true href=#github---openaicodex-lightweight-coding-agent-that-runs-in-your-terminal>#</a></h3><h3 id=key-facts-17>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-17>#</a></h3><ul><li>OpenAI&rsquo;s Codex CLI is a lightweight coding agent designed to run in terminal environments.</li><li>It supports multiple providers, including OpenAI, OpenRouter, Gemini, Ollama, Mistral, DeepSeek, XAI, and Groq, with configurable API keys.</li><li>The project is licensed under Apache-2.0, with over 19,300 stars and active development on GitHub.</li></ul><h3 id=summary-17>Summary<a hidden class=anchor aria-hidden=true href=#summary-17>#</a></h3><p>OpenAI&rsquo;s <a href=https://github.com/openai/codex>Codex CLI</a> is an open-source, terminal-based coding agent enabling developers to generate, modify, and execute code through natural language prompts. It supports various models and providers, allowing flexible integration with different AI backends, such as OpenAI, Gemini, Ollama, and others, with configurable API keys via environment variables or config files. The CLI offers interactive and non-interactive modes, including full auto-approval for code changes, with safety features like sandboxing on macOS (via Apple Seatbelt) and Linux (using Docker). It requires Node.js 22+, operates across macOS 12+, Ubuntu 20.04+, and Windows 11 with WSL2, and manages dependencies with package managers like pnpm, migrated from npm for efficiency. The project includes comprehensive documentation, CLI commands, and configuration options, along with development workflows emphasizing high-quality contributions, testing, and code standards. Recent updates include support for pnpm workspace management, a Nix flake for reproducible environments, a new <code>/diff</code> command, and version check improvements supporting multiple package managers. The repository is actively maintained, with over 80 contributors, and emphasizes responsible AI use, security, and community engagement.</p><hr><h3 id=15000-lines-of-verified-cryptography-now-in-python--jonathan-protzenko-1><a href=https://jonathan.protzenko.fr/2025/04/18/python.html>15,000 lines of verified cryptography now in Python | Jonathan Protzenko</a><a hidden class=anchor aria-hidden=true href=#15000-lines-of-verified-cryptography-now-in-python--jonathan-protzenko-1>#</a></h3><h3 id=key-facts-18>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-18>#</a></h3><ul><li>Python&rsquo;s hash and HMAC algorithms are now fully implemented using <a href=https://github.com/project-everest/hacl-star/>HACL*</a>, a verified cryptographic library, replacing previous implementations.</li><li>The transition, completed after 2.5 years of development, includes 15,000 lines of verified C code integrated into Python without functionality loss.</li><li>Upstream updates from HACL* are automated via a script, ensuring maintainability and synchronization with the verified library.</li></ul><h3 id=summary-18>Summary<a hidden class=anchor aria-hidden=true href=#summary-18>#</a></h3><p>Python has integrated <a href=https://github.com/project-everest/hacl-star/>HACL*</a>, a verified cryptographic library, to implement all default hash and HMAC algorithms, replacing previous code after a GitHub issue was opened in November 2022. This move enhances security by ensuring cryptographic primitives are formally verified, covering algorithms such as SHA3, Blake2, and their variants, with support for additional modes and error management. The integration involved adapting HACL*&rsquo;s generic streaming API, which uses dependent types for block algorithms, to handle Python&rsquo;s diverse cryptographic needs, including pre-input handling, variable output lengths, and state retention. Technical challenges included refactoring C code generated from F* to use abstract structs for compatibility across compilers and architectures, and propagating memory allocation failures through the verification model. The process also required refining build systems to handle multiple toolchains, architectures, and compiler behaviors, especially for AVX2-specific code in implementations like Blake2b-256. Upstream updates from HACL* are managed via a shell script, simplifying maintenance. This large-scale adoption demonstrates verified cryptography&rsquo;s maturity for real-world applications, ensuring cryptographic correctness and robustness in Python&rsquo;s core infrastructure.</p><hr><h3 id=nlnet-42-free-and-open-source-projects-receive-funding-to-reclaim-the-public-nature-of-the-internet><a href=https://nlnet.nl/news/2025/20250422-announcement-grants-CommonsFund.html>NLnet; 42 Free and Open Source Projects Receive Funding to Reclaim the Public Nature of the Internet</a><a hidden class=anchor aria-hidden=true href=#nlnet-42-free-and-open-source-projects-receive-funding-to-reclaim-the-public-nature-of-the-internet>#</a></h3><h3 id=key-facts-19>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-19>#</a></h3><ul><li>42 projects funded by NLnet&rsquo;s October NGI Zero Commons Fund call, the largest in NGI Zero history</li><li>Projects aim to reclaim the public nature of the internet, focusing on open hardware, privacy, decentralization, and open source tools</li><li>Notable projects include open hardware (Solar FemtoTX motherboard, FuseSoC Web Catalog), privacy tools (LiberaForms, Alaveteli GDPR search), decentralized identity (DID SASL), and AI/data workflows (Livebook, LLM2FPGA)</li></ul><h3 id=summary-19>Summary<a hidden class=anchor aria-hidden=true href=#summary-19>#</a></h3><p>NLnet announced funding for 42 open source projects through the October NGI Zero Commons Fund, supporting initiatives that promote the public, privacy-respecting internet. Projects span open hardware (e.g., Solar FemtoTX low-power motherboard, FuseSoC Web Catalog), privacy-enhancing forms (LiberaForms), decentralized identity (DID SASL), and AI workflows (Livebook, LLM2FPGA). Additional efforts include improving web standards for print, developing federated XR content, and enhancing open communication protocols. This funding aims to strengthen digital commons, transparency, and user control over online infrastructure and data.</p><hr><h3 id=nine-reasons-to-use-osh><a href=https://oils.pub/osh.html>Nine Reasons to Use OSH</a><a hidden class=anchor aria-hidden=true href=#nine-reasons-to-use-osh>#</a></h3><h3 id=key-facts-20>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-20>#</a></h3><ul><li>OSH is a modern, compatible Unix shell implementation focused on scripting and interactive features, part of the <a href=https://oils.pub/>Oils</a> project</li><li>It supports running existing shell scripts, including POSIX and Bash, with nine years of increasing compatibility</li><li>Provides precise error messages, pretty printing for debugging, strict mode for error checking, and additional runtime features</li><li>Small size (~2MB), minimal dependencies, suitable for building GUIs and headless protocols</li><li>Upgradable to <a href=https://ysh.html>YSH</a>, a versatile language combining shell, Python, JSON, and YAML</li><li>Open for contributions, with high-level, statically typed code, and performance advantages over C-based shells</li></ul><h3 id=summary-20>Summary<a hidden class=anchor aria-hidden=true href=#summary-20>#</a></h3><p>OSH is a high-compatibility, script-focused Unix shell that runs existing scripts from POSIX and Bash, with nine years of incremental improvements. It offers precise error reporting, debugging tools like pretty printing, and a strict mode to enhance script reliability. The lightweight design (~2MB, minimal dependencies) facilitates system building and GUI development. Users can upgrade to <a href=https://ysh.html>YSH</a>, a flexible language integrating shell, Python, and data formats. The project encourages contributions, featuring high-level, statically typed code and superior performance benchmarks compared to C shells.</p><hr><h3 id=getting-forked-by-microsoft--philip-laine><a href=https://philiplaine.com/posts/getting-forked-by-microsoft/>Getting Forked by Microsoft • Philip Laine</a><a hidden class=anchor aria-hidden=true href=#getting-forked-by-microsoft--philip-laine>#</a></h3><h3 id=key-facts-21>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-21>#</a></h3><ul><li>Philip Laine&rsquo;s open source project Spegel, a P2P container image sharing tool, was forked and maintained by Microsoft under an MIT license.</li><li>Microsoft’s Peerd project contains code, test cases, and comments directly copied from Spegel without attribution, leading to confusion among users.</li><li>Laine’s Spegel has over 1,700 stars and 14.4 million pulls since its release over two years ago; he questions the implications of corporate forks on individual maintainers and open source sustainability.</li></ul><h3 id=summary-21>Summary<a hidden class=anchor aria-hidden=true href=#summary-21>#</a></h3><p>Philip Laine’s open source project Spegel, a peer-to-peer container image sharing solution designed to improve scalability and reduce downtime caused by registry outages, was acknowledged by Microsoft during a conference talk. Subsequently, Microsoft developed Peerd, a fork of Spegel, incorporating code, test cases, and comments directly copied from Laine’s project without attribution, despite Spegel being licensed under MIT. This has caused confusion among users and raised concerns about intellectual property and community trust. Laine, who maintained Spegel with community support and over 1,700 stars, felt marginalized as his work was effectively appropriated by a corporate entity. The incident highlights challenges faced by individual open source maintainers when collaborating with or being forked by large corporations, especially amid declining open source investment and licensing complexities. Laine has considered changing Spegel’s license and has enabled GitHub Sponsors to fund ongoing development.</p><hr><h3 id=ex-openai-staff-and-top-ai-experts-seek-to-block-proposed-for-profit-restructure><a href=https://t.co/xnU6Dx8DFk>Ex-OpenAI staff and top AI experts seek to block proposed for-profit restructure</a><a hidden class=anchor aria-hidden=true href=#ex-openai-staff-and-top-ai-experts-seek-to-block-proposed-for-profit-restructure>#</a></h3><h3 id=key-facts-22>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-22>#</a></h3><ul><li>Ex-OpenAI employees and leading AI experts, including Geoffrey Hinton, Margaret Mitchell, and Stuart Russell, oppose the company&rsquo;s proposed transition from a non-profit to a for-profit public benefit corporation (PBC).</li><li>The joint letter to California and Delaware attorneys-general warns that the restructuring would transfer control of artificial general intelligence (AGI) development to a profit-driven entity, contradicting OpenAI’s founding mission to benefit all humanity.</li><li>OpenAI aims to complete the for-profit conversion by the end of 2023 to secure additional investments, including a $30bn funding round from SoftBank, amid concerns that the change could undermine its mission and increase societal risks associated with AGI.</li></ul><h3 id=summary-22>Summary<a hidden class=anchor aria-hidden=true href=#summary-22>#</a></h3><p>Former OpenAI staff and top AI researchers, including Geoffrey Hinton and Stuart Russell, have petitioned US authorities to block the company&rsquo;s plan to convert from a non-profit to a for-profit public benefit corporation (PBC). They argue this shift would prioritize profit over the company&rsquo;s original mission to ensure AGI benefits all humanity, risking increased societal and safety concerns. OpenAI, valued at $300bn and seeking to secure further investments, contends that the restructuring will strengthen its non-profit and align with its mission, but critics warn it could lead to loss of control over AGI development and heightened risks of misuse.</p><hr><h3 id=a-new-open-source-text-to-speech-model-called-dia-has-arrived-to-challenge-elevenlabs-openai-and-more--venturebeat><a href=https://venturebeat.com/ai/a-new-open-source-text-to-speech-model-called-dia-has-arrived-to-challenge-elevenlabs-openai-and-more/>A new, open source text-to-speech model called Dia has arrived to challenge ElevenLabs, OpenAI and more | VentureBeat</a><a hidden class=anchor aria-hidden=true href=#a-new-open-source-text-to-speech-model-called-dia-has-arrived-to-challenge-elevenlabs-openai-and-more--venturebeat>#</a></h3><h3 id=key-facts-23>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-23>#</a></h3><ul><li>Nari Labs released Dia, a 1.6 billion parameter open-source TTS model, designed for naturalistic dialogue generation from text prompts</li><li>Supports nuanced features like emotional tone, speaker tagging, nonverbal cues, and audio conditioning for voice cloning</li><li>Model runs on PyTorch 2.0+ with CUDA 12.6, requiring ~10GB VRAM; inference on enterprise GPUs (~NVIDIA A4000) achieves ~40 tokens/sec</li><li>Code and weights available on <a href=https://github.com/nari-labs/dia>GitHub</a> and <a href=https://huggingface.co/nari-labs/Dia-1.6B>Hugging Face</a>; accessible via demo on Hugging Face Spaces</li><li>Fully open source under Apache 2.0 license, permitting commercial use with restrictions on impersonation and misinformation; development supported by Google TPU Cloud and Hugging Face grants</li></ul><h3 id=summary-23>Summary<a hidden class=anchor aria-hidden=true href=#summary-23>#</a></h3><p>Nari Labs introduced Dia, an open-source 1.6B parameter TTS model capable of producing highly natural dialogue with features like emotional tone, speaker tags, and nonverbal cues. Built with PyTorch 2.0+ and CUDA 12.6, it requires approximately 10GB VRAM, delivering around 40 tokens/sec on enterprise GPUs such as NVIDIA A4000. The model’s code and weights are available on <a href=https://github.com/nari-labs/dia>GitHub</a> and <a href=https://huggingface.co/nari-labs/Dia-1.6B>Hugging Face</a>, with a demo accessible via Hugging Face Spaces. Distributed under Apache 2.0 license, Dia aims to support diverse applications from content creation to assistive tech, emphasizing responsible use.</p><hr><h3 id=claude-code-best-practices--anthropic><a href=https://www.anthropic.com/engineering/claude-code-best-practices>Claude Code Best Practices \ Anthropic</a><a hidden class=anchor aria-hidden=true href=#claude-code-best-practices--anthropic>#</a></h3><h3 id=key-facts-24>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-24>#</a></h3><ul><li>Claude Code is a command line tool for agentic coding, released by Anthropic on April 18, 2025</li><li>Designed as a low-level, unopinionated, flexible, and scriptable system for integrating Claude into coding workflows</li><li>Emphasizes environment customization via <code>CLAUDE.md</code> files, tool allowlist management, and integration with MCP, GitHub, and other tools</li></ul><h3 id=summary-24>Summary<a hidden class=anchor aria-hidden=true href=#summary-24>#</a></h3><p>Claude Code is a research-developed command line tool aimed at agentic coding, providing raw model access without enforcing specific workflows. It allows users to customize their setup through <code>CLAUDE.md</code> files, which document commands, style, and environment specifics, and can be placed in various directory levels or the home folder. Users can tune these files for efficiency and clarity, and curate Claude’s allowed tools, including file system actions, bash commands, MCP tools, and GitHub interactions. Claude Code inherits the user’s shell environment, enabling integration with custom scripts, MCP servers, slash commands, and GitHub. It supports common workflows such as exploration, planning, coding, testing, and iteration, with best practices emphasizing specificity, visual aids, early course correction, and context management via <code>/clear</code>. The system also offers headless mode for automation in CI/CD pipelines, issue triage, linting, and large-scale data processing, with features like <code>-p</code> flag and <code>--output-format stream-json</code>. Multi-Claude workflows involve parallel instances for code generation, verification, and repository management, utilizing git worktrees or multiple checkouts for efficiency. Overall, Claude Code aims to enhance agentic coding productivity through flexible, customizable, and multi-instance workflows, with detailed documentation available at <a href=https://claude.ai/code>claude.ai/code</a>.</p><hr><h3 id=openai-would-buy-googles-chrome-browser-chatgpt-chief-says---bloomberg><a href=https://www.bloomberg.com/news/articles/2025-04-22/openai-would-buy-google-s-chrome-browser-chatgpt-chief-says>OpenAI Would Buy Google’s Chrome Browser, ChatGPT Chief Says - Bloomberg</a><a hidden class=anchor aria-hidden=true href=#openai-would-buy-googles-chrome-browser-chatgpt-chief-says---bloomberg>#</a></h3><h3 id=key-facts-25>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-25>#</a></h3><ul><li>OpenAI’s ChatGPT chief, Nick Turley, stated in court that OpenAI would be interested in acquiring Google’s Chrome browser if a federal court orders its spin-off</li><li>Many other parties would also seek to buy Chrome under such circumstances</li><li>The statement was made during a court hearing related to antitrust actions against Google</li></ul><h3 id=summary-25>Summary<a hidden class=anchor aria-hidden=true href=#summary-25>#</a></h3><p>OpenAI’s ChatGPT chief, Nick Turley, indicated in a court hearing that OpenAI would pursue purchasing Google’s Chrome browser if a federal court mandates its divestment, with many other entities also interested. This statement aligns with ongoing antitrust litigation efforts aimed at breaking up Google’s dominance in search and advertising. The potential sale of Chrome is linked to broader antitrust cases, including DOJ efforts to force Google to sell off parts of its business to reduce monopoly power.</p><hr><h3 id=new-research-backs-up-what-gamers-have-thought-for-years-cozy-video-games-can-be-an-antidote-to-stress-and-anxiety><a href=https://www.reuters.com/business/retail-consumer/cozy-video-games-can-quell-stress-anxiety-2025-01-27/>New research backs up what gamers have thought for years: cozy video games can be an antidote to stress and anxiety.</a><a hidden class=anchor aria-hidden=true href=#new-research-backs-up-what-gamers-have-thought-for-years-cozy-video-games-can-be-an-antidote-to-stress-and-anxiety>#</a></h3><h3 id=key-facts-26>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-26>#</a></h3><ul><li>Recent research supports that cozy video games can reduce stress and anxiety.</li><li>Studies indicate that playing such games increases mental well-being, with an extra hour of gameplay linked to higher life satisfaction.</li><li>The genre, originating with titles like Harvest Moon (1996) and popularized by Animal Crossing: New Horizons (2020), emphasizes relaxation, community-building, and non-violent challenges.</li></ul><h3 id=summary-26>Summary<a hidden class=anchor aria-hidden=true href=#summary-26>#</a></h3><p>New research confirms that cozy video games can serve as effective tools for alleviating stress and anxiety. These games, characterized by their relaxing gameplay, community focus, and non-violent mechanics, attract both long-time gamers and newcomers. The genre gained prominence with titles like Harvest Moon (1996) and surged in popularity after Animal Crossing: New Horizons launched on March 20, 2020, coinciding with COVID-19 lockdowns, and sold over 13 million units within six weeks. Studies, including Hiroyuki Egami’s 2022 research in Japan, show that owning a game console and increasing gameplay time by one hour daily correlates with reduced psychological distress and enhanced life satisfaction. Other research, such as Michael Wong’s 2021 survey at McMaster University, found no significant difference in stress reduction between casual gaming and mindfulness meditation. Therapeutic applications are emerging, with video games being explored as interventions for ADHD and as tools for emotional processing, exemplified by Spiritfarer, which helps players explore death and grief. These games often feature inclusive design, customizable characters, and tasks like gardening and house decoration, fostering a sense of comfort and community. Developers like Dorian Signargout emphasize promoting inclusivity through diverse character representations. Overall, cozy games are increasingly recognized for their mental health benefits, offering a sanctuary of simplicity and connection amid a complex world.</p><hr><h3 id=building-a-website-fit-for-1999---wesley-moore><a href=https://www.wezm.net/v2/posts/2025/website-fit-for-1999/>Building a Website Fit for 1999 - Wesley Moore</a><a hidden class=anchor aria-hidden=true href=#building-a-website-fit-for-1999---wesley-moore>#</a></h3><h3 id=key-facts-27>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-27>#</a></h3><ul><li>Wesley Moore built a retro-themed website in HTML4, hosted at <a href=http://home.wezm.net/~wmoore/>home.wezm.net/~wmoore/</a>, inspired by Ruben’s Retro Corner and a Raspberry Pi project.</li><li>The site was developed to run on old hardware and browsers, tested in IE 4.01 and Netscape Navigator 3.01 in Mac OS 8.1 via Basilisk II emulator.</li><li>The website uses static HTML generated with MiniJinja, jaq, and make, with dynamic content updated every 5 minutes via a Rust server using Axum, served through Nginx reverse proxy with Tailscale for network access.</li><li>Static content is stored on a Qotom mini PC running Chimera Linux, with deployment via git pull and make, and the Rust binary managed as a systemd-like process with Dinit.</li><li>The site includes static pages with animated GIFs, pixel icons, and server stats, emphasizing minimal CSS and table-based layouts for compatibility with legacy browsers.</li></ul><h3 id=summary-27>Summary<a hidden class=anchor aria-hidden=true href=#summary-27>#</a></h3><p>Wesley Moore created a retro-styled website emulating the 1999 web aesthetic, implemented entirely in HTML4 and served over plain HTTP to ensure compatibility with outdated browsers like IE 4.01 and Netscape 3.01, tested in Mac OS 8.1 emulation. The site features static pages generated with MiniJinja, jaq, and make, with dynamic content such as server uptime, memory, and energy data refreshed every five minutes via a Rust server built with Axum, reverse-proxied through Nginx and connected over Tailscale. Hosting is on a Qotom mini PC running Chimera Linux, with deployment managed through git and a custom package build process. The website emphasizes minimal CSS, table-based layouts, and direct HTML4 coding, including animated GIFs and pixel icons, to preserve authenticity and functionality on legacy hardware. The project code is available on <a href=https://github.com/wezm/wezm.net>GitHub</a>, with future plans to expand content on additional pages.</p><hr><h3 id=i-thought-i-bought-a-camera-but-no-dji-sold-me-a-license-to-use-their-camera----youtube><a href="https://www.youtube.com/watch?v=aUOnQ_boqCw">I thought I bought a camera, but no! DJI sold me a LICENSE to use their camera 🤦‍♂️ - YouTube</a><a hidden class=anchor aria-hidden=true href=#i-thought-i-bought-a-camera-but-no-dji-sold-me-a-license-to-use-their-camera----youtube>#</a></h3><h3 id=key-facts-28>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-28>#</a></h3><ul><li>The YouTube video titled &ldquo;I thought I bought a camera, but no! DJI sold me a LICENSE to use their camera&rdquo; was uploaded by Louis Rossmann on April 17, 2025, with 416,986 views.</li><li>The content highlights that DJI sells users a license to operate their cameras rather than the camera hardware itself.</li><li>The video has received 28K likes and discusses issues related to proprietary licensing models in consumer electronics.</li></ul><h3 id=summary-28>Summary<a hidden class=anchor aria-hidden=true href=#summary-28>#</a></h3><p>Louis Rossmann&rsquo;s video critiques DJI&rsquo;s business model, revealing that purchasing a DJI camera does not grant ownership of the hardware but instead provides a license to use the camera. This licensing approach implies that users do not own the device outright, but are granted permission to operate it under specific terms. The video emphasizes the technical and legal implications of this model, suggesting it shifts ownership rights and control from consumers to the manufacturer. The discussion underscores broader concerns about proprietary licensing in consumer electronics, where users may face restrictions on repair, modification, or resale. The video&rsquo;s key data points include the date of upload (April 17, 2025), view count (416,986), and the focus on the distinction between hardware ownership and licensing rights in DJI products.</p><h2 id=-software-development>▶️ Software Development<a hidden class=anchor aria-hidden=true href=#-software-development>#</a></h2><h3 id=pipelining-might-be-my-favorite-programming-language-feature--mondtech-magazine><a href=https://herecomesthemoon.net/2025/04/pipelining/>Pipelining might be my favorite programming language feature | MOND←TECH MAGAZINE</a><a hidden class=anchor aria-hidden=true href=#pipelining-might-be-my-favorite-programming-language-feature--mondtech-magazine>#</a></h3><h3 id=key-facts-29>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-29>#</a></h3><ul><li>The article advocates for pipelining as a programming language feature that allows passing previous values as arguments, enhancing code readability and editing ease.</li><li>Pipelining enables method chaining and function composition, exemplified by Rust’s trait-based approach, Haskell’s <code>&</code> and <code>$</code> operators, and SQL’s pipe syntax.</li><li>Benefits include improved code discovery, simplified refactoring, better IDE support, and clearer data flow, reducing nested parentheses and complex variable tracking.</li></ul><h3 id=summary-29>Summary<a hidden class=anchor aria-hidden=true href=#summary-29>#</a></h3><p>The article presents pipelining as a highly valued programming language feature that simplifies code structure by passing previous values directly into subsequent functions or methods. It contrasts traditional nested or imperative code with pipelined syntax, emphasizing how the latter improves readability, ease of editing, and IDE support, especially in languages like Rust, Haskell, and SQL. Rust’s trait-based method calls exemplify effective pipelining without requiring higher-kinded types, while Haskell’s <code>&</code> and <code>$</code> operators demonstrate functional composition that enhances clarity. SQL’s proposed pipe syntax reduces nested queries into linear, line-by-line transformations, aligning with LINQ-style readability. The article also discusses the builder pattern as a form of pipelining for object construction, and critiques Haskell’s complex operator ecosystem, advocating for more approachable, pipeline-friendly syntax. Overall, pipelining is praised for enabling modular, understandable, and maintainable code, with a focus on top-to-bottom data flow and minimal parentheses or variable tracking.</p><h2 id=-management-and-leadership>▶️ Management and Leadership<a hidden class=anchor aria-hidden=true href=#-management-and-leadership>#</a></h2><h3 id=android-phones-will-soon-reboot-themselves-after-sitting-unused-for-3-days---ars-technica><a href=https://arstechnica.com/gadgets/2025/04/android-phones-will-soon-reboot-themselves-after-sitting-unused-for-3-days/>Android phones will soon reboot themselves after sitting unused for 3 days - Ars Technica</a><a hidden class=anchor aria-hidden=true href=#android-phones-will-soon-reboot-themselves-after-sitting-unused-for-3-days---ars-technica>#</a></h3><h3 id=key-facts-30>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-30>#</a></h3><ul><li>Android devices will automatically reboot after being locked and unused for 3 consecutive days via a Google Play Services update (version 25.14), rolling out gradually from April 14, 2025.</li><li>The feature enhances security by encrypting data in the &ldquo;Before First Unlock&rdquo; (BFU) state, making data retrieval more difficult, especially after prolonged inactivity.</li><li>The update is part of a broader set of improvements, including UI enhancements, better connectivity with cars and watches, and content previews in Quick Share.</li></ul><h3 id=summary-30>Summary<a hidden class=anchor aria-hidden=true href=#summary-30>#</a></h3><p>A silent update to Google Play Services (version 25.14), released on April 14, 2025, introduces an auto-restart feature for Android devices, activating after three days of inactivity when the device remains locked. This feature aims to improve security by encrypting all data in the BFU state, where biometrics and location-based unlocking are disabled, and access is limited to PIN or passcode. The automatic reboot makes data extraction significantly more difficult, aligning with similar features like Apple&rsquo;s Inactivity Reboot introduced in iOS 18.1. The update is delivered automatically to certified devices over the next week or more, with no user intervention required. This mechanism leverages Google Play Services&rsquo; background update system, which has been central to Android&rsquo;s system update strategy, allowing Google to enhance security and functionality remotely. The broader update includes UI improvements, enhanced device connectivity, and content sharing features.</p><hr><h3 id=deciphering-glyph--stop-writing><a href=https://blog.glyph.im/2025/04/stop-writing-init-methods.html>Deciphering Glyph :: Stop Writing <code>__init__</code> Methods</a><a hidden class=anchor aria-hidden=true href=#deciphering-glyph--stop-writing>#</a></h3><h3 id=key-facts-31>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-31>#</a></h3><ul><li>The article critiques the widespread use of custom <code>__init__</code> methods in Python classes, especially before dataclasses (introduced in Python 3.7), highlighting their drawbacks.</li><li>It presents a structured approach: using <code>@dataclass</code> for attribute management, <code>@classmethod</code> factory methods for object creation, and <code>typing.NewType</code> for enforcing constraints on primitive types.</li><li>This methodology ensures valid object construction, improves discoverability, and enhances testability without relying on side-effect-laden <code>__init__</code> methods.</li></ul><h3 id=summary-31>Summary<a hidden class=anchor aria-hidden=true href=#summary-31>#</a></h3><p>The article argues that defining custom <code>__init__</code> methods in Python classes is an anti-pattern, historically used to facilitate object instantiation with attributes like <code>x</code> and <code>y</code> in data structures such as <code>2DCoordinate</code>. Prior to Python 3.7&rsquo;s <code>@dataclass</code>, developers resorted to complex alternatives like factory functions or attribute defaults, which had significant drawbacks. Using <code>__init__</code> for side-effect-laden setup, such as opening file descriptors in a <code>FileReader</code> class, introduces problems including reduced testability, increased complexity, and potential invalid object states. The proposed solution involves leveraging <code>@dataclass</code> to automatically generate attribute assignment, replacing <code>__init__</code> with <code>@classmethod</code> factory methods (e.g., <code>FileReader.open</code>) for flexible, asynchronous, or context-specific instantiation, and employing <code>typing.NewType</code> to enforce constraints on primitive types like file descriptors. This approach ensures objects are always valid, simplifies testing, and makes object creation more discoverable and future-proof, aligning with Python&rsquo;s idiomatic practices.</p><hr><h3 id=fossil-fuels-fall-below-50-of-us-electricity-for-the-first-month-on-record--ember><a href=https://ember-energy.org/latest-updates/fossil-fuels-fall-below-50-of-us-electricity-for-the-first-month-on-record/>Fossil fuels fall below 50% of US electricity for the first month on record | Ember</a><a hidden class=anchor aria-hidden=true href=#fossil-fuels-fall-below-50-of-us-electricity-for-the-first-month-on-record--ember>#</a></h3><h3 id=key-facts-32>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-32>#</a></h3><ul><li>In March 2025, fossil fuels accounted for 49.2% of US electricity, the first month below 50%, surpassing the previous record low of 51% in April 2024</li><li>Clean energy sources generated 50.8% of US electricity in March 2025, driven by record-high wind and solar power</li><li>Wind and solar reached a combined 24.4% of US electricity, with solar increasing 37% (+8.3 TWh) and wind increasing 12% (+5.7 TWh) compared to March 2024; total wind and solar generation hit 83 TWh, an 11% rise over April 2024</li></ul><h3 id=summary-32>Summary<a hidden class=anchor aria-hidden=true href=#summary-32>#</a></h3><p>In March 2025, the US experienced a historic shift as fossil fuels contributed only 49.2% to electricity generation, marking the first month on record below the 50% threshold, according to Ember. This milestone reflects a long-term decline in fossil fuel reliance, with wind and solar power reaching a record 24.4% of the energy mix—solar alone increased by 37% (+8.3 TWh) and wind by 12% (+5.7 TWh) year-over-year. Combined, wind and solar generated 83 TWh, an 11% increase from April 2024, while fossil fuel generation decreased by 2.5% (-4.3 TWh). Since March 2015, when fossil fuels accounted for 65% and solar just 1%, the share of wind and solar has more than quadrupled, with solar expected to constitute over half of new US capacity in 2025. The rapid growth of renewable energy signifies a pivotal transition toward a cleaner power system, with wind and solar increasingly displacing coal and gas, and signals a potential approaching tipping point where clean energy surpasses fossil fuel reliance in the US electricity sector. More details are available at <a href=https://ember-energy.org/latest-updates/fossil-fuels-fall-below-50-of-us-electricity-for-the-first-month-on-record/>Ember</a>.</p><hr><h3 id=github---lukasogunfeitimitiktok-reverseengineering><a href=https://github.com/LukasOgunfeitimi/TikTok-ReverseEngineering>GitHub - LukasOgunfeitimi/TikTok-ReverseEngineering</a><a hidden class=anchor aria-hidden=true href=#github---lukasogunfeitimitiktok-reverseengineering>#</a></h3><h3 id=key-facts-33>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-33>#</a></h3><ul><li>The project reverse engineers TikTok&rsquo;s custom virtual machine (VM) used for obfuscation and security.</li><li>It includes tools for deobfuscating <code>webmssdk.js</code>, decompiling VM bytecode, injecting deobfuscated scripts, and generating signed URLs.</li><li>Bytecode is stored as a long string XOR-encrypted with a key embedded within the string; decryption involves base64 decoding, XOR with a key derived from the string, and leb128 decoding for compression.</li></ul><h3 id=summary-33>Summary<a hidden class=anchor aria-hidden=true href=#summary-33>#</a></h3><p>The repository provides tools to reverse engineer TikTok&rsquo;s sophisticated VM-based obfuscation system. It deobfuscates heavily encrypted <code>webmssdk.js</code> by reversing bracket notation encoding, replacing function arrays with standard functions, and reconstructing the VM&rsquo;s bytecode execution logic. The bytecode is stored as a long base64-encoded string, XOR-encrypted with an embedded key, and leb128 compressed, which is decoded to extract functions, strings, and metadata. The VM supports scopes, nested functions, and exception handling, indicating a complex, custom bytecode interpreter. Decompilation involves analyzing VM switch-case structures, with manual and AI-assisted efforts to produce readable code snippets. The system also includes a URL signing mechanism that replicates TikTok&rsquo;s request signatures (<code>msToken</code>, <code>X-Bogus</code>, <code>_signature</code>) by executing specific VM functions, enabling authenticated requests such as posting comments. The project emphasizes that TikTok&rsquo;s VM is subject to frequent updates, requiring ongoing reverse engineering efforts. It leverages browser extensions like Tampermonkey and CSP bypasses for debugging and script injection, facilitating real-time analysis and testing.</p><hr><h3 id=native-visionos-platform-support-by-rsanchezsaez--pull-request-105628--godotenginegodot--github><a href=https://github.com/godotengine/godot/pull/105628>Native visionOS platform support by rsanchezsaez · Pull Request #105628 · godotengine/godot · GitHub</a><a hidden class=anchor aria-hidden=true href=#native-visionos-platform-support-by-rsanchezsaez--pull-request-105628--godotenginegodot--github>#</a></h3><h3 id=key-facts-34>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-34>#</a></h3><ul><li>This PR adds native support for the visionOS platform to Godot, using iOS as the base and reusing code via <code>drivers/apple_embedded</code>.</li><li>Implements platform-specific logic, including app instantiation, display server, and export plugin, with no OpenGL support due to visionOS limitations.</li><li>Renames export plugin files and shared configuration to accommodate both iOS and visionOS, and moves shared code into <code>drivers/apple_embedded</code>.</li><li>Tests confirm native execution on visionOS with Metal renderer; DPI metrics are hardcoded, and icon asset support is pending.</li><li>The PR is part of a three-step plan, with subsequent PRs planned for SwiftUI integration and immersive VR plugin support.</li></ul><h3 id=summary-34>Summary<a hidden class=anchor aria-hidden=true href=#summary-34>#</a></h3><p>This PR introduces a native <code>visionOS</code> platform support for Godot, closely aligned with the existing iOS implementation. It refactors shared code into <code>drivers/apple_embedded</code>, renames relevant export and platform files, and disables OpenGL due to visionOS constraints. Testing with the Platformer demo confirms native execution using Metal, though DPI metrics are static and icon asset generation is incomplete. The contribution lays the groundwork for future immersive VR support and SwiftUI integration, with ongoing discussions about platform maintenance and compatibility considerations.</p><hr><h3 id=github---humanlayer12-factor-agents-what-are-the-principles-we-can-use-to-build-llm-powered-software-that-is-actually-good-enough-to-put-in-the-hands-of-production-customers><a href=https://github.com/humanlayer/12-factor-agents>GitHub - humanlayer/12-factor-agents: What are the principles we can use to build LLM-powered software that is actually good enough to put in the hands of production customers?</a><a hidden class=anchor aria-hidden=true href=#github---humanlayer12-factor-agents-what-are-the-principles-we-can-use-to-build-llm-powered-software-that-is-actually-good-enough-to-put-in-the-hands-of-production-customers>#</a></h3><h3 id=key-facts-35>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-35>#</a></h3><ul><li>The project outlines 12 core principles for building reliable, scalable, and maintainable LLM-powered software suitable for production deployment.</li><li>These principles include owning prompts, controlling context windows, structuring tool outputs, unifying execution and business state, and owning control flow.</li><li>The guide emphasizes integrating modular concepts from agent design into existing products rather than relying solely on frameworks, to accelerate deployment of high-quality AI features.</li></ul><h3 id=summary-35>Summary<a hidden class=anchor aria-hidden=true href=#summary-35>#</a></h3><p>The &ldquo;12-factor agents&rdquo; framework, inspired by the <a href=https://12factor.net/>12 Factor Apps</a>, provides foundational principles for developing production-ready LLM-powered software. It addresses common challenges such as managing prompts, context windows, structured tool outputs, and control flow, advocating for ownership and modularity in these areas. The approach discourages building from scratch with full frameworks; instead, it recommends incorporating small, modular agent concepts into existing products to improve reliability, scalability, and maintainability. The twelve factors include transforming natural language into tool calls, owning prompts and context, unifying execution and business state, enabling simple APIs for launching and pausing agents, and designing small, focused agents that can trigger from anywhere and act as stateless reducers. The guide also discusses the historical evolution from DAG-based orchestrators to agent loops, highlighting the limitations of purely loop-based agent architectures. It emphasizes that core engineering techniques, rather than frameworks alone, are key to deploying effective AI agents in customer-facing environments. Additional resources and related frameworks are linked throughout, supporting developers in applying these principles across various languages and platforms.</p><hr><h3 id=things-zig-comptime-won><a href=https://matklad.github.io/2025/04/19/things-zig-comptime-wont-do.html>Things Zig comptime Won&rsquo;t Do</a><a hidden class=anchor aria-hidden=true href=#things-zig-comptime-won>#</a></h3><h3 id=key-facts-36>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-36>#</a></h3><ul><li>Zig’s comptime is designed to be restrictive, preventing host architecture leakage, dynamic code evaluation (#eval), DSL creation, RTTI, API extension, and IO operations</li><li>Comptime code observes the target architecture, not the host, ensuring cross-compilation correctness</li><li>Zig lacks facilities for dynamic source code generation, custom syntax extension, runtime type information, and input/output during compilation</li></ul><h3 id=summary-36>Summary<a hidden class=anchor aria-hidden=true href=#summary-36>#</a></h3><p>Zig’s compile-time evaluation (comptime) features are intentionally limited to maintain safety, portability, and simplicity. Comptime code executes in the target environment, not on the host machine, preventing host architecture leakage, as demonstrated by examples showing architecture-dependent behavior during cross-compilation. Zig does not support dynamic source code injection or evaluation (#eval), relying instead on partial evaluation and specialization, such as marking function parameters with <code>comptime</code> to generate optimized code paths. It also lacks support for custom syntax DSLs, as all code operates on Zig values, with embedded DSLs like <code>print</code> using format strings. Zig does not include runtime type information (RTTI), requiring users to implement manual reflection for dynamic data handling, exemplified by a custom <code>RTTI</code> union and <code>print_dyn</code> function. Additionally, Zig types cannot be extended with methods post-generation; API modifications are limited to reflection-based internal logic. Finally, Zig’s compile-time evaluation is hermetic, with no I/O capabilities, ensuring reproducibility and safety, though build systems can invoke external programs for code generation.</p><hr><h3 id=matthewsinclaircom--intelligence-innovation-leadership-influence><a href=https://matthewsinclair.com/blog/0178-why-llm-powered-programming-is-more-mech-suit-than-artificial-human>matthewsinclair.com · Intelligence. Innovation. Leadership. Influence.</a><a hidden class=anchor aria-hidden=true href=#matthewsinclaircom--intelligence-innovation-leadership-influence>#</a></h3><h3 id=key-facts-37>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-37>#</a></h3><ul><li>Author used Claude Code to develop approximately 30,000 lines of code across backend and frontend projects within weeks</li><li>AI tools function as amplifiers (&ldquo;mech suit&rdquo;) rather than replacements, requiring human oversight and architectural judgment</li><li>Vigilance is necessary due to AI&rsquo;s tendency to make bewildering or inappropriate decisions, necessitating constant review and control</li></ul><h3 id=summary-37>Summary<a hidden class=anchor aria-hidden=true href=#summary-37>#</a></h3><p>The article argues that LLM-powered programming tools like Claude Code serve as amplifiers—&ldquo;mech suits&rdquo;—that enhance developer capabilities rather than replace humans. The author built two applications totaling around 30,000 lines of code in weeks, demonstrating significant acceleration in development speed. These tools provide tremendous &ldquo;lifting power,&rdquo; but require developers to maintain constant vigilance, guiding the AI and correcting its often bewildering or biased decisions, similar to piloting an aircraft. The process shifts focus from coding to designing, reviewing, and maintaining architectural integrity, emphasizing the importance of experience and domain knowledge to recognize when AI output is flawed. The author highlights the &ldquo;centaur effect,&rdquo; where human-AI collaboration outperforms either alone, with AI handling pattern recognition and tactical execution, and humans providing strategic oversight. The article stresses that effective use of these tools demands new skills—particularly in delegation, ruthless discarding of suboptimal solutions, and clear problem understanding. While some fear AI will replace programmers, the author sees the future as augmentation, where mastery of AI tools becomes a core skill, transforming the role of developers into strategic operators of powerful systems. The key to success lies in balancing delegation with control, leveraging AI for speed while applying human judgment to ensure quality and direction.</p><hr><h3 id=microsoft-just-launched-powerful-ai><a href=https://venturebeat.com/ai/microsoft-just-launched-powerful-ai-agents-that-could-completely-transform-your-workday-and-challenge-googles-workplace-dominance/>Microsoft just launched powerful AI &lsquo;agents&rsquo; that could completely transform your workday &ndash; and challenge Google&rsquo;s workplace dominance | VentureBeat</a><a hidden class=anchor aria-hidden=true href=#microsoft-just-launched-powerful-ai>#</a></h3><h3 id=key-facts-38>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-38>#</a></h3><ul><li>Microsoft announced the &ldquo;Microsoft 365 Copilot Wave 2 Spring release&rdquo; introducing AI agents with deep reasoning capabilities for workplace tasks.</li><li>New AI agents, Researcher and Analyst, leverage OpenAI’s reasoning models to handle complex research and data analysis, connecting information across sources.</li><li>These agents will be accessible via an &ldquo;Agent Store&rdquo; featuring integrations with Jira, Monday.com, Miro, and custom organizational agents; rollout begins in late May 2025.</li></ul><h3 id=summary-38>Summary<a hidden class=anchor aria-hidden=true href=#summary-38>#</a></h3><p>Microsoft launched the &ldquo;Microsoft 365 Copilot Wave 2 Spring release,&rdquo; expanding AI tools with agents capable of deep reasoning for enterprise tasks. The Researcher and Analyst agents, powered by OpenAI’s reasoning models, perform complex research and data analysis, aiding in activities like preparing business reviews. They will be available through an &ldquo;Agent Store&rdquo; with integrations from partners such as Jira, Monday.com, and Miro, with deployment starting in late May 2025. Microsoft envisions AI as an organizational collaborator, transforming workplace productivity and structure.</p><hr><h3 id=intel-intc-to-announce-plans-this-week-to-lay-off-more-than-20-of-staff---bloomberg><a href=https://www.bloomberg.com/news/articles/2025-04-23/intel-to-announce-plans-this-week-to-cut-more-than-20-of-staff>Intel (INTC) to Announce Plans This Week to Lay Off More Than 20% of Staff - Bloomberg</a><a hidden class=anchor aria-hidden=true href=#intel-intc-to-announce-plans-this-week-to-lay-off-more-than-20-of-staff---bloomberg>#</a></h3><h3 id=key-facts-39>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-39>#</a></h3><ul><li>Intel plans to announce a reduction of over 20% of its staff this week</li><li>The restructuring aims to eliminate bureaucracy and rebuild an engineering-driven culture</li><li>This is the first major restructuring under CEO Lip-Bu Tan, who assumed leadership last month</li></ul><h3 id=summary-39>Summary<a hidden class=anchor aria-hidden=true href=#summary-39>#</a></h3><p>Intel Corporation is set to disclose plans this week to cut more than 20% of its workforce, targeting organizational streamlining and management restructuring, according to sources. The initiative seeks to reduce bureaucracy and foster an engineering-focused culture. This marks the first significant restructuring under new CEO Lip-Bu Tan, who took over leadership last month. The move reflects Intel’s efforts to address its financial struggles and improve operational efficiency.</p><hr><h3 id=tls-certificate-lifetimes-will-officially-reduce-to-47-days--digicert><a href=https://www.digicert.com/blog/tls-certificate-lifetimes-will-officially-reduce-to-47-days>TLS Certificate Lifetimes Will Officially Reduce to 47 Days | DigiCert</a><a hidden class=anchor aria-hidden=true href=#tls-certificate-lifetimes-will-officially-reduce-to-47-days--digicert>#</a></h3><h3 id=key-facts-40>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-40>#</a></h3><ul><li>CA/Browser Forum voted to reduce TLS certificate lifetime to 47 days, with implementation starting March 15, 2026</li><li>Maximum TLS certificate validity decreases from 398 days (current) to 200 days (2026), then to 100 days (2027), and finally to 47 days (2029)</li><li>Reuse period for domain/IP validation info drops from 398 days to 10 days by March 15, 2029; SII reuse in OV/EV certificates reduces from 825 days to 398 days</li></ul><h3 id=summary-40>Summary<a hidden class=anchor aria-hidden=true href=#summary-40>#</a></h3><p>The CA/Browser Forum has officially amended the <a href=https://groups.google.com/a/groups.cabforum.org/g/servercert-wg/c/bvWh5RN6tYI>TLS Baseline Requirements</a> to progressively shorten TLS certificate lifetimes, with the maximum validity decreasing from 398 days to 47 days by March 15, 2029. The schedule begins with a reduction to 200 days in 2026, then to 100 days in 2027, and finally to 47 days in 2029. Concurrently, the reuse period for domain and IP address validation information will decline from 398 days to 10 days, and validation of Subject Identity Information (SII) in OV and EV certificates will be limited to 398 days, down from 825 days. The rationale emphasizes increased trustworthiness of certificate data and mitigates reliance on unreliable revocation systems like CRLs and OCSP. Apple justified the change by highlighting the necessity of automation for managing shorter-lived certificates, with DigiCert supporting this transition through solutions like <a href=https://www.digicert.com/trust-lifecycle-manager>Trust Lifecycle Manager</a> and <a href=https://www.digicert.com/tls-ssl/certcentral-tls-ssl-manager>CertCentral</a>, including ACME support for automated issuance and renewal. The move aims to enhance security, reduce outages, and promote widespread adoption of automation in certificate management.</p><hr><h3 id=why-i-cannot-be-technical><a href=https://www.fightforthehuman.com/why-i-cannot-be-technical/>Why I Cannot Be Technical</a><a hidden class=anchor aria-hidden=true href=#why-i-cannot-be-technical>#</a></h3><h3 id=key-facts-41>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-41>#</a></h3><ul><li>Author asserts she cannot be truly &ldquo;Technical&rdquo; due to structural and social barriers within tech systems</li><li>&ldquo;Technical&rdquo; is a legitimacy-based, exclusionary label rooted in social hierarchies and identity policing</li><li>The essay critiques how tech systems perpetuate dehumanization, inequality, and reinforce boundaries based on gender, race, class, and ideology</li></ul><h3 id=summary-41>Summary<a hidden class=anchor aria-hidden=true href=#summary-41>#</a></h3><p>Cat Hicks argues that she cannot genuinely be &ldquo;Technical&rdquo; because the label functions as a structural designation that enforces legitimacy through social exclusion, rather than problem-solving ability. She emphasizes that &ldquo;Technical&rdquo; operates outside of actual problem-solving, serving to uphold hierarchies, policing boundaries, and perpetuating systemic inequalities based on gender, race, class, and social location. Hicks highlights how the social and political context of tech creates unearned privileges and unwarranted exclusions, making it impossible for her to be recognized as legitimate within that framework despite her expertise in psychology and her impactful work. She critiques the hierarchical, hierarchical, and geo-located nature of tech, which maintains a culture of dehumanization and marginalization of those deemed &ldquo;not-Technical.&rdquo; Hicks advocates for recognizing the full humanity of all individuals, emphasizing that true work involves caring, safety, and community rather than perpetuating the &ldquo;hamster wheel&rdquo; of performance and exclusion. She calls for a space of rehumanization and honest conversation about the systemic issues within tech, emphasizing that &ldquo;Technical&rdquo; is a social construct that cannot be earned or bestowed but is maintained through systemic reinforcement. Hicks urges a focus on collective healing, shared storytelling, and challenging the structural barriers that devalue human connection in technology.</p><hr><h3 id=america-underestimates-the-difficulty-of-bringing-manufacturing-back--molson-hart><a href=https://www.molsonhart.com/blog/america-underestimates-the-difficulty-of-bringing-manufacturing-back>America Underestimates the Difficulty of Bringing Manufacturing Back — Molson Hart</a><a hidden class=anchor aria-hidden=true href=#america-underestimates-the-difficulty-of-bringing-manufacturing-back--molson-hart>#</a></h3><h3 id=key-facts-42>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-42>#</a></h3><ul><li>President announced tariffs on imports ranging from 10% to 49% on April 2, 2025, aiming to revive U.S. manufacturing</li><li>The article presents 14 reasons why these tariffs will not succeed in bringing manufacturing back and may worsen economic decline</li><li>Expert with 15 years in manufacturing argues that tariffs are insufficient due to supply chain weaknesses, high costs, lack of knowhow, infrastructure deficits, and complex policy environment</li></ul><h3 id=summary-42>Summary<a hidden class=anchor aria-hidden=true href=#summary-42>#</a></h3><p>The April 2025 U.S. tariff policy, imposing import taxes between 10% and 49%, aims to restore domestic manufacturing but is fundamentally flawed. Key issues include tariffs being too low to offset higher U.S. production costs, as manufacturing in the U.S. remains more expensive than in Asia even with tariffs. The U.S. supply chain for industrial components is weak, relying heavily on Asian factories, making local production uncompetitive. The country lacks essential manufacturing knowhow, such as moldmaking and semiconductor fabrication, which cannot be quickly or easily developed. Labor costs in the U.S. are higher not only due to wages but also because of lower productivity, work ethic, and infrastructure quality compared to China. Infrastructure deficits, including electricity generation and transportation networks, further hinder manufacturing revival. The lengthy process to build and operationalize new factories (minimum two years) and the uncertainty caused by fluctuating tariffs discourage investment. Complex and inconsistent tariff enforcement, along with a litigious business environment, exacerbate risks. The policy risks causing a recession, as supply chain disruptions and increased costs lead to inflation or deflation. The article predicts that unless policies change, globalization will bypass the U.S., with manufacturing shifting to countries like Vietnam and Mexico. To genuinely rebuild manufacturing, the U.S. must address fundamental social, infrastructural, and educational issues, implement gradual tariff increases, and incentivize high-end production, rather than relying solely on tariffs.</p><hr><h3 id=decreased-co2-saturation-during-circular-breathwork-supports-emergence-of-altered-states-of-consciousness--communications-psychology><a href=https://www.nature.com/articles/s44271-025-00247-0>Decreased CO2 saturation during circular breathwork supports emergence of altered states of consciousness | Communications Psychology</a><a hidden class=anchor aria-hidden=true href=#decreased-co2-saturation-during-circular-breathwork-supports-emergence-of-altered-states-of-consciousness--communications-psychology>#</a></h3><h3 id=key-facts-43>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-43>#</a></h3><ul><li>Circular breathwork induces significant reductions in end-tidal CO2 pressure (etCO2), with active participants reaching levels as low as 10–20 mmHg, compared to 36.7 ± 1.5 mmHg in non-hyperventilating controls</li><li>Decreases in etCO2 are significantly correlated with the onset and depth of altered states of consciousness (ASCs), resembling psychedelic experiences across domains such as ego dissolution and unity</li><li>Both Holotropic and Conscious-Connected breathwork produce similar physiological and experiential outcomes, with session durations of approximately 3 hours and 1.5 hours respectively, engaging the same mechanisms</li></ul><h3 id=summary-43>Summary<a hidden class=anchor aria-hidden=true href=#summary-43>#</a></h3><p>This study demonstrates that circular breathwork, involving deliberate hyperventilation, can reliably trigger altered states of consciousness (ASCs) akin to those produced by psychedelics, with experience scales (MEQ30 and 11D-ASC) reaching levels comparable to moderate doses of psilocybin, LSD, and MDMA. Physiologically, active participants exhibited a marked reduction in end-tidal CO2 (etCO2), dropping to as low as 10–20 mmHg, which was strongly associated with ASC onset (<em>r</em> = -0.46 to -0.47, <em>p</em> &lt; 0.01). These reductions in CO2 pressure, supported by prior hyperventilation research, appear to serve as a physiological trigger for ASC emergence, with lower etCO2 levels (&lt;35 mmHg) virtually guaranteeing profound experiences when falling below 20 mmHg. The experiential depth correlated with physiological changes and persisted even as etCO2 levels normalized, suggesting a transition into a neuronal state of heightened perception, consistent with the concept of Pivotal Mental States. Both breathwork styles—Holotropic and Conscious-Connected—elicited similar physiological and subjective effects despite differences in session length, indicating shared underlying mechanisms. Post-session assessments revealed sustained improvements in well-being and reductions in depressive symptoms, with deeper ASC experiences predicting greater long-term benefits. Physiologically, breathwork caused a decrease in salivary alpha-amylase (sympathetic activity marker) and an increase in IL-1β (inflammatory marker), with subjective ASC depth inversely related to inflammation. These findings position breathwork as a non-pharmacological tool capable of inducing profound, psychedelic-like states through physiological modulation, with potential therapeutic applications supported by its safety profile and accessibility.</p><hr><h3 id=synology-lost-the-plot-with-hard-drive-locking-move---servethehome><a href=https://www.servethehome.com/synology-lost-the-plot-with-hard-drive-locking-move/>Synology Lost the Plot with Hard Drive Locking Move - ServeTheHome</a><a hidden class=anchor aria-hidden=true href=#synology-lost-the-plot-with-hard-drive-locking-move---servethehome>#</a></h3><h3 id=key-facts-44>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-44>#</a></h3><ul><li>Synology plans to restrict its 2025 Plus NAS models to use only its own branded hard drives</li><li>This move disables features like volume-wide deduplication, lifespan analysis, and automatic firmware updates for third-party drives</li><li>The new policy limits maximum raw capacity to 128TB in an 8-bay NAS, compared to 208TB with competing brands like QNAP and TrueNAS using third-party drives</li></ul><h3 id=summary-44>Summary<a hidden class=anchor aria-hidden=true href=#summary-44>#</a></h3><p>Synology is moving towards vendor-locking its 2025 Plus series NAS devices to only support Synology-branded hard drives, a shift that restricts compatibility with third-party drives and disables features such as volume-wide deduplication, lifespan analysis, and automatic firmware updates for non-branded drives. This strategy appears to be driven by a desire to increase margins, as Synology’s current drives max out at 16TB (e.g., HAT3310-16T), whereas competitors like WD Red Pro and Toshiba N300 Pro offer drives up to 26TB. Locking drives limits data security options, as users cannot quickly replace failed drives with models from other vendors, which can delay rebuild times and compromise data safety. Additionally, long-term availability of Synology drives is uncertain, raising concerns about future replacements and ecosystem dependence. The move is viewed as a step back in hardware flexibility and could harm Synology’s reputation, especially given the aging hardware and limited hardware refresh cycles. Critics argue that this lock-in reduces consumer choice and may lead to higher costs, as Synology’s drives are priced higher and have slower delivery times compared to third-party options. The industry context shows that vendor lock-in strategies often backfire, and many users prefer building custom solutions or switching to open-source NAS platforms like TrueNAS.</p><hr><h3 id=apple-meta-fined-by-eu-ordered-to-comply-with-tech-competition-rules---wsj><a href="https://www.wsj.com/tech/apple-meta-fined-by-eu-ordered-to-comply-with-tech-competition-rules-9063b7e6?st=hbwZ53&amp;reflink=desktopwebshare_permalink">Apple, Meta Fined by EU, Ordered to Comply With Tech Competition Rules - WSJ</a><a hidden class=anchor aria-hidden=true href=#apple-meta-fined-by-eu-ordered-to-comply-with-tech-competition-rules---wsj>#</a></h3><h3 id=key-facts-45>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-45>#</a></h3><ul><li>The EU fined Apple €500 million (approximately $570 million) for breaching the Digital Markets Act and Meta €200 million.</li><li>The fines target violations related to Apple’s App Store rules and Meta’s personalized ad practices.</li><li>The European Commission issued cease-and-desist orders requiring changes to Apple’s app store transparency and Meta’s ad personalization, with a 60-day compliance window before additional penalties up to 5% of daily revenue.</li></ul><h3 id=summary-45>Summary<a hidden class=anchor aria-hidden=true href=#summary-45>#</a></h3><p>The European Union imposed €500 million ($570 million) fine on Apple and €200 million on Meta for violations of the Digital Markets Act, focusing on App Store transparency and personalized advertising practices. The EU ordered Apple to allow app developers to inform users of alternative purchase options and directed Meta to cease requiring user consent for personalized ads on Facebook and Instagram. Both companies plan to appeal. The enforcement aims to promote competition, with potential fines up to 10% of global revenue, and comes amid heightened US-EU trade tensions over digital regulations.</p><h2 id=-technology>▶️ Technology<a hidden class=anchor aria-hidden=true href=#-technology>#</a></h2><h3 id=try-generating-video-in-gemini-powered-by-veo-2><a href=https://blog.google/products/gemini/video-generation/>Try generating video in Gemini, powered by Veo 2</a><a hidden class=anchor aria-hidden=true href=#try-generating-video-in-gemini-powered-by-veo-2>#</a></h3><h3 id=key-facts-46>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-46>#</a></h3><ul><li>Google has introduced video generation features in Gemini, powered by Veo 2, available to Google One AI Premium subscribers.</li><li>Users can transform text prompts into high-resolution, eight-second videos at 720p in a 16:9 aspect ratio, with a monthly creation limit.</li><li>Additionally, Whisk Animate enables turning images into eight-second videos using Veo 2, accessible in over 60 countries.</li></ul><h3 id=summary-46>Summary<a hidden class=anchor aria-hidden=true href=#summary-46>#</a></h3><p>Google has launched new video generation capabilities within the Gemini AI platform, utilizing the advanced Veo 2 model to produce high-resolution, cinematic-quality videos from text prompts. Users can generate eight-second videos at 720p resolution in a 16:9 landscape format by selecting Veo 2 from the model dropdown in Gemini. The process involves describing a scene or concept, with more detailed prompts offering greater control over the output. Examples include scenes of a glacial cavern, animated mice reading, coastal landscapes, and voxel-style melting ice cream. The feature is rolling out globally to Gemini Advanced users and Google One AI Premium subscribers, with a monthly limit on video creation. Additionally, Whisk Animate allows users to animate images into short videos, expanding creative possibilities. All generated videos are marked with SynthID watermarks for safety and transparency. The platform emphasizes safety through extensive red teaming and content moderation policies. Users can share videos directly to platforms like TikTok and YouTube Shorts, and the service continues to evolve with UI adjustments and ongoing improvements. More information is available at <a href=http://gemini.google/>gemini.google</a>.</p><hr><h3 id=gemma-3-qat-models-bringing-state-of-the-art-ai-to-consumer-gpus---google-developers-blog><a href=https://developers.googleblog.com/en/gemma-3-quantized-aware-trained-state-of-the-art-ai-to-consumer-gpus/>Gemma 3 QAT Models: Bringing state-of-the-Art AI to consumer GPUs - Google Developers Blog</a><a hidden class=anchor aria-hidden=true href=#gemma-3-qat-models-bringing-state-of-the-art-ai-to-consumer-gpus---google-developers-blog>#</a></h3><h3 id=key-facts-47>Key Facts<a hidden class=anchor aria-hidden=true href=#key-facts-47>#</a></h3><ul><li>Google announced quantization-aware training (QAT) optimized versions of Gemma 3 models on April 18, 2025</li><li>QAT reduces VRAM requirements by up to 4x, enabling deployment of large models like Gemma 3 27B on consumer GPUs such as NVIDIA RTX 3090</li><li>Quantized models (int4, Q4_0) maintain high accuracy, with perplexity drop reduced by 54% using QAT during training</li></ul><h3 id=summary-47>Summary<a hidden class=anchor aria-hidden=true href=#summary-47>#</a></h3><p>Google introduced optimized Gemma 3 models with Quantization-Aware Training (QAT) to enhance accessibility of large language models on consumer-grade GPUs. The original Gemma 3 models deliver state-of-the-art performance, capable of running on high-end GPUs like NVIDIA H100 with BF16 precision. QAT enables these models to be quantized to lower-precision formats such as int4 and Q4_0, significantly reducing VRAM usage—e.g., Gemma 3 27B&rsquo;s weights decrease from 54 GB (BF16) to 14.1 GB (int4). This reduction allows deployment on GPUs with limited memory, such as NVIDIA RTX 3090 (24GB VRAM) for the 27B model, and NVIDIA RTX 4060 Laptop GPU (8GB VRAM) for the 12B variant. The models are trained with QAT over approximately 5,000 steps, using non-quantized checkpoints as targets, which reduces perplexity degradation by 54%. The quantized models are compatible with inference engines like Ollama, llama.cpp, MLX, Gemma.cpp, and support integration via Hugging Face and Kaggle. Additional community-driven quantization options are available through the Gemmaverse, often utilizing Post-Training Quantization (PTQ). These advancements democratize access to powerful AI models, enabling local deployment on desktops, laptops, and mobile devices.</p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=next href=https://alobbs.com/posts/2025-04-22/><span class=title>Next »</span><br><span>2025-04-22 Briefing</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share 2025-04-23 Briefing on x" href="https://x.com/intent/tweet/?text=2025-04-23%20Briefing&amp;url=https%3a%2f%2falobbs.com%2fposts%2f2025-04-23%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 2025-04-23 Briefing on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2falobbs.com%2fposts%2f2025-04-23%2f&amp;title=2025-04-23%20Briefing&amp;summary=2025-04-23%20Briefing&amp;source=https%3a%2f%2falobbs.com%2fposts%2f2025-04-23%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 2025-04-23 Briefing on reddit" href="https://reddit.com/submit?url=https%3a%2f%2falobbs.com%2fposts%2f2025-04-23%2f&title=2025-04-23%20Briefing"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 2025-04-23 Briefing on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2falobbs.com%2fposts%2f2025-04-23%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 2025-04-23 Briefing on whatsapp" href="https://api.whatsapp.com/send?text=2025-04-23%20Briefing%20-%20https%3a%2f%2falobbs.com%2fposts%2f2025-04-23%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 2025-04-23 Briefing on telegram" href="https://telegram.me/share/url?text=2025-04-23%20Briefing&amp;url=https%3a%2f%2falobbs.com%2fposts%2f2025-04-23%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share 2025-04-23 Briefing on ycombinator" href="https://news.ycombinator.com/submitlink?t=2025-04-23%20Briefing&u=https%3a%2f%2falobbs.com%2fposts%2f2025-04-23%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://alobbs.com/>Alvaro's Site</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><a rel=me href=https://mastodon.social/@alobbs style=display:none>Mastodon</a>
<script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>